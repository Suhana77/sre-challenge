{
  "version": 4,
  "terraform_version": "1.9.6",
  "serial": 100,
  "lineage": "5a8e2f15-b615-8baa-fb18-ac7c75a1864b",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "metrics-server",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "metrics-server",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "metrics-server",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.5.2",
                "chart": "metrics-server",
                "first_deployed": 1727665974,
                "last_deployed": 1727665974,
                "name": "metrics-server",
                "namespace": "kube-system",
                "notes": "***********************************************************************\n* Metrics Server                                                      *\n***********************************************************************\n  Chart version: 3.7.0\n  App version:   0.5.2\n  Image tag:     k8s.gcr.io/metrics-server/metrics-server:v0.5.2\n***********************************************************************\n",
                "revision": 1,
                "values": "{\"args\":[\"--kubelet-preferred-address-types=InternalIP\",\"--kubelet-insecure-tls\"],\"hostNetwork\":{\"enabled\":true},\"server.resources\":\"\\\"limits\\\":\\n  \\\"cpu\\\": \\\"200m\\\"\\n  \\\"memory\\\": \\\"50Mi\\\"\\n\\\"requests\\\":\\n  \\\"cpu\\\": \\\"100m\\\"\\n  \\\"memory\\\": \\\"30Mi\\\"\\n\"}",
                "version": "3.7.0"
              }
            ],
            "name": "metrics-server",
            "namespace": "kube-system",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://kubernetes-sigs.github.io/metrics-server",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "server\\.resources",
                "type": "",
                "value": "\"limits\":\n  \"cpu\": \"200m\"\n  \"memory\": \"50Mi\"\n\"requests\":\n  \"cpu\": \"100m\"\n  \"memory\": \"30Mi\"\n"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 2000,
            "upgrade_install": null,
            "values": [
              "# https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md\r\n\r\nhostNetwork:\r\n  enabled: true\r\n\r\nargs:\r\n  - \"--kubelet-preferred-address-types=InternalIP\"\r\n  - \"--kubelet-insecure-tls\""
            ],
            "verify": false,
            "version": "3.7.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.76.1",
                "chart": "kube-prometheus-stack",
                "first_deployed": 1727665985,
                "last_deployed": 1727665985,
                "name": "prometheus",
                "namespace": "prometheus",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace sre-challenge -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace sre-challenge $POD_NAME 9100\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace sre-challenge get pods -l \"release=prometheus\"\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create \u0026 configure Alertmanager and Prometheus instances using the Operator.\n\nkube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, prometheus-kube-state-metrics.sre-challenge.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace sre-challenge prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   prometheus-grafana.sre-challenge.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n     export POD_NAME=$(kubectl get pods --namespace sre-challenge -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n     kubectl --namespace sre-challenge port-forward $POD_NAME 3000\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n",
                "revision": 1,
                "values": "{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigMatcherStrategy\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{},\"alertmanagerConfiguration\":{},\"automountServiceAccountToken\":true,\"clusterAdvertiseAddress\":false,\"clusterGossipInterval\":\"\",\"clusterLabel\":\"\",\"clusterPeerTimeout\":\"\",\"clusterPushpullInterval\":\"\",\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.27.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"scheme\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tlsConfig\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[],\"web\":{}},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"inhibit_rules\":[{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = critical\"],\"target_matchers\":[\"severity =~ warning|info\"]},{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = warning\"],\"target_matchers\":[\"severity = info\"]},{\"equal\":[\"namespace\"],\"source_matchers\":[\"alertname = InfoInhibitor\"],\"target_matchers\":[\"severity = info\"]},{\"target_matchers\":[\"alertname = InfoInhibitor\"]}],\"receivers\":[{\"name\":\"null\"}],\"route\":{\"group_by\":[\"namespace\"],\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"null\",\"repeat_interval\":\"12h\",\"routes\":[{\"matchers\":[\"alertname = \\\"Watchdog\\\"\"],\"receiver\":\"null\"}]},\"templates\":[\"/etc/alertmanager/config/*.tmpl\"]},\"enableFeatures\":[],\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"enableHttp2\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"stringConfig\":\"\",\"templateFiles\":{},\"tplConfig\":false},\"checkDeprecation\":true,\"cleanPrometheusOperatorObjectNames\":false,\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"crds\":{\"enabled\":true},\"customRules\":{},\"defaultRules\":{\"additionalAggregationLabels\":[],\"additionalRuleAnnotations\":{},\"additionalRuleGroupAnnotations\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleGroupLabels\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8sContainerCpuUsageSecondsTotal\":{},\"k8sContainerMemoryCache\":{},\"k8sContainerMemoryRss\":{},\"k8sContainerMemorySwap\":{},\"k8sContainerResource\":{},\"k8sPodOwner\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"keepFiringFor\":\"\",\"labels\":{},\"node\":{\"fsSelector\":\"fstype!=\\\"\\\"\"},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8sContainerCpuUsageSecondsTotal\":true,\"k8sContainerMemoryCache\":true,\"k8sContainerMemoryRss\":true,\"k8sContainerMemorySwap\":true,\"k8sContainerMemoryWorkingSetBytes\":true,\"k8sContainerResource\":true,\"k8sPodOwner\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverBurnrate\":true,\"kubeApiserverHistogram\":true,\"kubeApiserverSlos\":true,\"kubeControllerManager\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeSchedulerAlerting\":true,\"kubeSchedulerRecording\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true,\"windows\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"extraManifests\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"rbac\":{\"create\":true,\"createAggregateClusterRoles\":false,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEditable\":true,\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":true,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"sre-challenge\",\"prune\":false,\"rbac\":{\"pspEnabled\":false},\"service\":{\"ipFamilies\":[],\"ipFamilyPolicy\":\"\",\"portName\":\"http-web\"},\"serviceAccount\":{\"autoMount\":true,\"create\":true},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enableNewTablePanelSyntax\":false,\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false},\"searchNamespace\":\"ALL\"},\"datasources\":{\"alertmanager\":{\"enabled\":true,\"handleGrafanaManagedAlerts\":false,\"implementation\":\"prometheus\",\"name\":\"Alertmanager\",\"uid\":\"alertmanager\"},\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"httpMethod\":\"POST\",\"isDefaultDatasource\":true,\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"name\":\"Prometheus\",\"uid\":\"prometheus\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"sre-challenge\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}},\"targetLimit\":0},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLimit\":0}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":2381,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"keyFile\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"http\",\"selector\":{},\"serverName\":\"\",\"targetLimit\":0}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":false,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"targetLimit\":0}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"port\":\"http-metrics\",\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{},\"serverName\":null,\"targetLimit\":0}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"attachMetadata\":{\"node\":false},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"honorLabels\":true,\"honorTimestamps\":true,\"https\":true,\"insecureSkipVerify\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"sampleLimit\":0,\"targetLimit\":0}},\"kubernetesServiceMonitors\":{\"enabled\":true},\"nameOverride\":\"\",\"namespaceOverride\":\"sre-challenge\",\"nodeExporter\":{\"enabled\":true,\"forceDeployDashboards\":false,\"operatingSystems\":{\"darwin\":{\"enabled\":true},\"linux\":{\"enabled\":true}}},\"podSecurityPolicy\":{\"enabled\":true},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"agentMode\":false,\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalAlertRelabelConfigsSecret\":{},\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"automountServiceAccountToken\":true,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enableRemoteWriteReceiver\":false,\"enforcedKeepDroppedTargets\":0,\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"excludedFromEnforcement\":[],\"exemplars\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"hostAliases\":[],\"hostNetwork\":false,\"ignoreNamespaceSelectors\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.54.1\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"maximumStartupDurationSeconds\":0,\"minReadySeconds\":0,\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"persistentVolumeClaimRetentionPolicy\":{},\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorSelector\":{\"matchLabels\":null},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{\"matchLabels\":{\"release\":\"{{ $.Release.Name }}\"}},\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{\"matchLabels\":{\"release\":\"{{ $.Release.Name }}\"}},\"sampleLimit\":false,\"scrapeClasses\":[],\"scrapeConfigNamespaceSelector\":{},\"scrapeConfigSelector\":{\"matchLabels\":{\"release\":\"{{ $.Release.Name }}\"}},\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceDiscoveryRole\":\"\",\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{\"matchLabels\":{\"release\":\"{{ $.Release.Name }}\"}},\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"tracingConfig\":{},\"tsdb\":{\"outOfOrderTimeWindow\":\"0s\"},\"version\":\"\",\"volumeMounts\":[],\"volumes\":[],\"walCompression\":true,\"web\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"reloaderWebPort\":8080,\"sessionAffinity\":\"None\",\"sessionAffinityConfig\":{\"clientIP\":{\"timeoutSeconds\":10800}},\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"},\"thanosServiceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"sre-challenge\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"pspEnabled\":false},\"releaseLabel\":true,\"service\":{\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{\"jobLabel\":\"node-exporter\"},\"portName\":\"http-metrics\"}},\"prometheus-windows-exporter\":{\"config\":\"collectors:\\n  enabled: '[defaults],memory,container'\",\"podLabels\":{\"jobLabel\":\"windows-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"jobLabel\":\"jobLabel\"}},\"releaseLabel\":true},\"prometheusOperator\":{\"admissionWebhooks\":{\"annotations\":{},\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"createSecretJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"deployment\":{\"affinity\":{},\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"dnsConfig\":{},\"enabled\":false,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/admission-webhook\",\"sha\":\"\",\"tag\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":30,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"nodeSelector\":{},\"podAnnotations\":{},\"podDisruptionBudget\":{},\"podLabels\":{},\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":5,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"revisionHistoryLimit\":10,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":31080,\"nodePortTls\":31443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":false,\"create\":true,\"name\":\"\"},\"strategy\":{},\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[]},\"enabled\":true,\"failurePolicy\":\"\",\"mutatingWebhookConfiguration\":{\"annotations\":{}},\"namespaceSelector\":{},\"objectSelector\":{},\"patch\":{\"affinity\":{},\"annotations\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"repository\":\"ingress-nginx/kube-webhook-certgen\",\"sha\":\"\",\"tag\":\"v20221220-controller-v1.5.1-58-g787ea74b6\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true},\"tolerations\":[],\"ttlSecondsAfterFinished\":60},\"patchWebhookJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"timeoutSeconds\":10,\"validatingWebhookConfiguration\":{\"annotations\":{}}},\"affinity\":{},\"alertmanagerConfigNamespaces\":[],\"alertmanagerInstanceNamespaces\":[],\"alertmanagerInstanceSelector\":\"\",\"annotations\":{},\"automountServiceAccountToken\":true,\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"env\":{\"GOGC\":\"30\"},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"\"},\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\",\"selector\":\"\"},\"labels\":{},\"livenessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"namespaces\":{},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"enableProbe\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"\"},\"resources\":{}},\"prometheusInstanceNamespaces\":[],\"prometheusInstanceSelector\":\"\",\"readinessProbe\":{\"enabled\":true,\"failureThreshold\":3,\"initialDelaySeconds\":0,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"revisionHistoryLimit\":10,\"secretFieldSelector\":\"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"selfMonitor\":true,\"targetLimit\":0},\"strategy\":{},\"thanosImage\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.36.1\"},\"thanosRulerInstanceNamespaces\":[],\"thanosRulerInstanceSelector\":\"\",\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[],\"verticalPodAutoscaler\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{},\"updatePolicy\":{\"updateMode\":\"Auto\"}}},\"server\":{\"persistentVolume\":{\"enabled\":false}},\"server.resources\":\"\\\"limits\\\":\\n  \\\"cpu\\\": \\\"200m\\\"\\n  \\\"memory\\\": \\\"50Mi\\\"\\n\\\"requests\\\":\\n  \\\"cpu\\\": \\\"100m\\\"\\n  \\\"memory\\\": \\\"30Mi\\\"\\n\",\"thanosRuler\":{\"annotations\":{},\"enabled\":false,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"ipDualStack\":{\"enabled\":false,\"ipFamilies\":[\"IPv6\",\"IPv4\"],\"ipFamilyPolicy\":\"PreferDualStack\"},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30905,\"port\":10902,\"targetPort\":10902,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalEndpoints\":[],\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"thanosRulerSpec\":{\"additionalArgs\":[],\"additionalConfig\":{},\"additionalConfigString\":\"\",\"affinity\":{},\"alertDropLabels\":[],\"alertmanagersConfig\":{\"existingSecret\":{},\"secret\":{}},\"containers\":[],\"evaluationInterval\":\"\",\"externalPrefix\":null,\"externalPrefixNilUsesHelmValues\":true,\"image\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.36.1\"},\"initContainers\":[],\"labels\":{},\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"objectStorageConfig\":{\"existingSecret\":{},\"secret\":{}},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"web\",\"priorityClassName\":\"\",\"queryConfig\":{\"existingSecret\":{},\"secret\":{}},\"queryEndpoints\":[],\"replicas\":1,\"resources\":{},\"retention\":\"24h\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{\"matchLabels\":{\"release\":\"{{ $.Release.Name }}\"}},\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[],\"web\":{}}},\"windowsMonitoring\":{\"enabled\":false}}",
                "version": "63.1.0"
              }
            ],
            "name": "prometheus",
            "namespace": "prometheus",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "podSecurityPolicy.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "server.persistentVolume.enabled",
                "type": "",
                "value": "false"
              },
              {
                "name": "server\\.resources",
                "type": "",
                "value": "\"limits\":\n  \"cpu\": \"200m\"\n  \"memory\": \"50Mi\"\n\"requests\":\n  \"cpu\": \"100m\"\n  \"memory\": \"30Mi\"\n"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 2000,
            "upgrade_install": null,
            "values": [
              "\r\n# Default values for kube-prometheus-stack.\r\n# This is a YAML-formatted file.\r\n# Declare variables to be passed into your templates.\r\n\r\n## Provide a name in place of kube-prometheus-stack for `app:` labels\r\n##\r\nnameOverride: \"\"\r\n\r\n## Override the deployment namespace\r\n##\r\nnamespaceOverride: \"sre-challenge\"\r\n\r\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6\r\n##\r\nkubeTargetVersionOverride: \"\"\r\n\r\n## Allow kubeVersion to be overridden while creating the ingress\r\n##\r\nkubeVersionOverride: \"\"\r\n\r\n## Provide a name to substitute for the full names of resources\r\n##\r\nfullnameOverride: \"\"\r\n\r\n## Labels to apply to all resources\r\n##\r\ncommonLabels: {}\r\n# scmhash: abc123\r\n# myLabel: aakkmd\r\n\r\n## Checks if any deprecated values are used\r\n##\r\ncheckDeprecation: true\r\n\r\n## Install Prometheus Operator CRDs\r\n##\r\ncrds:\r\n  enabled: true\r\n\r\n## custom Rules to override \"for\" and \"severity\" in defaultRules\r\n##\r\ncustomRules: {}\r\n  # AlertmanagerFailedReload:\r\n  #   for: 3m\r\n  # AlertmanagerMembersInconsistent:\r\n  #   for: 5m\r\n  #   severity: \"warning\"\r\n\r\n## Create default rules for monitoring the cluster\r\n##\r\ndefaultRules:\r\n  create: true\r\n  rules:\r\n    alertmanager: true\r\n    etcd: true\r\n    configReloaders: true\r\n    general: true\r\n    k8sContainerCpuUsageSecondsTotal: true\r\n    k8sContainerMemoryCache: true\r\n    k8sContainerMemoryRss: true\r\n    k8sContainerMemorySwap: true\r\n    k8sContainerResource: true\r\n    k8sContainerMemoryWorkingSetBytes: true\r\n    k8sPodOwner: true\r\n    kubeApiserverAvailability: true\r\n    kubeApiserverBurnrate: true\r\n    kubeApiserverHistogram: true\r\n    kubeApiserverSlos: true\r\n    kubeControllerManager: true\r\n    kubelet: true\r\n    kubeProxy: true\r\n    kubePrometheusGeneral: true\r\n    kubePrometheusNodeRecording: true\r\n    kubernetesApps: true\r\n    kubernetesResources: true\r\n    kubernetesStorage: true\r\n    kubernetesSystem: true\r\n    kubeSchedulerAlerting: true\r\n    kubeSchedulerRecording: true\r\n    kubeStateMetrics: true\r\n    network: true\r\n    node: true\r\n    nodeExporterAlerting: true\r\n    nodeExporterRecording: true\r\n    prometheus: true\r\n    prometheusOperator: true\r\n    windows: true\r\n\r\n  ## Reduce app namespace alert scope\r\n  appNamespacesTarget: \".*\"\r\n\r\n  ## Set keep_firing_for for all alerts\r\n  keepFiringFor: \"\"\r\n\r\n  ## Labels for default rules\r\n  labels: {}\r\n  ## Annotations for default rules\r\n  annotations: {}\r\n\r\n  ## Additional labels for PrometheusRule alerts\r\n  additionalRuleLabels: {}\r\n\r\n  ## Additional annotations for PrometheusRule alerts\r\n  additionalRuleAnnotations: {}\r\n\r\n  ## Additional labels for specific PrometheusRule alert groups\r\n  additionalRuleGroupLabels:\r\n    alertmanager: {}\r\n    etcd: {}\r\n    configReloaders: {}\r\n    general: {}\r\n    k8sContainerCpuUsageSecondsTotal: {}\r\n    k8sContainerMemoryCache: {}\r\n    k8sContainerMemoryRss: {}\r\n    k8sContainerMemorySwap: {}\r\n    k8sContainerResource: {}\r\n    k8sPodOwner: {}\r\n    kubeApiserverAvailability: {}\r\n    kubeApiserverBurnrate: {}\r\n    kubeApiserverHistogram: {}\r\n    kubeApiserverSlos: {}\r\n    kubeControllerManager: {}\r\n    kubelet: {}\r\n    kubeProxy: {}\r\n    kubePrometheusGeneral: {}\r\n    kubePrometheusNodeRecording: {}\r\n    kubernetesApps: {}\r\n    kubernetesResources: {}\r\n    kubernetesStorage: {}\r\n    kubernetesSystem: {}\r\n    kubeSchedulerAlerting: {}\r\n    kubeSchedulerRecording: {}\r\n    kubeStateMetrics: {}\r\n    network: {}\r\n    node: {}\r\n    nodeExporterAlerting: {}\r\n    nodeExporterRecording: {}\r\n    prometheus: {}\r\n    prometheusOperator: {}\r\n\r\n  ## Additional annotations for specific PrometheusRule alerts groups\r\n  additionalRuleGroupAnnotations:\r\n    alertmanager: {}\r\n    etcd: {}\r\n    configReloaders: {}\r\n    general: {}\r\n    k8sContainerCpuUsageSecondsTotal: {}\r\n    k8sContainerMemoryCache: {}\r\n    k8sContainerMemoryRss: {}\r\n    k8sContainerMemorySwap: {}\r\n    k8sContainerResource: {}\r\n    k8sPodOwner: {}\r\n    kubeApiserverAvailability: {}\r\n    kubeApiserverBurnrate: {}\r\n    kubeApiserverHistogram: {}\r\n    kubeApiserverSlos: {}\r\n    kubeControllerManager: {}\r\n    kubelet: {}\r\n    kubeProxy: {}\r\n    kubePrometheusGeneral: {}\r\n    kubePrometheusNodeRecording: {}\r\n    kubernetesApps: {}\r\n    kubernetesResources: {}\r\n    kubernetesStorage: {}\r\n    kubernetesSystem: {}\r\n    kubeSchedulerAlerting: {}\r\n    kubeSchedulerRecording: {}\r\n    kubeStateMetrics: {}\r\n    network: {}\r\n    node: {}\r\n    nodeExporterAlerting: {}\r\n    nodeExporterRecording: {}\r\n    prometheus: {}\r\n    prometheusOperator: {}\r\n\r\n  additionalAggregationLabels: []\r\n\r\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\r\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\r\n\r\n  node:\r\n    fsSelector: 'fstype!=\"\"'\r\n    # fsSelector: 'fstype=~\"ext[234]|btrfs|xfs|zfs\"'\r\n\r\n  ## Disabled PrometheusRule alerts\r\n  disabled: {}\r\n  # KubeAPIDown: true\r\n  # NodeRAIDDegraded: true\r\n\r\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\r\n##\r\n# additionalPrometheusRules: []\r\n#  - name: my-rule-file\r\n#    groups:\r\n#      - name: my_group\r\n#        rules:\r\n#        - record: my_record\r\n#          expr: 100 * my_record\r\n\r\n## Provide custom recording or alerting rules to be deployed into the cluster.\r\n##\r\nadditionalPrometheusRulesMap: {}\r\n#  rule-name:\r\n#    groups:\r\n#    - name: my_group\r\n#      rules:\r\n#      - record: my_record\r\n#        expr: 100 * my_record\r\n\r\n##\r\nglobal:\r\n  rbac:\r\n    create: true\r\n\r\n    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs\r\n    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\r\n    createAggregateClusterRoles: false\r\n    pspEnabled: false\r\n    pspAnnotations: {}\r\n      ## Specify pod annotations\r\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\r\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\r\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\r\n      ##\r\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\r\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\r\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\r\n\r\n  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)\r\n  ##\r\n  imageRegistry: \"\"\r\n\r\n  ## Reference to one or more secrets to be used when pulling images\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\r\n  ##\r\n  imagePullSecrets: []\r\n  # - name: \"image-pull-secret\"\r\n  # or\r\n  # - \"image-pull-secret\"\r\n\r\nwindowsMonitoring:\r\n  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')\r\n  enabled: false\r\n\r\n## Configuration for prometheus-windows-exporter\r\n## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter\r\n##\r\nprometheus-windows-exporter:\r\n  ## Enable ServiceMonitor and set Kubernetes label to use as a job label\r\n  ##\r\n  prometheus:\r\n    monitor:\r\n      enabled: true\r\n      jobLabel: jobLabel\r\n\r\n  releaseLabel: true\r\n\r\n  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards\r\n  ##\r\n  podLabels:\r\n    jobLabel: windows-exporter\r\n\r\n  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards\r\n  ##\r\n  config: |-\r\n    collectors:\r\n      enabled: '[defaults],memory,container'\r\n\r\n## Configuration for alertmanager\r\n## ref: https://prometheus.io/docs/alerting/alertmanager/\r\n##\r\nalertmanager:\r\n\r\n  ## Deploy alertmanager\r\n  ##\r\n  enabled: true\r\n\r\n  ## Annotations for Alertmanager\r\n  ##\r\n  annotations: {}\r\n\r\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\r\n  ##\r\n  apiVersion: v2\r\n\r\n  ## @param alertmanager.enableFeatures Enable access to Alertmanager disabled features.\r\n  ##\r\n  enableFeatures: []\r\n\r\n  ## Service account for Alertmanager to use.\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n  ##\r\n  serviceAccount:\r\n    create: true\r\n    name: \"\"\r\n    annotations: {}\r\n    automountServiceAccountToken: true\r\n\r\n  ## Configure pod disruption budgets for Alertmanager\r\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\r\n  ##\r\n  podDisruptionBudget:\r\n    enabled: false\r\n    minAvailable: 1\r\n    maxUnavailable: \"\"\r\n\r\n  ## Alertmanager configuration directives\r\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\r\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\r\n  ##\r\n  config:\r\n    global:\r\n      resolve_timeout: 5m\r\n    inhibit_rules:\r\n      - source_matchers:\r\n          - 'severity = critical'\r\n        target_matchers:\r\n          - 'severity =~ warning|info'\r\n        equal:\r\n          - 'namespace'\r\n          - 'alertname'\r\n      - source_matchers:\r\n          - 'severity = warning'\r\n        target_matchers:\r\n          - 'severity = info'\r\n        equal:\r\n          - 'namespace'\r\n          - 'alertname'\r\n      - source_matchers:\r\n          - 'alertname = InfoInhibitor'\r\n        target_matchers:\r\n          - 'severity = info'\r\n        equal:\r\n          - 'namespace'\r\n      - target_matchers:\r\n          - 'alertname = InfoInhibitor'\r\n    route:\r\n      group_by: ['namespace']\r\n      group_wait: 30s\r\n      group_interval: 5m\r\n      repeat_interval: 12h\r\n      receiver: 'null'\r\n      routes:\r\n      - receiver: 'null'\r\n        matchers:\r\n          - alertname = \"Watchdog\"\r\n    receivers:\r\n    - name: 'null'\r\n    templates:\r\n    - '/etc/alertmanager/config/*.tmpl'\r\n\r\n  ## Alertmanager configuration directives (as string type, preferred over the config hash map)\r\n  ## stringConfig will be used only, if tplConfig is true\r\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\r\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\r\n  ##\r\n  stringConfig: \"\"\r\n\r\n  ## Pass the Alertmanager configuration directives through Helm's templating\r\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\r\n  ## they'll need to be properly escaped so that they are not interpreted by\r\n  ## Helm\r\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\r\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\r\n  ##      https://prometheus.io/docs/alerting/notifications/\r\n  ##      https://prometheus.io/docs/alerting/notification_examples/\r\n  tplConfig: false\r\n\r\n  ## Alertmanager template files to format alerts\r\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\r\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\r\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\r\n  ## config.templates above to include those suffixes.\r\n  ## ref: https://prometheus.io/docs/alerting/notifications/\r\n  ##      https://prometheus.io/docs/alerting/notification_examples/\r\n  ##\r\n  templateFiles: {}\r\n  #\r\n  ## An example template:\r\n  #   template_1.tmpl: |-\r\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\r\n  #\r\n  #       {{ define \"slack.myorg.text\" }}\r\n  #       {{- $root := . -}}\r\n  #       {{ range .Alerts }}\r\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\r\n  #         *Cluster:* {{ template \"cluster\" $root }}\r\n  #         *Description:* {{ .Annotations.description }}\r\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\r\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\r\n  #         *Details:*\r\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\r\n  #           {{ end }}\r\n  #       {{ end }}\r\n  #       {{ end }}\r\n\r\n  ingress:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n\r\n    labels: {}\r\n\r\n    ## Override ingress to a different defined port on the service\r\n    # servicePort: 8081\r\n    ## Override ingress to a different service then the default, this is useful if you need to\r\n    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)\r\n    # serviceName: kube-prometheus-stack-alertmanager-0\r\n\r\n    ## Hosts must be provided if Ingress is enabled.\r\n    ##\r\n    hosts: []\r\n      # - alertmanager.domain.com\r\n\r\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## TLS configuration for Alertmanager Ingress\r\n    ## Secret must be manually created in the namespace\r\n    ##\r\n    tls: []\r\n    # - secretName: alertmanager-general-tls\r\n    #   hosts:\r\n    #   - alertmanager.example.com\r\n\r\n  ## Configuration for Alertmanager secret\r\n  ##\r\n  secret:\r\n    annotations: {}\r\n\r\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\r\n  ## alertmanager.servicePerReplica must be enabled\r\n  ##\r\n  ingressPerReplica:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n    labels: {}\r\n\r\n    ## Final form of the hostname for each per replica ingress is\r\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\r\n    ##\r\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\r\n    ## appended to the end\r\n    hostPrefix: \"\"\r\n    ## Domain that will be used for the per replica ingress\r\n    hostDomain: \"\"\r\n\r\n    ## Paths to use for ingress rules\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\r\n    ## Secret must be manually created in the namespace\r\n    tlsSecretName: \"\"\r\n\r\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\r\n    ##\r\n    tlsSecretPerReplica:\r\n      enabled: false\r\n      ## Final form of the secret for each per replica ingress is\r\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\r\n      ##\r\n      prefix: \"alertmanager\"\r\n\r\n  ## Configuration for Alertmanager service\r\n  ##\r\n  service:\r\n    annotations: {}\r\n    labels: {}\r\n    clusterIP: \"\"\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n    ## Port for Alertmanager Service to listen on\r\n    ##\r\n    port: 9093\r\n    ## To be used with a proxy extraContainer port\r\n    ##\r\n    targetPort: 9093\r\n    ## Port to expose on each node\r\n    ## Only used if service.type is 'NodePort'\r\n    ##\r\n    nodePort: 30903\r\n    ## List of IP addresses at which the Prometheus server service is available\r\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\r\n    ##\r\n\r\n    ## Additional ports to open for Alertmanager service\r\n    ##\r\n    additionalPorts: []\r\n    # - name: oauth-proxy\r\n    #   port: 8081\r\n    #   targetPort: 8081\r\n    # - name: oauth-metrics\r\n    #   port: 8082\r\n    #   targetPort: 8082\r\n\r\n    externalIPs: []\r\n    loadBalancerIP: \"\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\r\n    ## Accepts 'ClientIP' or 'None'\r\n    ##\r\n    sessionAffinity: None\r\n\r\n    ## If you want to modify the ClientIP sessionAffinity timeout\r\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\r\n    ##\r\n    sessionAffinityConfig:\r\n      clientIP:\r\n        timeoutSeconds: 10800\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\r\n  ##\r\n  servicePerReplica:\r\n    enabled: false\r\n    annotations: {}\r\n\r\n    ## Port for Alertmanager Service per replica to listen on\r\n    ##\r\n    port: 9093\r\n\r\n    ## To be used with a proxy extraContainer port\r\n    targetPort: 9093\r\n\r\n    ## Port to expose on each node\r\n    ## Only used if servicePerReplica.type is 'NodePort'\r\n    ##\r\n    nodePort: 30904\r\n\r\n    ## Loadbalancer source IP ranges\r\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n  ## Configuration for creating a ServiceMonitor for AlertManager\r\n  ##\r\n  serviceMonitor:\r\n    ## If true, a ServiceMonitor will be created for the AlertManager service.\r\n    ##\r\n    selfMonitor: true\r\n\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\r\n    scheme: \"\"\r\n\r\n    ## enableHttp2: Whether to enable HTTP2.\r\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\r\n    enableHttp2: true\r\n\r\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\r\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\r\n    tlsConfig: {}\r\n\r\n    bearerTokenFile:\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional Endpoints\r\n    ##\r\n    additionalEndpoints: []\r\n    # - port: oauth-metrics\r\n    #   path: /metrics\r\n\r\n  ## Settings affecting alertmanagerSpec\r\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\r\n  ##\r\n  alertmanagerSpec:\r\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\r\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\r\n    ##\r\n    podMetadata: {}\r\n\r\n    ## Image of Alertmanager\r\n    ##\r\n    image:\r\n      registry: quay.io\r\n      repository: prometheus/alertmanager\r\n      tag: v0.27.0\r\n      sha: \"\"\r\n\r\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\r\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\r\n    ##\r\n    useExistingSecret: false\r\n\r\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\r\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\r\n    ##\r\n    secrets: []\r\n\r\n    ## If false then the user will opt out of automounting API credentials.\r\n    ##\r\n    automountServiceAccountToken: true\r\n\r\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\r\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\r\n    ##\r\n    configMaps: []\r\n\r\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\r\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\r\n    ##\r\n    # configSecret:\r\n\r\n    ## WebTLSConfig defines the TLS parameters for HTTPS\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec\r\n    web: {}\r\n\r\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\r\n    ##\r\n    alertmanagerConfigSelector: {}\r\n    ## Example which selects all alertmanagerConfig resources\r\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\r\n    # alertmanagerConfigSelector:\r\n    #   matchExpressions:\r\n    #     - key: alertconfig\r\n    #       operator: In\r\n    #       values:\r\n    #         - example-config\r\n    #         - example-config-2\r\n    #\r\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\r\n    # alertmanagerConfigSelector:\r\n    #   matchLabels:\r\n    #     role: example-config\r\n\r\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\r\n    ##\r\n    alertmanagerConfigNamespaceSelector: {}\r\n    ## Example which selects all namespaces\r\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\r\n    # alertmanagerConfigNamespaceSelector:\r\n    #   matchExpressions:\r\n    #     - key: alertmanagerconfig\r\n    #       operator: In\r\n    #       values:\r\n    #         - example-namespace\r\n    #         - example-namespace-2\r\n\r\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\r\n    # alertmanagerConfigNamespaceSelector:\r\n    #   matchLabels:\r\n    #     alertmanagerconfig: enabled\r\n\r\n    ## AlermanagerConfig to be used as top level configuration\r\n    ##\r\n    alertmanagerConfiguration: {}\r\n    ## Example with select a global alertmanagerconfig\r\n    # alertmanagerConfiguration:\r\n    #   name: global-alertmanager-Configuration\r\n\r\n    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:\r\n    ##\r\n    alertmanagerConfigMatcherStrategy: {}\r\n    ## Example with use OnNamespace strategy\r\n    # alertmanagerConfigMatcherStrategy:\r\n    #   type: OnNamespace\r\n\r\n    ## Define Log Format\r\n    # Use logfmt (default) or json logging\r\n    logFormat: logfmt\r\n\r\n    ## Log level for Alertmanager to be configured with.\r\n    ##\r\n    logLevel: info\r\n\r\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\r\n    ## running cluster equal to the expected size.\r\n    replicas: 1\r\n\r\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\r\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\r\n    ##\r\n    retention: 120h\r\n\r\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\r\n    ##\r\n    storage: {}\r\n    # volumeClaimTemplate:\r\n    #   spec:\r\n    #     storageClassName: gluster\r\n    #     accessModes: [\"ReadWriteOnce\"]\r\n    #     resources:\r\n    #       requests:\r\n    #         storage: 50Gi\r\n    #   selector: {}\r\n\r\n\r\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\r\n    ##\r\n    externalUrl:\r\n\r\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\r\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\r\n    ##\r\n    routePrefix: /\r\n\r\n    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.\r\n    scheme: \"\"\r\n\r\n    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.\r\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\r\n    tlsConfig: {}\r\n\r\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\r\n    ##\r\n    paused: false\r\n\r\n    ## Define which Nodes the Pods are scheduled on.\r\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\r\n    ##\r\n    nodeSelector: {}\r\n\r\n    ## Define resources requests and limits for single Pods.\r\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\r\n    ##\r\n    resources: {}\r\n    # requests:\r\n    #   memory: 400Mi\r\n\r\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\r\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\r\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\r\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\r\n    ##\r\n    podAntiAffinity: \"\"\r\n\r\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\r\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\r\n    ##\r\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\r\n\r\n    ## Assign custom affinity rules to the alertmanager instance\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\r\n    ##\r\n    affinity: {}\r\n    # nodeAffinity:\r\n    #   requiredDuringSchedulingIgnoredDuringExecution:\r\n    #     nodeSelectorTerms:\r\n    #     - matchExpressions:\r\n    #       - key: kubernetes.io/e2e-az-name\r\n    #         operator: In\r\n    #         values:\r\n    #         - e2e-az1\r\n    #         - e2e-az2\r\n\r\n    ## If specified, the pod's tolerations.\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\r\n    ##\r\n    tolerations: []\r\n    # - key: \"key\"\r\n    #   operator: \"Equal\"\r\n    #   value: \"value\"\r\n    #   effect: \"NoSchedule\"\r\n\r\n    ## If specified, the pod's topology spread constraints.\r\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\r\n    ##\r\n    topologySpreadConstraints: []\r\n    # - maxSkew: 1\r\n    #   topologyKey: topology.kubernetes.io/zone\r\n    #   whenUnsatisfiable: DoNotSchedule\r\n    #   labelSelector:\r\n    #     matchLabels:\r\n    #       app: alertmanager\r\n\r\n    ## SecurityContext holds pod-level security attributes and common container settings.\r\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\r\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n    ##\r\n    securityContext:\r\n      runAsGroup: 2000\r\n      runAsNonRoot: true\r\n      runAsUser: 1000\r\n      fsGroup: 2000\r\n      seccompProfile:\r\n        type: RuntimeDefault\r\n\r\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\r\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\r\n    ##\r\n    listenLocal: false\r\n\r\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\r\n    ##\r\n    containers: []\r\n    # containers:\r\n    # - name: oauth-proxy\r\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\r\n    #   args:\r\n    #   - --upstream=http://127.0.0.1:9093\r\n    #   - --http-address=0.0.0.0:8081\r\n    #   - --metrics-address=0.0.0.0:8082\r\n    #   - ...\r\n    #   ports:\r\n    #   - containerPort: 8081\r\n    #     name: oauth-proxy\r\n    #     protocol: TCP\r\n    #   - containerPort: 8082\r\n    #     name: oauth-metrics\r\n    #     protocol: TCP\r\n    #   resources: {}\r\n\r\n    # Additional volumes on the output StatefulSet definition.\r\n    volumes: []\r\n\r\n    # Additional VolumeMounts on the output StatefulSet definition.\r\n    volumeMounts: []\r\n\r\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\r\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\r\n    initContainers: []\r\n\r\n    ## Priority class assigned to the Pods\r\n    ##\r\n    priorityClassName: \"\"\r\n\r\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\r\n    ##\r\n    additionalPeers: []\r\n\r\n    ## PortName to use for Alert Manager.\r\n    ##\r\n    portName: \"http-web\"\r\n\r\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\r\n    ##\r\n    clusterAdvertiseAddress: false\r\n\r\n    ## clusterGossipInterval determines interval between gossip attempts.\r\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\r\n    clusterGossipInterval: \"\"\r\n\r\n    ## clusterPeerTimeout determines timeout for cluster peering.\r\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\r\n    clusterPeerTimeout: \"\"\r\n\r\n    ## clusterPushpullInterval determines interval between pushpull attempts.\r\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\r\n    clusterPushpullInterval: \"\"\r\n\r\n    ## clusterLabel defines the identifier that uniquely identifies the Alertmanager cluster.\r\n    clusterLabel: \"\"\r\n\r\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\r\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\r\n    forceEnableClusterMode: false\r\n\r\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\r\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\r\n    minReadySeconds: 0\r\n\r\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\r\n    additionalConfig: {}\r\n\r\n    ## Additional configuration which is not covered by the properties above.\r\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\r\n    ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)\r\n    additionalConfigString: \"\"\r\n\r\n  ## ExtraSecret can be used to store various data in an extra secret\r\n  ## (use it for example to store hashed basic auth credentials)\r\n  extraSecret:\r\n    ## if not set, name will be auto generated\r\n    # name: \"\"\r\n    annotations: {}\r\n    data: {}\r\n  #   auth: |\r\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\r\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\r\n\r\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\r\n##\r\ngrafana:\r\n  enabled: true\r\n  namespaceOverride: \"sre-challenge\"\r\n\r\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\r\n  ##\r\n  forceDeployDatasources: false\r\n\r\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\r\n  ##\r\n  forceDeployDashboards: false\r\n\r\n  ## Deploy default dashboards\r\n  ##\r\n  defaultDashboardsEnabled: true\r\n\r\n  ## Timezone for the default dashboards\r\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\r\n  ##\r\n  defaultDashboardsTimezone: utc\r\n\r\n  ## Editable flag for the default dashboards\r\n  ##\r\n  defaultDashboardsEditable: true\r\n\r\n  adminPassword: prom-operator\r\n\r\n  rbac:\r\n    ## If true, Grafana PSPs will be created\r\n    ##\r\n    pspEnabled: false\r\n\r\n  ingress:\r\n    ## If true, Grafana Ingress will be created\r\n    ##\r\n    enabled: false\r\n\r\n    ## IngressClassName for Grafana Ingress.\r\n    ## Should be provided if Ingress is enable.\r\n    ##\r\n    # ingressClassName: nginx\r\n\r\n    ## Annotations for Grafana Ingress\r\n    ##\r\n    annotations: {}\r\n      # kubernetes.io/ingress.class: nginx\r\n      # kubernetes.io/tls-acme: \"true\"\r\n\r\n    ## Labels to be added to the Ingress\r\n    ##\r\n    labels: {}\r\n\r\n    ## Hostnames.\r\n    ## Must be provided if Ingress is enable.\r\n    ##\r\n    # hosts:\r\n    #   - grafana.domain.com\r\n    hosts: []\r\n\r\n    ## Path for grafana ingress\r\n    path: /\r\n\r\n    ## TLS configuration for grafana Ingress\r\n    ## Secret must be manually created in the namespace\r\n    ##\r\n    tls: []\r\n    # - secretName: grafana-general-tls\r\n    #   hosts:\r\n    #   - grafana.example.com\r\n\r\n  # # To make Grafana persistent (Using Statefulset)\r\n  # #\r\n  # persistence:\r\n  #   enabled: true\r\n  #   type: sts\r\n  #   storageClassName: \"storageClassName\"\r\n  #   accessModes:\r\n  #     - ReadWriteOnce\r\n  #   size: 20Gi\r\n  #   finalizers:\r\n  #     - kubernetes.io/pvc-protection\r\n\r\n  serviceAccount:\r\n    create: true\r\n    autoMount: true\r\n\r\n  sidecar:\r\n    dashboards:\r\n      enabled: true\r\n      label: grafana_dashboard\r\n      labelValue: \"1\"\r\n      # Allow discovery in all namespaces for dashboards\r\n      searchNamespace: ALL\r\n\r\n      # Support for new table panels, when enabled grafana auto migrates the old table panels to newer table panels\r\n      enableNewTablePanelSyntax: false\r\n\r\n      ## Annotations for Grafana dashboard configmaps\r\n      ##\r\n      annotations: {}\r\n      multicluster:\r\n        global:\r\n          enabled: false\r\n        etcd:\r\n          enabled: false\r\n      provider:\r\n        allowUiUpdates: false\r\n    datasources:\r\n      enabled: true\r\n      defaultDatasourceEnabled: true\r\n      isDefaultDatasource: true\r\n\r\n      name: Prometheus\r\n      uid: prometheus\r\n\r\n      ## URL of prometheus datasource\r\n      ##\r\n      # url: http://prometheus-stack-prometheus:9090/\r\n\r\n      ## Prometheus request timeout in seconds\r\n      # timeout: 30\r\n\r\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\r\n      # defaultDatasourceScrapeInterval: 15s\r\n\r\n      ## Annotations for Grafana datasource configmaps\r\n      ##\r\n      annotations: {}\r\n\r\n      ## Set method for HTTP to send query to datasource\r\n      httpMethod: POST\r\n\r\n      ## Create datasource for each Pod of Prometheus StatefulSet;\r\n      ## this uses headless service `prometheus-operated` which is\r\n      ## created by Prometheus Operator\r\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\r\n      createPrometheusReplicasDatasources: false\r\n      label: grafana_datasource\r\n      labelValue: \"1\"\r\n\r\n      ## Field with internal link pointing to existing data source in Grafana.\r\n      ## Can be provisioned via additionalDataSources\r\n      exemplarTraceIdDestinations: {}\r\n        # datasourceUid: Jaeger\r\n        # traceIdLabelName: trace_id\r\n      alertmanager:\r\n        enabled: true\r\n        name: Alertmanager\r\n        uid: alertmanager\r\n        handleGrafanaManagedAlerts: false\r\n        implementation: prometheus\r\n\r\n  extraConfigmapMounts: []\r\n  # - name: certs-configmap\r\n  #   mountPath: /etc/grafana/ssl/\r\n  #   configMap: certs-configmap\r\n  #   readOnly: true\r\n\r\n  deleteDatasources: []\r\n  # - name: example-datasource\r\n  #   orgId: 1\r\n\r\n  ## Configure additional grafana datasources (passed through tpl)\r\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\r\n  additionalDataSources: []\r\n  # - name: prometheus-sample\r\n  #   access: proxy\r\n  #   basicAuth: true\r\n  #   basicAuthPassword: pass\r\n  #   basicAuthUser: daco\r\n  #   editable: false\r\n  #   jsonData:\r\n  #       tlsSkipVerify: true\r\n  #   orgId: 1\r\n  #   type: prometheus\r\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\r\n  #   version: 1\r\n\r\n  # Flag to mark provisioned data sources for deletion if they are no longer configured.\r\n  # It takes no effect if data sources are already listed in the deleteDatasources section.\r\n  # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file\r\n  prune: false\r\n\r\n  ## Passed to grafana subchart and used by servicemonitor below\r\n  ##\r\n  service:\r\n    portName: http-web\r\n    ipFamilies: []\r\n    ipFamilyPolicy: \"\"\r\n\r\n  serviceMonitor:\r\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\r\n    # https://github.com/coreos/prometheus-operator\r\n    #\r\n    enabled: true\r\n\r\n    # Path to use for scraping metrics. Might be different if server.root_url is set\r\n    # in grafana.ini\r\n    path: \"/metrics\"\r\n\r\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\r\n\r\n    # labels for the ServiceMonitor\r\n    labels: {}\r\n\r\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    #\r\n    interval: \"\"\r\n    scheme: http\r\n    tlsConfig: {}\r\n    scrapeTimeout: 30s\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n## Flag to disable all the kubernetes component scrapers\r\n##\r\nkubernetesServiceMonitors:\r\n  enabled: true\r\n\r\n## Component scraping the kube api server\r\n##\r\nkubeApiServer:\r\n  enabled: true\r\n  tlsConfig:\r\n    serverName: kubernetes\r\n    insecureSkipVerify: false\r\n  serviceMonitor:\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    jobLabel: component\r\n    selector:\r\n      matchLabels:\r\n        component: apiserver\r\n        provider: kubernetes\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings:\r\n      # Drop excessively noisy apiserver buckets.\r\n      - action: drop\r\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\r\n        sourceLabels:\r\n          - __name__\r\n          - le\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels:\r\n    #     - __meta_kubernetes_namespace\r\n    #     - __meta_kubernetes_service_name\r\n    #     - __meta_kubernetes_endpoint_port_name\r\n    #   action: keep\r\n    #   regex: default;kubernetes;https\r\n    # - targetLabel: __address__\r\n    #   replacement: kubernetes.default.svc:443\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping the kubelet and kubelet-hosted cAdvisor\r\n##\r\nkubelet:\r\n  enabled: true\r\n  namespace: kube-system\r\n\r\n  serviceMonitor:\r\n    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.\r\n    ##\r\n    attachMetadata:\r\n      node: false\r\n\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## If true, Prometheus use (respect) labels provided by exporter.\r\n    ##\r\n    honorLabels: true\r\n\r\n    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.\r\n    ##\r\n    honorTimestamps: true\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## Enable scraping the kubelet over https. For requirements to enable this see\r\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\r\n    ##\r\n    https: true\r\n\r\n    ## Skip TLS certificate validation when scraping.\r\n    ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed\r\n    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs\r\n    ##\r\n    insecureSkipVerify: true\r\n\r\n    ## Enable scraping /metrics/cadvisor from kubelet's service\r\n    ##\r\n    cAdvisor: true\r\n\r\n    ## Enable scraping /metrics/probes from kubelet's service\r\n    ##\r\n    probes: true\r\n\r\n    ## Enable scraping /metrics/resource from kubelet's service\r\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\r\n    ##\r\n    resource: false\r\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\r\n    resourcePath: \"/metrics/resource/v1alpha1\"\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    cAdvisorMetricRelabelings:\r\n      # Drop less useful container CPU metrics.\r\n      - sourceLabels: [__name__]\r\n        action: drop\r\n        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'\r\n      # Drop less useful container / always zero filesystem metrics.\r\n      - sourceLabels: [__name__]\r\n        action: drop\r\n        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'\r\n      # Drop less useful / always zero container memory metrics.\r\n      - sourceLabels: [__name__]\r\n        action: drop\r\n        regex: 'container_memory_(mapped_file|swap)'\r\n      # Drop less useful container process metrics.\r\n      - sourceLabels: [__name__]\r\n        action: drop\r\n        regex: 'container_(file_descriptors|tasks_state|threads_max)'\r\n      # Drop container spec metrics that overlap with kube-state-metrics.\r\n      - sourceLabels: [__name__]\r\n        action: drop\r\n        regex: 'container_spec.*'\r\n      # Drop cgroup metrics with no pod.\r\n      - sourceLabels: [id, pod]\r\n        action: drop\r\n        regex: '.+;'\r\n    # - sourceLabels: [__name__, image]\r\n    #   separator: ;\r\n    #   regex: container_([a-z_]+);\r\n    #   replacement: $1\r\n    #   action: drop\r\n    # - sourceLabels: [__name__]\r\n    #   separator: ;\r\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\r\n    #   replacement: $1\r\n    #   action: drop\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    probesMetricRelabelings: []\r\n    # - sourceLabels: [__name__, image]\r\n    #   separator: ;\r\n    #   regex: container_([a-z_]+);\r\n    #   replacement: $1\r\n    #   action: drop\r\n    # - sourceLabels: [__name__]\r\n    #   separator: ;\r\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\r\n    #   replacement: $1\r\n    #   action: drop\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    ## metrics_path is required to match upstream rules and charts\r\n    cAdvisorRelabelings:\r\n      - action: replace\r\n        sourceLabels: [__metrics_path__]\r\n        targetLabel: metrics_path\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    probesRelabelings:\r\n      - action: replace\r\n        sourceLabels: [__metrics_path__]\r\n        targetLabel: metrics_path\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    resourceRelabelings:\r\n      - action: replace\r\n        sourceLabels: [__metrics_path__]\r\n        targetLabel: metrics_path\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - sourceLabels: [__name__, image]\r\n    #   separator: ;\r\n    #   regex: container_([a-z_]+);\r\n    #   replacement: $1\r\n    #   action: drop\r\n    # - sourceLabels: [__name__]\r\n    #   separator: ;\r\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\r\n    #   replacement: $1\r\n    #   action: drop\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    ## metrics_path is required to match upstream rules and charts\r\n    relabelings:\r\n      - action: replace\r\n        sourceLabels: [__metrics_path__]\r\n        targetLabel: metrics_path\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping the kube controller manager\r\n##\r\nkubeControllerManager:\r\n  enabled: true\r\n\r\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\r\n  ##\r\n  endpoints: []\r\n  # - 10.141.4.22\r\n  # - 10.141.4.23\r\n  # - 10.141.4.24\r\n\r\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\r\n  ##\r\n  service:\r\n    enabled: true\r\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\r\n    ## of default port in Kubernetes 1.22.\r\n    ##\r\n    port: null\r\n    targetPort: null\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   component: kube-controller-manager\r\n\r\n  serviceMonitor:\r\n    enabled: true\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## port: Name of the port the metrics will be scraped from\r\n    ##\r\n    port: http-metrics\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    component: kube-controller-manager\r\n\r\n    ## Enable scraping kube-controller-manager over https.\r\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\r\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\r\n    ##\r\n    https: null\r\n\r\n    # Skip TLS certificate validation when scraping\r\n    insecureSkipVerify: null\r\n\r\n    # Name of the server to use when validating TLS certificate\r\n    serverName: null\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping coreDns. Use either this or kubeDns\r\n##\r\ncoreDns:\r\n  enabled: true\r\n  service:\r\n    enabled: true\r\n    port: 9153\r\n    targetPort: 9153\r\n\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   k8s-app: kube-dns\r\n  serviceMonitor:\r\n    enabled: true\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## port: Name of the port the metrics will be scraped from\r\n    ##\r\n    port: http-metrics\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    k8s-app: kube-dns\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping kubeDns. Use either this or coreDns\r\n##\r\nkubeDns:\r\n  enabled: false\r\n  service:\r\n    dnsmasq:\r\n      port: 10054\r\n      targetPort: 10054\r\n    skydns:\r\n      port: 10055\r\n      targetPort: 10055\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   k8s-app: kube-dns\r\n  serviceMonitor:\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    k8s-app: kube-dns\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    dnsmasqMetricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    dnsmasqRelabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping etcd\r\n##\r\nkubeEtcd:\r\n  enabled: true\r\n\r\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\r\n  ##\r\n  endpoints: []\r\n  # - 10.141.4.22\r\n  # - 10.141.4.23\r\n  # - 10.141.4.24\r\n\r\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\r\n  ##\r\n  service:\r\n    enabled: true\r\n    port: 2381\r\n    targetPort: 2381\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   component: etcd\r\n\r\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\r\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\r\n  ##\r\n  ## serviceMonitor:\r\n  ##   scheme: https\r\n  ##   insecureSkipVerify: false\r\n  ##   serverName: localhost\r\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\r\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\r\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\r\n  ##\r\n  serviceMonitor:\r\n    enabled: true\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n    scheme: http\r\n    insecureSkipVerify: false\r\n    serverName: \"\"\r\n    caFile: \"\"\r\n    certFile: \"\"\r\n    keyFile: \"\"\r\n\r\n    ## port: Name of the port the metrics will be scraped from\r\n    ##\r\n    port: http-metrics\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    component: etcd\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping kube scheduler\r\n##\r\nkubeScheduler:\r\n  enabled: true\r\n\r\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\r\n  ##\r\n  endpoints: []\r\n  # - 10.141.4.22\r\n  # - 10.141.4.23\r\n  # - 10.141.4.24\r\n\r\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\r\n  ##\r\n  service:\r\n    enabled: true\r\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\r\n    ## of default port in Kubernetes 1.23.\r\n    ##\r\n    port: null\r\n    targetPort: null\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   component: kube-scheduler\r\n\r\n  serviceMonitor:\r\n    enabled: true\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n    ## Enable scraping kube-scheduler over https.\r\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\r\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\r\n    ##\r\n    https: null\r\n\r\n    ## port: Name of the port the metrics will be scraped from\r\n    ##\r\n    port: http-metrics\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    component: kube-scheduler\r\n\r\n    ## Skip TLS certificate validation when scraping\r\n    insecureSkipVerify: null\r\n\r\n    ## Name of the server to use when validating TLS certificate\r\n    serverName: null\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping kube proxy\r\n##\r\nkubeProxy:\r\n  enabled: true\r\n\r\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\r\n  ##\r\n  endpoints: []\r\n  # - 10.141.4.22\r\n  # - 10.141.4.23\r\n  # - 10.141.4.24\r\n\r\n  service:\r\n    enabled: true\r\n    port: 10249\r\n    targetPort: 10249\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    # selector:\r\n    #   k8s-app: kube-proxy\r\n\r\n  serviceMonitor:\r\n    enabled: true\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## port: Name of the port the metrics will be scraped from\r\n    ##\r\n    port: http-metrics\r\n\r\n    jobLabel: jobLabel\r\n    selector: {}\r\n    #  matchLabels:\r\n    #    k8s-app: kube-proxy\r\n\r\n    ## Enable scraping kube-proxy over https.\r\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\r\n    ##\r\n    https: false\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n    #  foo: bar\r\n\r\n## Component scraping kube state metrics\r\n##\r\nkubeStateMetrics:\r\n  enabled: true\r\n\r\n## Configuration for kube-state-metrics subchart\r\n##\r\nkube-state-metrics:\r\n  namespaceOverride: \"sre-challenge\"\r\n  rbac:\r\n    create: true\r\n  releaseLabel: true\r\n  prometheus:\r\n    monitor:\r\n      enabled: true\r\n\r\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n      ##\r\n      interval: \"\"\r\n\r\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n      ##\r\n      sampleLimit: 0\r\n\r\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n      ##\r\n      targetLimit: 0\r\n\r\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelLimit: 0\r\n\r\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelNameLengthLimit: 0\r\n\r\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelValueLengthLimit: 0\r\n\r\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\r\n      ##\r\n      scrapeTimeout: \"\"\r\n\r\n      ## proxyUrl: URL of a proxy that should be used for scraping.\r\n      ##\r\n      proxyUrl: \"\"\r\n\r\n      # Keep labels from scraped data, overriding server-side labels\r\n      ##\r\n      honorLabels: true\r\n\r\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n      ##\r\n      metricRelabelings: []\r\n      # - action: keep\r\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n      #   sourceLabels: [__name__]\r\n\r\n      ## RelabelConfigs to apply to samples before scraping\r\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n      ##\r\n      relabelings: []\r\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n      #   separator: ;\r\n      #   regex: ^(.*)$\r\n      #   targetLabel: nodename\r\n      #   replacement: $1\r\n      #   action: replace\r\n\r\n  selfMonitor:\r\n    enabled: false\r\n\r\n## Deploy node exporter as a daemonset to all nodes\r\n##\r\nnodeExporter:\r\n  enabled: true\r\n  operatingSystems:\r\n    linux:\r\n      enabled: true\r\n    darwin:\r\n      enabled: true\r\n\r\n  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled\r\n  ##\r\n  forceDeployDashboards: false\r\n\r\n## Configuration for prometheus-node-exporter subchart\r\n##\r\nprometheus-node-exporter:\r\n  namespaceOverride: \"sre-challenge\"\r\n  podLabels:\r\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\r\n    ##\r\n    jobLabel: node-exporter\r\n  releaseLabel: true\r\n  extraArgs:\r\n    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\r\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\r\n  service:\r\n    portName: http-metrics\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n    labels:\r\n      jobLabel: node-exporter\r\n\r\n  prometheus:\r\n    monitor:\r\n      enabled: true\r\n\r\n      jobLabel: jobLabel\r\n\r\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n      ##\r\n      interval: \"\"\r\n\r\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n      ##\r\n      sampleLimit: 0\r\n\r\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n      ##\r\n      targetLimit: 0\r\n\r\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelLimit: 0\r\n\r\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelNameLengthLimit: 0\r\n\r\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n      ##\r\n      labelValueLengthLimit: 0\r\n\r\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\r\n      ##\r\n      scrapeTimeout: \"\"\r\n\r\n      ## proxyUrl: URL of a proxy that should be used for scraping.\r\n      ##\r\n      proxyUrl: \"\"\r\n\r\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n      ##\r\n      metricRelabelings: []\r\n      # - sourceLabels: [__name__]\r\n      #   separator: ;\r\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\r\n      #   replacement: $1\r\n      #   action: drop\r\n\r\n      ## RelabelConfigs to apply to samples before scraping\r\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n      ##\r\n      relabelings: []\r\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n      #   separator: ;\r\n      #   regex: ^(.*)$\r\n      #   targetLabel: nodename\r\n      #   replacement: $1\r\n      #   action: replace\r\n  rbac:\r\n    ## If true, create PSPs for node-exporter\r\n    ##\r\n    pspEnabled: false\r\n\r\n## Manages Prometheus and Alertmanager components\r\n##\r\nprometheusOperator:\r\n  enabled: true\r\n\r\n  ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-operator' by default\r\n  fullnameOverride: \"\"\r\n\r\n  ## Number of old replicasets to retain ##\r\n  ## The default value is 10, 0 will garbage-collect old replicasets ##\r\n  revisionHistoryLimit: 10\r\n\r\n  ## Strategy of the deployment\r\n  ##\r\n  strategy: {}\r\n\r\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\r\n  ##\r\n  tls:\r\n    enabled: true\r\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\r\n    tlsMinVersion: VersionTLS13\r\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\r\n    internalPort: 10250\r\n\r\n  ## Liveness probe for the prometheusOperator deployment\r\n  ##\r\n  livenessProbe:\r\n    enabled: true\r\n    failureThreshold: 3\r\n    initialDelaySeconds: 0\r\n    periodSeconds: 10\r\n    successThreshold: 1\r\n    timeoutSeconds: 1\r\n  ## Readiness probe for the prometheusOperator deployment\r\n  ##\r\n  readinessProbe:\r\n    enabled: true\r\n    failureThreshold: 3\r\n    initialDelaySeconds: 0\r\n    periodSeconds: 10\r\n    successThreshold: 1\r\n    timeoutSeconds: 1\r\n\r\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\r\n  ## rules from making their way into prometheus and potentially preventing the container from starting\r\n  admissionWebhooks:\r\n    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly\r\n    ## IgnoreOnInstallOnly - If Release.IsInstall returns \"true\", set \"Ignore\" otherwise \"Fail\"\r\n    failurePolicy: \"\"\r\n    ## The default timeoutSeconds is 10 and the maximum value is 30.\r\n    timeoutSeconds: 10\r\n    enabled: true\r\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\r\n    ## If unspecified, system trust roots on the apiserver are used.\r\n    caBundle: \"\"\r\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\r\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\r\n    ## certs ahead of time if you wish.\r\n    ##\r\n    annotations: {}\r\n    #   argocd.argoproj.io/hook: PreSync\r\n    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\r\n\r\n    namespaceSelector: {}\r\n    objectSelector: {}\r\n\r\n    mutatingWebhookConfiguration:\r\n      annotations: {}\r\n      #   argocd.argoproj.io/hook: PreSync\r\n\r\n    validatingWebhookConfiguration:\r\n      annotations: {}\r\n      #   argocd.argoproj.io/hook: PreSync\r\n\r\n    deployment:\r\n      enabled: false\r\n\r\n      ## Number of replicas\r\n      ##\r\n      replicas: 1\r\n\r\n      ## Strategy of the deployment\r\n      ##\r\n      strategy: {}\r\n\r\n      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\r\n      podDisruptionBudget: {}\r\n        # maxUnavailable: 1\r\n        # minAvailable: 1\r\n\r\n      ## Number of old replicasets to retain ##\r\n      ## The default value is 10, 0 will garbage-collect old replicasets ##\r\n      revisionHistoryLimit: 10\r\n\r\n      ## Prometheus-Operator v0.39.0 and later support TLS natively.\r\n      ##\r\n      tls:\r\n        enabled: true\r\n        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\r\n        tlsMinVersion: VersionTLS13\r\n        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\r\n        internalPort: 10250\r\n\r\n      ## Service account for Prometheus Operator Webhook to use.\r\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n      ##\r\n      serviceAccount:\r\n        annotations: {}\r\n        automountServiceAccountToken: false\r\n        create: true\r\n        name: \"\"\r\n\r\n      ## Configuration for Prometheus operator Webhook service\r\n      ##\r\n      service:\r\n        annotations: {}\r\n        labels: {}\r\n        clusterIP: \"\"\r\n        ipDualStack:\r\n          enabled: false\r\n          ipFamilies: [\"IPv6\", \"IPv4\"]\r\n          ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n        ## Port to expose on each node\r\n        ## Only used if service.type is 'NodePort'\r\n        ##\r\n        nodePort: 31080\r\n\r\n        nodePortTls: 31443\r\n\r\n        ## Additional ports to open for Prometheus operator Webhook service\r\n        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\r\n        ##\r\n        additionalPorts: []\r\n\r\n        ## Loadbalancer IP\r\n        ## Only use if service.type is \"LoadBalancer\"\r\n        ##\r\n        loadBalancerIP: \"\"\r\n        loadBalancerSourceRanges: []\r\n\r\n        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n        ##\r\n        externalTrafficPolicy: Cluster\r\n\r\n        ## Service type\r\n        ## NodePort, ClusterIP, LoadBalancer\r\n        ##\r\n        type: ClusterIP\r\n\r\n        ## List of IP addresses at which the Prometheus server service is available\r\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\r\n        ##\r\n        externalIPs: []\r\n\r\n      # ## Labels to add to the operator webhook deployment\r\n      # ##\r\n      labels: {}\r\n\r\n      ## Annotations to add to the operator webhook deployment\r\n      ##\r\n      annotations: {}\r\n\r\n      ## Labels to add to the operator webhook pod\r\n      ##\r\n      podLabels: {}\r\n\r\n      ## Annotations to add to the operator webhook pod\r\n      ##\r\n      podAnnotations: {}\r\n\r\n      ## Assign a PriorityClassName to pods if set\r\n      # priorityClassName: \"\"\r\n\r\n      ## Define Log Format\r\n      # Use logfmt (default) or json logging\r\n      # logFormat: logfmt\r\n\r\n      ## Decrease log verbosity to errors only\r\n      # logLevel: error\r\n\r\n      ## Prometheus-operator webhook image\r\n      ##\r\n      image:\r\n        registry: quay.io\r\n        repository: prometheus-operator/admission-webhook\r\n        # if not set appVersion field from Chart.yaml is used\r\n        tag: \"\"\r\n        sha: \"\"\r\n        pullPolicy: IfNotPresent\r\n\r\n      ## Define Log Format\r\n      # Use logfmt (default) or json logging\r\n      # logFormat: logfmt\r\n\r\n      ## Decrease log verbosity to errors only\r\n      # logLevel: error\r\n\r\n\r\n      ## Liveness probe\r\n      ##\r\n      livenessProbe:\r\n        enabled: true\r\n        failureThreshold: 3\r\n        initialDelaySeconds: 30\r\n        periodSeconds: 10\r\n        successThreshold: 1\r\n        timeoutSeconds: 1\r\n\r\n      ## Readiness probe\r\n      ##\r\n      readinessProbe:\r\n        enabled: true\r\n        failureThreshold: 3\r\n        initialDelaySeconds: 5\r\n        periodSeconds: 10\r\n        successThreshold: 1\r\n        timeoutSeconds: 1\r\n\r\n      ## Resource limits \u0026 requests\r\n      ##\r\n      resources: {}\r\n      # limits:\r\n      #   cpu: 200m\r\n      #   memory: 200Mi\r\n      # requests:\r\n      #   cpu: 100m\r\n      #   memory: 100Mi\r\n\r\n      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\r\n      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\r\n      ##\r\n      hostNetwork: false\r\n\r\n      ## Define which Nodes the Pods are scheduled on.\r\n      ## ref: https://kubernetes.io/docs/user-guide/node-selection/\r\n      ##\r\n      nodeSelector: {}\r\n\r\n      ## Tolerations for use with node taints\r\n      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\r\n      ##\r\n      tolerations: []\r\n      # - key: \"key\"\r\n      #   operator: \"Equal\"\r\n      #   value: \"value\"\r\n      #   effect: \"NoSchedule\"\r\n\r\n      ## Assign custom affinity rules to the prometheus operator\r\n      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\r\n      ##\r\n      affinity: {}\r\n        # nodeAffinity:\r\n        #   requiredDuringSchedulingIgnoredDuringExecution:\r\n        #     nodeSelectorTerms:\r\n        #     - matchExpressions:\r\n        #       - key: kubernetes.io/e2e-az-name\r\n        #         operator: In\r\n        #         values:\r\n        #         - e2e-az1\r\n      #         - e2e-az2\r\n      dnsConfig: {}\r\n        # nameservers:\r\n        #   - 1.2.3.4\r\n        # searches:\r\n        #   - ns1.svc.cluster-domain.example\r\n        #   - my.dns.search.suffix\r\n        # options:\r\n        #   - name: ndots\r\n        #     value: \"2\"\r\n        #   - name: edns0\r\n      securityContext:\r\n        fsGroup: 65534\r\n        runAsGroup: 65534\r\n        runAsNonRoot: true\r\n        runAsUser: 65534\r\n        seccompProfile:\r\n          type: RuntimeDefault\r\n\r\n      ## Container-specific security context configuration\r\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n      ##\r\n      containerSecurityContext:\r\n        allowPrivilegeEscalation: false\r\n        readOnlyRootFilesystem: true\r\n        capabilities:\r\n          drop:\r\n            - ALL\r\n\r\n      ## If false then the user will opt out of automounting API credentials.\r\n      ##\r\n      automountServiceAccountToken: true\r\n\r\n    patch:\r\n      enabled: true\r\n      image:\r\n        registry: registry.k8s.io\r\n        repository: ingress-nginx/kube-webhook-certgen\r\n        tag: v20221220-controller-v1.5.1-58-g787ea74b6\r\n        sha: \"\"\r\n        pullPolicy: IfNotPresent\r\n      resources: {}\r\n      ## Provide a priority class name to the webhook patching job\r\n      ##\r\n      priorityClassName: \"\"\r\n      ttlSecondsAfterFinished: 60\r\n      annotations: {}\r\n      #   argocd.argoproj.io/hook: PreSync\r\n      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\r\n      podAnnotations: {}\r\n      nodeSelector: {}\r\n      affinity: {}\r\n      tolerations: []\r\n\r\n      ## SecurityContext holds pod-level security attributes and common container settings.\r\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\r\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n      ##\r\n      securityContext:\r\n        runAsGroup: 2000\r\n        runAsNonRoot: true\r\n        runAsUser: 2000\r\n        seccompProfile:\r\n          type: RuntimeDefault\r\n      ## Service account for Prometheus Operator Webhook Job Patch to use.\r\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n      ##\r\n      serviceAccount:\r\n        create: true\r\n        annotations: {}\r\n        automountServiceAccountToken: true\r\n\r\n    # Security context for create job container\r\n    createSecretJob:\r\n      securityContext:\r\n        allowPrivilegeEscalation: false\r\n        readOnlyRootFilesystem: true\r\n        capabilities:\r\n          drop:\r\n          - ALL\r\n\r\n      # Security context for patch job container\r\n    patchWebhookJob:\r\n      securityContext:\r\n        allowPrivilegeEscalation: false\r\n        readOnlyRootFilesystem: true\r\n        capabilities:\r\n          drop:\r\n          - ALL\r\n\r\n    # Use certmanager to generate webhook certs\r\n    certManager:\r\n      enabled: false\r\n      # self-signed root certificate\r\n      rootCert:\r\n        duration: \"\"  # default to be 5y\r\n      admissionCert:\r\n        duration: \"\"  # default to be 1y\r\n      # issuerRef:\r\n      #   name: \"issuer\"\r\n      #   kind: \"ClusterIssuer\"\r\n\r\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\r\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\r\n  ##\r\n  namespaces: {}\r\n    # releaseNamespace: true\r\n    # additional:\r\n    # - kube-system\r\n\r\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\r\n  ##\r\n  denyNamespaces: []\r\n\r\n  ## Filter namespaces to look for prometheus-operator custom resources\r\n  ##\r\n  alertmanagerInstanceNamespaces: []\r\n  alertmanagerConfigNamespaces: []\r\n  prometheusInstanceNamespaces: []\r\n  thanosRulerInstanceNamespaces: []\r\n\r\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\r\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\r\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\r\n  ##\r\n  # clusterDomain: \"cluster.local\"\r\n\r\n  networkPolicy:\r\n    ## Enable creation of NetworkPolicy resources.\r\n    ##\r\n    enabled: false\r\n\r\n    ## Flavor of the network policy to use.\r\n    #  Can be:\r\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\r\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\r\n    flavor: kubernetes\r\n\r\n    # cilium:\r\n    #   egress:\r\n\r\n    ## match labels used in selector\r\n    # matchLabels: {}\r\n\r\n  ## Service account for Prometheus Operator to use.\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n  ##\r\n  serviceAccount:\r\n    create: true\r\n    name: \"\"\r\n    automountServiceAccountToken: true\r\n    annotations: {}\r\n\r\n  ## Configuration for Prometheus operator service\r\n  ##\r\n  service:\r\n    annotations: {}\r\n    labels: {}\r\n    clusterIP: \"\"\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n  ## Port to expose on each node\r\n  ## Only used if service.type is 'NodePort'\r\n  ##\r\n    nodePort: 30080\r\n\r\n    nodePortTls: 30443\r\n\r\n  ## Additional ports to open for Prometheus operator service\r\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\r\n  ##\r\n    additionalPorts: []\r\n\r\n  ## Loadbalancer IP\r\n  ## Only use if service.type is \"LoadBalancer\"\r\n  ##\r\n    loadBalancerIP: \"\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n  ## Service type\r\n  ## NodePort, ClusterIP, LoadBalancer\r\n  ##\r\n    type: ClusterIP\r\n\r\n    ## List of IP addresses at which the Prometheus server service is available\r\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\r\n    ##\r\n    externalIPs: []\r\n\r\n  # ## Labels to add to the operator deployment\r\n  # ##\r\n  labels: {}\r\n\r\n  ## Annotations to add to the operator deployment\r\n  ##\r\n  annotations: {}\r\n\r\n  ## Labels to add to the operator pod\r\n  ##\r\n  podLabels: {}\r\n\r\n  ## Annotations to add to the operator pod\r\n  ##\r\n  podAnnotations: {}\r\n\r\n  ## Assign a PriorityClassName to pods if set\r\n  # priorityClassName: \"\"\r\n\r\n  ## Define Log Format\r\n  # Use logfmt (default) or json logging\r\n  # logFormat: logfmt\r\n\r\n  ## Decrease log verbosity to errors only\r\n  # logLevel: error\r\n\r\n  kubeletService:\r\n    ## If true, the operator will create and maintain a service for scraping kubelets\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\r\n    ##\r\n    enabled: true\r\n    namespace: kube-system\r\n    selector: \"\"\r\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\r\n    name: \"\"\r\n\r\n  ## Create a servicemonitor for the operator\r\n  ##\r\n  serviceMonitor:\r\n    ## If true, create a serviceMonitor for prometheus operator\r\n    ##\r\n    selfMonitor: true\r\n\r\n    ## Labels for ServiceMonitor\r\n    additionalLabels: {}\r\n\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\r\n    scrapeTimeout: \"\"\r\n\r\n    ## Metric relabel configs to apply to samples before ingestion.\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    #   relabel configs to apply to samples before ingestion.\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n  ## Resource limits \u0026 requests\r\n  ##\r\n  resources: {}\r\n  # limits:\r\n  #   cpu: 200m\r\n  #   memory: 200Mi\r\n  # requests:\r\n  #   cpu: 100m\r\n  #   memory: 100Mi\r\n\r\n  ## Operator Environment\r\n  ##  env:\r\n  ##    VARIABLE: value\r\n  env:\r\n    GOGC: \"30\"\r\n\r\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\r\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\r\n  ##\r\n  hostNetwork: false\r\n\r\n  ## Define which Nodes the Pods are scheduled on.\r\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\r\n  ##\r\n  nodeSelector: {}\r\n\r\n  ## Tolerations for use with node taints\r\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\r\n  ##\r\n  tolerations: []\r\n  # - key: \"key\"\r\n  #   operator: \"Equal\"\r\n  #   value: \"value\"\r\n  #   effect: \"NoSchedule\"\r\n\r\n  ## Assign custom affinity rules to the prometheus operator\r\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\r\n  ##\r\n  affinity: {}\r\n    # nodeAffinity:\r\n    #   requiredDuringSchedulingIgnoredDuringExecution:\r\n    #     nodeSelectorTerms:\r\n    #     - matchExpressions:\r\n    #       - key: kubernetes.io/e2e-az-name\r\n    #         operator: In\r\n    #         values:\r\n    #         - e2e-az1\r\n    #         - e2e-az2\r\n  dnsConfig: {}\r\n    # nameservers:\r\n    #   - 1.2.3.4\r\n    # searches:\r\n    #   - ns1.svc.cluster-domain.example\r\n    #   - my.dns.search.suffix\r\n    # options:\r\n    #   - name: ndots\r\n    #     value: \"2\"\r\n  #   - name: edns0\r\n  securityContext:\r\n    fsGroup: 65534\r\n    runAsGroup: 65534\r\n    runAsNonRoot: true\r\n    runAsUser: 65534\r\n    seccompProfile:\r\n      type: RuntimeDefault\r\n\r\n  ## Container-specific security context configuration\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n  ##\r\n  containerSecurityContext:\r\n    allowPrivilegeEscalation: false\r\n    readOnlyRootFilesystem: true\r\n    capabilities:\r\n      drop:\r\n      - ALL\r\n\r\n  # Enable vertical pod autoscaler support for prometheus-operator\r\n  verticalPodAutoscaler:\r\n    enabled: false\r\n\r\n    # Recommender responsible for generating recommendation for the object.\r\n    # List should be empty (then the default recommender will generate the recommendation)\r\n    # or contain exactly one recommender.\r\n    # recommenders:\r\n    # - name: custom-recommender-performance\r\n\r\n    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\r\n    controlledResources: []\r\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\r\n    # controlledValues: RequestsAndLimits\r\n\r\n    # Define the max allowed resources for the pod\r\n    maxAllowed: {}\r\n    # cpu: 200m\r\n    # memory: 100Mi\r\n    # Define the min allowed resources for the pod\r\n    minAllowed: {}\r\n    # cpu: 200m\r\n    # memory: 100Mi\r\n\r\n    updatePolicy:\r\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\r\n      # minReplicas: 1\r\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\r\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\r\n      updateMode: Auto\r\n\r\n  ## Prometheus-operator image\r\n  ##\r\n  image:\r\n    registry: quay.io\r\n    repository: prometheus-operator/prometheus-operator\r\n    # if not set appVersion field from Chart.yaml is used\r\n    tag: \"\"\r\n    sha: \"\"\r\n    pullPolicy: IfNotPresent\r\n\r\n  ## Prometheus image to use for prometheuses managed by the operator\r\n  ##\r\n  # prometheusDefaultBaseImage: prometheus/prometheus\r\n\r\n  ## Prometheus image registry to use for prometheuses managed by the operator\r\n  ##\r\n  # prometheusDefaultBaseImageRegistry: quay.io\r\n\r\n  ## Alertmanager image to use for alertmanagers managed by the operator\r\n  ##\r\n  # alertmanagerDefaultBaseImage: prometheus/alertmanager\r\n\r\n  ## Alertmanager image registry to use for alertmanagers managed by the operator\r\n  ##\r\n  # alertmanagerDefaultBaseImageRegistry: quay.io\r\n\r\n  ## Prometheus-config-reloader\r\n  ##\r\n  prometheusConfigReloader:\r\n    image:\r\n      registry: quay.io\r\n      repository: prometheus-operator/prometheus-config-reloader\r\n      # if not set appVersion field from Chart.yaml is used\r\n      tag: \"\"\r\n      sha: \"\"\r\n\r\n    # add prometheus config reloader liveness and readiness probe. Default: false\r\n    enableProbe: false\r\n\r\n    # resource config for prometheusConfigReloader\r\n    resources: {}\r\n      # requests:\r\n      #   cpu: 200m\r\n      #   memory: 50Mi\r\n      # limits:\r\n      #   cpu: 200m\r\n      #   memory: 50Mi\r\n\r\n  ## Thanos side-car image when configured\r\n  ##\r\n  thanosImage:\r\n    registry: quay.io\r\n    repository: thanos/thanos\r\n    tag: v0.36.1\r\n    sha: \"\"\r\n\r\n  ## Set a Label Selector to filter watched prometheus and prometheusAgent\r\n  ##\r\n  prometheusInstanceSelector: \"\"\r\n\r\n  ## Set a Label Selector to filter watched alertmanager\r\n  ##\r\n  alertmanagerInstanceSelector: \"\"\r\n\r\n  ## Set a Label Selector to filter watched thanosRuler\r\n  thanosRulerInstanceSelector: \"\"\r\n\r\n  ## Set a Field Selector to filter watched secrets\r\n  ##\r\n  secretFieldSelector: \"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\"\r\n\r\n  ## If false then the user will opt out of automounting API credentials.\r\n  ##\r\n  automountServiceAccountToken: true\r\n\r\n  ## Additional volumes\r\n  ##\r\n  extraVolumes: []\r\n\r\n  ## Additional volume mounts\r\n  ##\r\n  extraVolumeMounts: []\r\n\r\n## Deploy a Prometheus instance\r\n##\r\nprometheus:\r\n  enabled: true\r\n\r\n  ## Toggle prometheus into agent mode\r\n  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.\r\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md\r\n  ##\r\n  agentMode: false\r\n\r\n  ## Annotations for Prometheus\r\n  ##\r\n  annotations: {}\r\n\r\n  ## Configure network policy for the prometheus\r\n  networkPolicy:\r\n    enabled: false\r\n\r\n    ## Flavor of the network policy to use.\r\n    #  Can be:\r\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\r\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\r\n    flavor: kubernetes\r\n\r\n    # cilium:\r\n    #   endpointSelector:\r\n    #   egress:\r\n    #   ingress:\r\n\r\n    # egress:\r\n    # - {}\r\n    # ingress:\r\n    # - {}\r\n    # podSelector:\r\n    #   matchLabels:\r\n    #     app: prometheus\r\n\r\n  ## Service account for Prometheuses to use.\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n  ##\r\n  serviceAccount:\r\n    create: true\r\n    name: \"\"\r\n    annotations: {}\r\n    automountServiceAccountToken: true\r\n\r\n  # Service for thanos service discovery on sidecar\r\n  # Enable this can make Thanos Query can use\r\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\r\n  # Thanos sidecar on prometheus nodes\r\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\r\n  thanosService:\r\n    enabled: false\r\n    annotations: {}\r\n    labels: {}\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n    ## Service dual stack\r\n    ##\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n    ## gRPC port config\r\n    portName: grpc\r\n    port: 10901\r\n    targetPort: \"grpc\"\r\n\r\n    ## HTTP port config (for metrics)\r\n    httpPortName: http\r\n    httpPort: 10902\r\n    targetHttpPort: \"http\"\r\n\r\n    ## ClusterIP to assign\r\n    # Default is to make this a headless service (\"None\")\r\n    clusterIP: \"None\"\r\n\r\n    ## Port to expose on each node, if service type is NodePort\r\n    ##\r\n    nodePort: 30901\r\n    httpNodePort: 30902\r\n\r\n  # ServiceMonitor to scrape Sidecar metrics\r\n  # Needs thanosService to be enabled as well\r\n  thanosServiceMonitor:\r\n    enabled: false\r\n    interval: \"\"\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n\r\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\r\n    scheme: \"\"\r\n\r\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\r\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\r\n    tlsConfig: {}\r\n\r\n    bearerTokenFile:\r\n\r\n    ## Metric relabel configs to apply to samples before ingestion.\r\n    metricRelabelings: []\r\n\r\n    ## relabel configs to apply to samples before ingestion.\r\n    relabelings: []\r\n\r\n  # Service for external access to sidecar\r\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\r\n  thanosServiceExternal:\r\n    enabled: false\r\n    annotations: {}\r\n    labels: {}\r\n    loadBalancerIP: \"\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## gRPC port config\r\n    portName: grpc\r\n    port: 10901\r\n    targetPort: \"grpc\"\r\n\r\n    ## HTTP port config (for metrics)\r\n    httpPortName: http\r\n    httpPort: 10902\r\n    targetHttpPort: \"http\"\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: LoadBalancer\r\n\r\n    ## Port to expose on each node\r\n    ##\r\n    nodePort: 30901\r\n    httpNodePort: 30902\r\n\r\n  ## Configuration for Prometheus service\r\n  ##\r\n  service:\r\n    annotations: {}\r\n    labels: {}\r\n    clusterIP: \"\"\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n    ## Port for Prometheus Service to listen on\r\n    ##\r\n    port: 9090\r\n\r\n    ## To be used with a proxy extraContainer port\r\n    targetPort: 9090\r\n\r\n    ## Port for Prometheus Reloader to listen on\r\n    ##\r\n    reloaderWebPort: 8080\r\n\r\n    ## List of IP addresses at which the Prometheus server service is available\r\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\r\n    ##\r\n    externalIPs: []\r\n\r\n    ## Port to expose on each node\r\n    ## Only used if service.type is 'NodePort'\r\n    ##\r\n    nodePort: 30090\r\n\r\n    ## Loadbalancer IP\r\n    ## Only use if service.type is \"LoadBalancer\"\r\n    loadBalancerIP: \"\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n    ## Additional ports to open for Prometheus service\r\n    ##\r\n    additionalPorts: []\r\n    # additionalPorts:\r\n    # - name: oauth-proxy\r\n    #   port: 8081\r\n    #   targetPort: 8081\r\n    # - name: oauth-metrics\r\n    #   port: 8082\r\n    #   targetPort: 8082\r\n\r\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\r\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\r\n    publishNotReadyAddresses: false\r\n\r\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\r\n    ## Accepts 'ClientIP' or 'None'\r\n    ##\r\n    sessionAffinity: None\r\n\r\n    ## If you want to modify the ClientIP sessionAffinity timeout\r\n    ## The value must be \u003e0 \u0026\u0026 \u003c=86400(for 1 day) if ServiceAffinity == \"ClientIP\"\r\n    ##\r\n    sessionAffinityConfig:\r\n      clientIP:\r\n        timeoutSeconds: 10800\r\n\r\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\r\n  ##\r\n  servicePerReplica:\r\n    enabled: false\r\n    annotations: {}\r\n\r\n    ## Port for Prometheus Service per replica to listen on\r\n    ##\r\n    port: 9090\r\n\r\n    ## To be used with a proxy extraContainer port\r\n    targetPort: 9090\r\n\r\n    ## Port to expose on each node\r\n    ## Only used if servicePerReplica.type is 'NodePort'\r\n    ##\r\n    nodePort: 30091\r\n\r\n    ## Loadbalancer source IP ranges\r\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n    ## Service dual stack\r\n    ##\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n  ## Configure pod disruption budgets for Prometheus\r\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\r\n  ##\r\n  podDisruptionBudget:\r\n    enabled: false\r\n    minAvailable: 1\r\n    maxUnavailable: \"\"\r\n\r\n  # Ingress exposes thanos sidecar outside the cluster\r\n  thanosIngress:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n    labels: {}\r\n    servicePort: 10901\r\n\r\n    ## Port to expose on each node\r\n    ## Only used if service.type is 'NodePort'\r\n    ##\r\n    nodePort: 30901\r\n\r\n    ## Hosts must be provided if Ingress is enabled.\r\n    ##\r\n    hosts: []\r\n      # - thanos-gateway.domain.com\r\n\r\n    ## Paths to use for ingress rules\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## TLS configuration for Thanos Ingress\r\n    ## Secret must be manually created in the namespace\r\n    ##\r\n    tls: []\r\n    # - secretName: thanos-gateway-tls\r\n    #   hosts:\r\n    #   - thanos-gateway.domain.com\r\n    #\r\n\r\n  ## ExtraSecret can be used to store various data in an extra secret\r\n  ## (use it for example to store hashed basic auth credentials)\r\n  extraSecret:\r\n    ## if not set, name will be auto generated\r\n    # name: \"\"\r\n    annotations: {}\r\n    data: {}\r\n  #   auth: |\r\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\r\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\r\n\r\n  ingress:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n    labels: {}\r\n\r\n    ## Redirect ingress to an additional defined port on the service\r\n    # servicePort: 8081\r\n\r\n    ## Hostnames.\r\n    ## Must be provided if Ingress is enabled.\r\n    ##\r\n    # hosts:\r\n    #   - prometheus.domain.com\r\n    hosts: []\r\n\r\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## TLS configuration for Prometheus Ingress\r\n    ## Secret must be manually created in the namespace\r\n    ##\r\n    tls: []\r\n      # - secretName: prometheus-general-tls\r\n      #   hosts:\r\n      #     - prometheus.example.com\r\n\r\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\r\n  ## prometheus.servicePerReplica must be enabled\r\n  ##\r\n  ingressPerReplica:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n    labels: {}\r\n\r\n    ## Final form of the hostname for each per replica ingress is\r\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\r\n    ##\r\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\r\n    ## appended to the end\r\n    hostPrefix: \"\"\r\n    ## Domain that will be used for the per replica ingress\r\n    hostDomain: \"\"\r\n\r\n    ## Paths to use for ingress rules\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\r\n    ## Secret must be manually created in the namespace\r\n    tlsSecretName: \"\"\r\n\r\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\r\n    ##\r\n    tlsSecretPerReplica:\r\n      enabled: false\r\n      ## Final form of the secret for each per replica ingress is\r\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\r\n      ##\r\n      prefix: \"prometheus\"\r\n\r\n  ## Configure additional options for default pod security policy for Prometheus\r\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\r\n  podSecurityPolicy:\r\n    allowedCapabilities: []\r\n    allowedHostPaths: []\r\n    volumes: []\r\n\r\n  serviceMonitor:\r\n    ## If true, create a serviceMonitor for prometheus\r\n    ##\r\n    selfMonitor: true\r\n\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\r\n    scheme: \"\"\r\n\r\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\r\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\r\n    tlsConfig: {}\r\n\r\n    bearerTokenFile:\r\n\r\n    ## Metric relabel configs to apply to samples before ingestion.\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    #   relabel configs to apply to samples before ingestion.\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional Endpoints\r\n    ##\r\n    additionalEndpoints: []\r\n    # - port: oauth-metrics\r\n    #   path: /metrics\r\n\r\n  ## Settings affecting prometheusSpec\r\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\r\n  ##\r\n  prometheusSpec:\r\n    ## Statefulset's persistent volume claim retention policy\r\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\r\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\r\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\r\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\r\n    persistentVolumeClaimRetentionPolicy: {}\r\n    #  whenDeleted: Retain\r\n    #  whenScaled: Retain\r\n\r\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\r\n    ##\r\n    ## AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in the pod,\r\n    ## If the field isn’t set, the operator mounts the service account token by default.\r\n    ## Warning: be aware that by default, Prometheus requires the service account token for Kubernetes service discovery,\r\n    ## It is possible to use strategic merge patch to project the service account token into the ‘prometheus’ container.\r\n    automountServiceAccountToken: true\r\n\r\n    disableCompaction: false\r\n    ## APIServerConfig\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\r\n    ##\r\n    apiserverConfig: {}\r\n\r\n    ## Allows setting additional arguments for the Prometheus container\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus\r\n    additionalArgs: []\r\n\r\n    ## Interval between consecutive scrapes.\r\n    ## Defaults to 30s.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\r\n    ##\r\n    scrapeInterval: \"\"\r\n\r\n    ## Number of seconds to wait for target to respond before erroring\r\n    ##\r\n    scrapeTimeout: \"\"\r\n\r\n    ## List of scrape classes to expose to scraping objects such as\r\n    ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.\r\n    ##\r\n    scrapeClasses: []\r\n    # - name: istio-mtls\r\n    #   default: false\r\n    #   tlsConfig:\r\n    #     caFile: /etc/prometheus/secrets/istio.default/root-cert.pem\r\n    #     certFile: /etc/prometheus/secrets/istio.default/cert-chain.pem\r\n\r\n    ## Interval between consecutive evaluations.\r\n    ##\r\n    evaluationInterval: \"\"\r\n\r\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\r\n    ##\r\n    listenLocal: false\r\n\r\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\r\n    ## This is disabled by default.\r\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\r\n    ##\r\n    enableAdminAPI: false\r\n\r\n    ## Sets version of Prometheus overriding the Prometheus version as derived\r\n    ## from the image tag. Useful in cases where the tag does not follow semver v2.\r\n    version: \"\"\r\n\r\n    ## WebTLSConfig defines the TLS parameters for HTTPS\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\r\n    web: {}\r\n\r\n    ## Exemplars related settings that are runtime reloadable.\r\n    ## It requires to enable the exemplar storage feature to be effective.\r\n    exemplars: \"\"\r\n      ## Maximum number of exemplars stored in memory for all series.\r\n      ## If not set, Prometheus uses its default value.\r\n      ## A value of zero or less than zero disables the storage.\r\n      # maxSize: 100000\r\n\r\n    # EnableFeatures API enables access to Prometheus disabled features.\r\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\r\n    enableFeatures: []\r\n    # - exemplar-storage\r\n\r\n    ## Image of Prometheus.\r\n    ##\r\n    image:\r\n      registry: quay.io\r\n      repository: prometheus/prometheus\r\n      tag: v2.54.1\r\n      sha: \"\"\r\n\r\n    ## Tolerations for use with node taints\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\r\n    ##\r\n    tolerations: []\r\n    #  - key: \"key\"\r\n    #    operator: \"Equal\"\r\n    #    value: \"value\"\r\n    #    effect: \"NoSchedule\"\r\n\r\n    ## If specified, the pod's topology spread constraints.\r\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\r\n    ##\r\n    topologySpreadConstraints: []\r\n    # - maxSkew: 1\r\n    #   topologyKey: topology.kubernetes.io/zone\r\n    #   whenUnsatisfiable: DoNotSchedule\r\n    #   labelSelector:\r\n    #     matchLabels:\r\n    #       app: prometheus\r\n\r\n    ## Alertmanagers to which alerts will be sent\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\r\n    ##\r\n    ## Default configuration will connect to the alertmanager deployed as part of this release\r\n    ##\r\n    alertingEndpoints: []\r\n    # - name: \"\"\r\n    #   namespace: \"\"\r\n    #   port: http\r\n    #   scheme: http\r\n    #   pathPrefix: \"\"\r\n    #   tlsConfig: {}\r\n    #   bearerTokenFile: \"\"\r\n    #   apiVersion: v2\r\n\r\n    ## External labels to add to any time series or alerts when communicating with external systems\r\n    ##\r\n    externalLabels: {}\r\n\r\n    ## enable --web.enable-remote-write-receiver flag on prometheus-server\r\n    ##\r\n    enableRemoteWriteReceiver: false\r\n\r\n    ## Name of the external label used to denote replica name\r\n    ##\r\n    replicaExternalLabelName: \"\"\r\n\r\n    ## If true, the Operator won't add the external label used to denote replica name\r\n    ##\r\n    replicaExternalLabelNameClear: false\r\n\r\n    ## Name of the external label used to denote Prometheus instance name\r\n    ##\r\n    prometheusExternalLabelName: \"\"\r\n\r\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\r\n    ##\r\n    prometheusExternalLabelNameClear: false\r\n\r\n    ## External URL at which Prometheus will be reachable.\r\n    ##\r\n    externalUrl: \"\"\r\n\r\n    ## Define which Nodes the Pods are scheduled on.\r\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\r\n    ##\r\n    nodeSelector: {}\r\n\r\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\r\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\r\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\r\n    ## with the new list of secrets.\r\n    ##\r\n    secrets: []\r\n\r\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\r\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\r\n    ##\r\n    configMaps: []\r\n\r\n    ## QuerySpec defines the query command line flags when starting Prometheus.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\r\n    ##\r\n    query: {}\r\n\r\n    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.\r\n    ruleNamespaceSelector: {}\r\n    ## Example which selects PrometheusRules in namespaces with label \"prometheus\" set to \"somelabel\"\r\n    # ruleNamespaceSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## PrometheusRules to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    ruleSelector:\r\n      matchLabels:\r\n        release: \"{{ $.Release.Name }}\"\r\n    ## Example which select all PrometheusRules resources\r\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\r\n    # ruleSelector:\r\n    #   matchExpressions:\r\n    #     - key: prometheus\r\n    #       operator: In\r\n    #       values:\r\n    #         - example-rules\r\n    #         - example-rules-2\r\n    #\r\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\r\n    # ruleSelector:\r\n    #   matchLabels:\r\n    #     role: example-rules\r\n\r\n    ## ServiceMonitors to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    serviceMonitorSelector:\r\n      matchLabels:\r\n        release: \"{{ $.Release.Name }}\"\r\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\r\n    # serviceMonitorSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## Namespaces to be selected for ServiceMonitor discovery.\r\n    ##\r\n    serviceMonitorNamespaceSelector: {}\r\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\r\n    # serviceMonitorNamespaceSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## PodMonitors to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    podMonitorSelector:\r\n       matchLabels: null\r\n\r\n    ## Probes to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    probeSelector:\r\n      matchLabels:\r\n        release: \"{{ $.Release.Name }}\"\r\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\r\n    # probeSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.\r\n    probeNamespaceSelector: {}\r\n    ## Example which selects Probe in namespaces with label \"prometheus\" set to \"somelabel\"\r\n    # probeNamespaceSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## scrapeConfigs to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    scrapeConfigSelector:\r\n      matchLabels:\r\n        release: \"{{ $.Release.Name }}\"\r\n    ## Example which selects scrapeConfigs with label \"prometheus\" set to \"somelabel\"\r\n    # scrapeConfigSelector:\r\n    #   matchLabels:\r\n    #     release: ~\r\n    #     prometheus: somelabel\r\n\r\n    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    scrapeConfigNamespaceSelector: {}\r\n    ## Example which selects scrapeConfig in namespaces with label \"prometheus\" set to \"somelabel\"\r\n    # scrapeConfigNamespaceSelector:\r\n    #   matchLabels:\r\n    #     prometheus: somelabel\r\n\r\n    ## How long to retain metrics\r\n    ##\r\n    retention: 10d\r\n\r\n    ## Maximum size of metrics\r\n    ##\r\n    retentionSize: \"\"\r\n\r\n    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration\r\n    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\r\n    tsdb:\r\n      outOfOrderTimeWindow: 0s\r\n\r\n    ## Enable compression of the write-ahead log using Snappy.\r\n    ##\r\n    walCompression: true\r\n\r\n    ## If true, the Operator won't process any Prometheus configuration changes\r\n    ##\r\n    paused: false\r\n\r\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\r\n    ## Number of replicas multiplied by shards is the total number of Pods created.\r\n    ##\r\n    replicas: 1\r\n\r\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\r\n    ## Number of replicas multiplied by shards is the total number of Pods created.\r\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\r\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\r\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\r\n    ## Sharding is done on the content of the `__address__` target meta-label.\r\n    ##\r\n    shards: 1\r\n\r\n    ## Log level for Prometheus be configured in\r\n    ##\r\n    logLevel: info\r\n\r\n    ## Log format for Prometheus be configured in\r\n    ##\r\n    logFormat: logfmt\r\n\r\n    ## Prefix used to register routes, overriding externalUrl route.\r\n    ## Useful for proxies that rewrite URLs.\r\n    ##\r\n    routePrefix: /\r\n\r\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\r\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\r\n    ##\r\n    podMetadata: {}\r\n    # labels:\r\n    #   app: prometheus\r\n    #   k8s-app: prometheus\r\n\r\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\r\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\r\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\r\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\r\n    podAntiAffinity: \"\"\r\n\r\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\r\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\r\n    ##\r\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\r\n\r\n    ## Assign custom affinity rules to the prometheus instance\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\r\n    ##\r\n    affinity: {}\r\n    # nodeAffinity:\r\n    #   requiredDuringSchedulingIgnoredDuringExecution:\r\n    #     nodeSelectorTerms:\r\n    #     - matchExpressions:\r\n    #       - key: kubernetes.io/e2e-az-name\r\n    #         operator: In\r\n    #         values:\r\n    #         - e2e-az1\r\n    #         - e2e-az2\r\n\r\n    ## The remote_read spec configuration for Prometheus.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\r\n    remoteRead: []\r\n    # - url: http://remote1/read\r\n    ## additionalRemoteRead is appended to remoteRead\r\n    additionalRemoteRead: []\r\n\r\n    ## The remote_write spec configuration for Prometheus.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\r\n    remoteWrite: []\r\n    # - url: http://remote1/push\r\n    ## additionalRemoteWrite is appended to remoteWrite\r\n    additionalRemoteWrite: []\r\n\r\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\r\n    remoteWriteDashboards: false\r\n\r\n    ## Resource limits \u0026 requests\r\n    ##\r\n    resources: {}\r\n    # requests:\r\n    #   memory: 400Mi\r\n\r\n    ## Prometheus StorageSpec for persistent data\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\r\n    ##\r\n    storageSpec: {}\r\n    ## Using PersistentVolumeClaim\r\n    ##\r\n    #  volumeClaimTemplate:\r\n    #    spec:\r\n    #      storageClassName: gluster\r\n    #      accessModes: [\"ReadWriteOnce\"]\r\n    #      resources:\r\n    #        requests:\r\n    #          storage: 50Gi\r\n    #    selector: {}\r\n\r\n    ## Using tmpfs volume\r\n    ##\r\n    #  emptyDir:\r\n    #    medium: Memory\r\n\r\n    # Additional volumes on the output StatefulSet definition.\r\n    volumes: []\r\n\r\n    # Additional VolumeMounts on the output StatefulSet definition.\r\n    volumeMounts: []\r\n\r\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\r\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\r\n    ## as specified in the official Prometheus documentation:\r\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\r\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\r\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\r\n    ## scrape configs are going to break Prometheus after the upgrade.\r\n    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.\r\n    ##\r\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\r\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\r\n    ##\r\n    additionalScrapeConfigs: []\r\n    # - job_name: kube-etcd\r\n    #   kubernetes_sd_configs:\r\n    #     - role: node\r\n    #   scheme: https\r\n    #   tls_config:\r\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\r\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\r\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\r\n    #   relabel_configs:\r\n    #   - action: labelmap\r\n    #     regex: __meta_kubernetes_node_label_(.+)\r\n    #   - source_labels: [__address__]\r\n    #     action: replace\r\n    #     targetLabel: __address__\r\n    #     regex: ([^:;]+):(\\d+)\r\n    #     replacement: ${1}:2379\r\n    #   - source_labels: [__meta_kubernetes_node_name]\r\n    #     action: keep\r\n    #     regex: .*mst.*\r\n    #   - source_labels: [__meta_kubernetes_node_name]\r\n    #     action: replace\r\n    #     targetLabel: node\r\n    #     regex: (.*)\r\n    #     replacement: ${1}\r\n    #   metric_relabel_configs:\r\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\r\n    #     action: labeldrop\r\n    #\r\n    ## If scrape config contains a repetitive section, you may want to use a template.\r\n    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones\r\n    # additionalScrapeConfigs: |\r\n    #  - job_name: \"node-exporter\"\r\n    #    gce_sd_configs:\r\n    #    {{range $zone := .Values.gcp_zones}}\r\n    #    - project: \"project1\"\r\n    #      zone: \"{{$zone}}\"\r\n    #      port: 9100\r\n    #    {{end}}\r\n    #    relabel_configs:\r\n    #    ...\r\n\r\n\r\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\r\n    ## Expected values are the secret name and key\r\n    ## Cannot be used with additionalScrapeConfigs\r\n    additionalScrapeConfigsSecret: {}\r\n      # enabled: false\r\n      # name:\r\n      # key:\r\n\r\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\r\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\r\n    additionalPrometheusSecretsAnnotations: {}\r\n\r\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\r\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\r\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\r\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\r\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\r\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\r\n    ##\r\n    additionalAlertManagerConfigs: []\r\n    # - consul_sd_configs:\r\n    #   - server: consul.dev.test:8500\r\n    #     scheme: http\r\n    #     datacenter: dev\r\n    #     tag_separator: ','\r\n    #     services:\r\n    #       - metrics-prometheus-alertmanager\r\n\r\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\r\n    ## them separately from the helm deployment, you can use this section.\r\n    ## Expected values are the secret name and key\r\n    ## Cannot be used with additionalAlertManagerConfigs\r\n    additionalAlertManagerConfigsSecret: {}\r\n      # name:\r\n      # key:\r\n      # optional: false\r\n\r\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\r\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\r\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\r\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\r\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\r\n    ## configs are going to break Prometheus after the upgrade.\r\n    ##\r\n    additionalAlertRelabelConfigs: []\r\n    # - separator: ;\r\n    #   regex: prometheus_replica\r\n    #   replacement: $1\r\n    #   action: labeldrop\r\n\r\n    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage\r\n    ## them separately from the helm deployment, you can use this section.\r\n    ## Expected values are the secret name and key\r\n    ## Cannot be used with additionalAlertRelabelConfigs\r\n    additionalAlertRelabelConfigsSecret: {}\r\n      # name:\r\n      # key:\r\n\r\n    ## SecurityContext holds pod-level security attributes and common container settings.\r\n    ## This defaults to non root user with uid 1000 and gid 2000.\r\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\r\n    ##\r\n    securityContext:\r\n      runAsGroup: 2000\r\n      runAsNonRoot: true\r\n      runAsUser: 1000\r\n      fsGroup: 2000\r\n      seccompProfile:\r\n        type: RuntimeDefault\r\n\r\n    ## Priority class assigned to the Pods\r\n    ##\r\n    priorityClassName: \"\"\r\n\r\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\r\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\r\n    ## This is experimental and may change significantly without backward compatibility in any release.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\r\n    ##\r\n    thanos: {}\r\n      # secretProviderClass:\r\n      #   provider: gcp\r\n      #   parameters:\r\n      #     secrets: |\r\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\r\n      #         fileName: \"objstore.yaml\"\r\n      ## ObjectStorageConfig configures object storage in Thanos.\r\n      # objectStorageConfig:\r\n      #   # use existing secret, if configured, objectStorageConfig.secret will not be used\r\n      #   existingSecret: {}\r\n      #     # name: \"\"\r\n      #     # key: \"\"\r\n      #   # will render objectStorageConfig secret data and configure it to be used by Thanos custom resource,\r\n      #   # ignored when prometheusspec.thanos.objectStorageConfig.existingSecret is set\r\n      #   # https://thanos.io/tip/thanos/storage.md/#s3\r\n      #   secret: {}\r\n      #     # type: S3\r\n      #     # config:\r\n      #     #   bucket: \"\"\r\n      #     #   endpoint: \"\"\r\n      #     #   region: \"\"\r\n      #     #   access_key: \"\"\r\n      #     #   secret_key: \"\"\r\n\r\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\r\n    ## if using proxy extraContainer update targetPort with proxy container port\r\n    containers: []\r\n    # containers:\r\n    # - name: oauth-proxy\r\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1\r\n    #   args:\r\n    #   - --upstream=http://127.0.0.1:9090\r\n    #   - --http-address=0.0.0.0:8081\r\n    #   - --metrics-address=0.0.0.0:8082\r\n    #   - ...\r\n    #   ports:\r\n    #   - containerPort: 8081\r\n    #     name: oauth-proxy\r\n    #     protocol: TCP\r\n    #   - containerPort: 8082\r\n    #     name: oauth-metrics\r\n    #     protocol: TCP\r\n    #   resources: {}\r\n\r\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\r\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\r\n    initContainers: []\r\n\r\n    ## PortName to use for Prometheus.\r\n    ##\r\n    portName: \"http-web\"\r\n\r\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\r\n    ## on the file system of the Prometheus container e.g. bearer token files.\r\n    arbitraryFSAccessThroughSMs: false\r\n\r\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\r\n    ## or PodMonitor to true, this overrides honor_labels to false.\r\n    overrideHonorLabels: false\r\n\r\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\r\n    overrideHonorTimestamps: false\r\n\r\n    ## When ignoreNamespaceSelectors is set to true, namespaceSelector from all PodMonitor, ServiceMonitor and Probe objects will be ignored,\r\n    ## they will only discover targets within the namespace of the PodMonitor, ServiceMonitor and Probe object,\r\n    ## and servicemonitors will be installed in the default service namespace.\r\n    ## Defaults to false.\r\n    ignoreNamespaceSelectors: false\r\n\r\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\r\n    ## The label value will always be the namespace of the object that is being created.\r\n    ## Disabled by default\r\n    enforcedNamespaceLabel: \"\"\r\n\r\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\r\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\r\n    ## Deprecated, use `excludedFromEnforcement` instead\r\n    prometheusRulesExcludedFromEnforce: []\r\n\r\n    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects\r\n    ## to be excluded from enforcing a namespace label of origin.\r\n    ## Works only if enforcedNamespaceLabel set to true.\r\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference\r\n    excludedFromEnforcement: []\r\n\r\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\r\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\r\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\r\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\r\n    queryLogFile: false\r\n\r\n    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.\r\n    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.\r\n    sampleLimit: false\r\n\r\n    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.\r\n    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets\r\n    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.\r\n    enforcedKeepDroppedTargets: 0\r\n\r\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\r\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\r\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\r\n    enforcedSampleLimit: false\r\n\r\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\r\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\r\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\r\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\r\n    enforcedTargetLimit: false\r\n\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\r\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\r\n    ## 2.27.0 and newer.\r\n    enforcedLabelLimit: false\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\r\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\r\n    ## 2.27.0 and newer.\r\n    enforcedLabelNameLengthLimit: false\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\r\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\r\n    ## versions 2.27.0 and newer.\r\n    enforcedLabelValueLengthLimit: false\r\n\r\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\r\n    ## in Prometheus so it may change in any upcoming release.\r\n    allowOverlappingBlocks: false\r\n\r\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\r\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\r\n    minReadySeconds: 0\r\n\r\n    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\r\n    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\r\n    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.\r\n    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.\r\n    hostNetwork: false\r\n\r\n    # HostAlias holds the mapping between IP and hostnames that will be injected\r\n    # as an entry in the pod’s hosts file.\r\n    hostAliases: []\r\n    #  - ip: 10.10.0.100\r\n    #    hostnames:\r\n    #      - a1.app.local\r\n    #      - b1.app.local\r\n\r\n    ## TracingConfig configures tracing in Prometheus.\r\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig\r\n    tracingConfig: {}\r\n\r\n    ## Defines the service discovery role used to discover targets from ServiceMonitor objects and Alertmanager endpoints.\r\n    ## If set, the value should be either “Endpoints” or “EndpointSlice”. If unset, the operator assumes the “Endpoints” role.\r\n    serviceDiscoveryRole: \"\"\r\n\r\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\r\n    additionalConfig: {}\r\n\r\n    ## Additional configuration which is not covered by the properties above.\r\n    ## Useful, if you need advanced templating inside alertmanagerSpec.\r\n    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)\r\n    additionalConfigString: \"\"\r\n\r\n    ## Defines the maximum time that the `prometheus` container's startup probe\r\n    ## will wait before being considered failed. The startup probe will return\r\n    ## success after the WAL replay is complete. If set, the value should be\r\n    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15\r\n    ## minutes).\r\n    maximumStartupDurationSeconds: 0\r\n\r\n  additionalRulesForClusterRole: []\r\n  #  - apiGroups: [ \"\" ]\r\n  #    resources:\r\n  #      - nodes/proxy\r\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\r\n\r\n  additionalServiceMonitors: []\r\n  ## Name of the ServiceMonitor to create\r\n  ##\r\n  # - name: \"\"\r\n\r\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\r\n    ## the chart\r\n    ##\r\n    # additionalLabels: {}\r\n\r\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\r\n    ## If no label is specified, the service name is used.\r\n    ##\r\n    # jobLabel: \"\"\r\n\r\n    ## labels to transfer from the kubernetes service to the target\r\n    ##\r\n    # targetLabels: []\r\n\r\n    ## labels to transfer from the kubernetes pods to the target\r\n    ##\r\n    # podTargetLabels: []\r\n\r\n    ## Label selector for services to which this ServiceMonitor applies\r\n    ##\r\n    # selector: {}\r\n\r\n    ## Namespaces from which services are selected\r\n    ##\r\n    # namespaceSelector:\r\n      ## Match any namespace\r\n      ##\r\n      # any: false\r\n\r\n      ## Explicit list of namespace names to select\r\n      ##\r\n      # matchNames: []\r\n\r\n    ## Endpoints of the selected service to be monitored\r\n    ##\r\n    # endpoints: []\r\n      ## Name of the endpoint's service port\r\n      ## Mutually exclusive with targetPort\r\n      # - port: \"\"\r\n\r\n      ## Name or number of the endpoint's target port\r\n      ## Mutually exclusive with port\r\n      # - targetPort: \"\"\r\n\r\n      ## File containing bearer token to be used when scraping targets\r\n      ##\r\n      #   bearerTokenFile: \"\"\r\n\r\n      ## Interval at which metrics should be scraped\r\n      ##\r\n      #   interval: 30s\r\n\r\n      ## HTTP path to scrape for metrics\r\n      ##\r\n      #   path: /metrics\r\n\r\n      ## HTTP scheme to use for scraping\r\n      ##\r\n      #   scheme: http\r\n\r\n      ## TLS configuration to use when scraping the endpoint\r\n      ##\r\n      #   tlsConfig:\r\n\r\n          ## Path to the CA file\r\n          ##\r\n          # caFile: \"\"\r\n\r\n          ## Path to client certificate file\r\n          ##\r\n          # certFile: \"\"\r\n\r\n          ## Skip certificate verification\r\n          ##\r\n          # insecureSkipVerify: false\r\n\r\n          ## Path to client key file\r\n          ##\r\n          # keyFile: \"\"\r\n\r\n          ## Server name used to verify host name\r\n          ##\r\n          # serverName: \"\"\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    # metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    # relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n  additionalPodMonitors: []\r\n  ## Name of the PodMonitor to create\r\n  ##\r\n  # - name: \"\"\r\n\r\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\r\n    ## the chart\r\n    ##\r\n    # additionalLabels: {}\r\n\r\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\r\n    ## If no label is specified, the pod endpoint name is used.\r\n    ##\r\n    # jobLabel: \"\"\r\n\r\n    ## Label selector for pods to which this PodMonitor applies\r\n    ##\r\n    # selector: {}\r\n\r\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\r\n    ##\r\n    # podTargetLabels: {}\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    # sampleLimit: 0\r\n\r\n    ## Namespaces from which pods are selected\r\n    ##\r\n    # namespaceSelector:\r\n      ## Match any namespace\r\n      ##\r\n      # any: false\r\n\r\n      ## Explicit list of namespace names to select\r\n      ##\r\n      # matchNames: []\r\n\r\n    ## Endpoints of the selected pods to be monitored\r\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\r\n    ##\r\n    # podMetricsEndpoints: []\r\n\r\n## Configuration for thanosRuler\r\n## ref: https://thanos.io/tip/components/rule.md/\r\n##\r\nthanosRuler:\r\n\r\n  ## Deploy thanosRuler\r\n  ##\r\n  enabled: false\r\n\r\n  ## Annotations for ThanosRuler\r\n  ##\r\n  annotations: {}\r\n\r\n  ## Service account for ThanosRuler to use.\r\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\r\n  ##\r\n  serviceAccount:\r\n    create: true\r\n    name: \"\"\r\n    annotations: {}\r\n\r\n  ## Configure pod disruption budgets for ThanosRuler\r\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\r\n  ##\r\n  podDisruptionBudget:\r\n    enabled: false\r\n    minAvailable: 1\r\n    maxUnavailable: \"\"\r\n\r\n  ingress:\r\n    enabled: false\r\n\r\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\r\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\r\n    # ingressClassName: nginx\r\n\r\n    annotations: {}\r\n\r\n    labels: {}\r\n\r\n    ## Hosts must be provided if Ingress is enabled.\r\n    ##\r\n    hosts: []\r\n      # - thanosruler.domain.com\r\n\r\n    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix\r\n    ##\r\n    paths: []\r\n    # - /\r\n\r\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\r\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\r\n    # pathType: ImplementationSpecific\r\n\r\n    ## TLS configuration for ThanosRuler Ingress\r\n    ## Secret must be manually created in the namespace\r\n    ##\r\n    tls: []\r\n    # - secretName: thanosruler-general-tls\r\n    #   hosts:\r\n    #   - thanosruler.example.com\r\n\r\n  ## Configuration for ThanosRuler service\r\n  ##\r\n  service:\r\n    annotations: {}\r\n    labels: {}\r\n    clusterIP: \"\"\r\n    ipDualStack:\r\n      enabled: false\r\n      ipFamilies: [\"IPv6\", \"IPv4\"]\r\n      ipFamilyPolicy: \"PreferDualStack\"\r\n\r\n    ## Port for ThanosRuler Service to listen on\r\n    ##\r\n    port: 10902\r\n    ## To be used with a proxy extraContainer port\r\n    ##\r\n    targetPort: 10902\r\n    ## Port to expose on each node\r\n    ## Only used if service.type is 'NodePort'\r\n    ##\r\n    nodePort: 30905\r\n    ## List of IP addresses at which the Prometheus server service is available\r\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\r\n    ##\r\n\r\n    ## Additional ports to open for ThanosRuler service\r\n    additionalPorts: []\r\n\r\n    externalIPs: []\r\n    loadBalancerIP: \"\"\r\n    loadBalancerSourceRanges: []\r\n\r\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\r\n    ##\r\n    externalTrafficPolicy: Cluster\r\n\r\n    ## Service type\r\n    ##\r\n    type: ClusterIP\r\n\r\n  ## Configuration for creating a ServiceMonitor for the ThanosRuler service\r\n  ##\r\n  serviceMonitor:\r\n    ## If true, create a serviceMonitor for thanosRuler\r\n    ##\r\n    selfMonitor: true\r\n\r\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\r\n    ##\r\n    interval: \"\"\r\n\r\n    ## Additional labels\r\n    ##\r\n    additionalLabels: {}\r\n\r\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\r\n    ##\r\n    sampleLimit: 0\r\n\r\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\r\n    ##\r\n    targetLimit: 0\r\n\r\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelNameLengthLimit: 0\r\n\r\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\r\n    ##\r\n    labelValueLengthLimit: 0\r\n\r\n    ## proxyUrl: URL of a proxy that should be used for scraping.\r\n    ##\r\n    proxyUrl: \"\"\r\n\r\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\r\n    scheme: \"\"\r\n\r\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\r\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\r\n    tlsConfig: {}\r\n\r\n    bearerTokenFile:\r\n\r\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    metricRelabelings: []\r\n    # - action: keep\r\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\r\n    #   sourceLabels: [__name__]\r\n\r\n    ## RelabelConfigs to apply to samples before scraping\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\r\n    ##\r\n    relabelings: []\r\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\r\n    #   separator: ;\r\n    #   regex: ^(.*)$\r\n    #   targetLabel: nodename\r\n    #   replacement: $1\r\n    #   action: replace\r\n\r\n    ## Additional Endpoints\r\n    ##\r\n    additionalEndpoints: []\r\n    # - port: oauth-metrics\r\n    #   path: /metrics\r\n\r\n  ## Settings affecting thanosRulerpec\r\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec\r\n  ##\r\n  thanosRulerSpec:\r\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\r\n    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.\r\n    ##\r\n    podMetadata: {}\r\n\r\n    ## Image of ThanosRuler\r\n    ##\r\n    image:\r\n      registry: quay.io\r\n      repository: thanos/thanos\r\n      tag: v0.36.1\r\n      sha: \"\"\r\n\r\n    ## Namespaces to be selected for PrometheusRules discovery.\r\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\r\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\r\n    ##\r\n    ruleNamespaceSelector: {}\r\n\r\n    ## PrometheusRules to be selected for target discovery.\r\n    ## If matchLabels.release: \"{{ $.Release.Name }}\" the prometheus resource will be created\r\n    ## with selectors based on values in the helm deployment, which will also match the scrapeConfigs created\r\n    ## To remove matchLabels from the selector condition, explicitly set matchLabels to null.\r\n    ## If no other selectors are configured, prometheus-operator will select all scrapeConfigs.\r\n    ## To remove the release label from the matchLabels condition, explicit set release to null.\r\n    ## If null, exclude the field from the prometheusSpec (keeping downward compatibility with older versions of CRD)\r\n    ##\r\n    ruleSelector:\r\n      matchLabels:\r\n        release: \"{{ $.Release.Name }}\"\r\n    ## Example which select all PrometheusRules resources\r\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\r\n    # ruleSelector:\r\n    #   matchLabels: ~\r\n    #   matchExpressions:\r\n    #     - key: prometheus\r\n    #       operator: In\r\n    #       values:\r\n    #         - example-rules\r\n    #         - example-rules-2\r\n    #\r\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\r\n    # ruleSelector:\r\n    #   matchLabels:\r\n    #     release: ~\r\n    #     role: example-rules\r\n\r\n    ## Define Log Format\r\n    # Use logfmt (default) or json logging\r\n    logFormat: logfmt\r\n\r\n    ## Log level for ThanosRuler to be configured with.\r\n    ##\r\n    logLevel: info\r\n\r\n    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the\r\n    ## running cluster equal to the expected size.\r\n    replicas: 1\r\n\r\n    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression\r\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\r\n    ##\r\n    retention: 24h\r\n\r\n    ## Interval between consecutive evaluations.\r\n    ##\r\n    evaluationInterval: \"\"\r\n\r\n    ## Storage is the definition of how storage will be used by the ThanosRuler instances.\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\r\n    ##\r\n    storage: {}\r\n    # volumeClaimTemplate:\r\n    #   spec:\r\n    #     storageClassName: gluster\r\n    #     accessModes: [\"ReadWriteOnce\"]\r\n    #     resources:\r\n    #       requests:\r\n    #         storage: 50Gi\r\n    #   selector: {}\r\n\r\n    ## AlertmanagerConfig define configuration for connecting to alertmanager.\r\n    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.\r\n    alertmanagersConfig:\r\n      # use existing secret, if configured, alertmanagersConfig.secret will not be used\r\n      existingSecret: {}\r\n        # name: \"\"\r\n        # key: \"\"\r\n      # will render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set\r\n      # https://thanos.io/tip/components/rule.md/#alertmanager\r\n      secret: {}\r\n        # alertmanagers:\r\n        # - api_version: v2\r\n        #   http_config:\r\n        #     basic_auth:\r\n        #       username: some_user\r\n        #       password: some_pass\r\n        #   static_configs:\r\n        #     - alertmanager.thanos.io\r\n        #   scheme: http\r\n        #   timeout: 10s\r\n\r\n    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.\r\n    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.\r\n    # alertmanagersUrl:\r\n\r\n    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false\r\n    ##\r\n    externalPrefix:\r\n\r\n    ## If true, http://{{ template \"kube-prometheus-stack.thanosRuler.name\" . }}.{{ template \"kube-prometheus-stack.namespace\" . }}:{{ .Values.thanosRuler.service.port }}\r\n    ## will be used as value for externalPrefix\r\n    externalPrefixNilUsesHelmValues: true\r\n\r\n    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\r\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\r\n    ##\r\n    routePrefix: /\r\n\r\n    ## ObjectStorageConfig configures object storage in Thanos\r\n    objectStorageConfig:\r\n      # use existing secret, if configured, objectStorageConfig.secret will not be used\r\n      existingSecret: {}\r\n        # name: \"\"\r\n        # key: \"\"\r\n      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set\r\n      # https://thanos.io/tip/thanos/storage.md/#s3\r\n      secret: {}\r\n        # type: S3\r\n        # config:\r\n        #   bucket: \"\"\r\n        #   endpoint: \"\"\r\n        #   region: \"\"\r\n        #   access_key: \"\"\r\n        #   secret_key: \"\"\r\n\r\n    ## Labels by name to drop before sending to alertmanager\r\n    ## Maps to the --alert.label-drop flag of thanos ruler.\r\n    alertDropLabels: []\r\n\r\n    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.\r\n    ## Maps to the --query flag of thanos ruler.\r\n    queryEndpoints: []\r\n\r\n    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.\r\n    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.\r\n    queryConfig:\r\n      # use existing secret, if configured, queryConfig.secret will not be used\r\n      existingSecret: {}\r\n        # name: \"\"\r\n        # key: \"\"\r\n      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set\r\n      # https://thanos.io/tip/components/rule.md/#query-api\r\n      secret: {}\r\n        # - http_config:\r\n        #     basic_auth:\r\n        #       username: some_user\r\n        #       password: some_pass\r\n        #   static_configs:\r\n        #     - URL\r\n        #   scheme: http\r\n        #   timeout: 10s\r\n\r\n    ## Labels configure the external label pairs to ThanosRuler. A default replica\r\n    ## label `thanos_ruler_replica` will be always added as a label with the value\r\n    ## of the pod's name and it will be dropped in the alerts.\r\n    labels: {}\r\n\r\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\r\n    ##\r\n    paused: false\r\n\r\n    ## Allows setting additional arguments for the ThanosRuler container\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler\r\n    ##\r\n    additionalArgs: []\r\n      # - name: remote-write.config\r\n      #   value: |-\r\n      #     \"remote_write\":\r\n      #     - \"name\": \"receiver-0\"\r\n      #       \"remote_timeout\": \"30s\"\r\n      #       \"url\": \"http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive\"\r\n\r\n    ## Define which Nodes the Pods are scheduled on.\r\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\r\n    ##\r\n    nodeSelector: {}\r\n\r\n    ## Define resources requests and limits for single Pods.\r\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\r\n    ##\r\n    resources: {}\r\n    # requests:\r\n    #   memory: 400Mi\r\n\r\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\r\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\r\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\r\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\r\n    ##\r\n    podAntiAffinity: \"\"\r\n\r\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\r\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\r\n    ##\r\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\r\n\r\n    ## Assign custom affinity rules to the thanosRuler instance\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\r\n    ##\r\n    affinity: {}\r\n    # nodeAffinity:\r\n    #   requiredDuringSchedulingIgnoredDuringExecution:\r\n    #     nodeSelectorTerms:\r\n    #     - matchExpressions:\r\n    #       - key: kubernetes.io/e2e-az-name\r\n    #         operator: In\r\n    #         values:\r\n    #         - e2e-az1\r\n    #         - e2e-az2\r\n\r\n    ## If specified, the pod's tolerations.\r\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\r\n    ##\r\n    tolerations: []\r\n    # - key: \"key\"\r\n    #   operator: \"Equal\"\r\n    #   value: \"value\"\r\n    #   effect: \"NoSchedule\"\r\n\r\n    ## If specified, the pod's topology spread constraints.\r\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\r\n    ##\r\n    topologySpreadConstraints: []\r\n    # - maxSkew: 1\r\n    #   topologyKey: topology.kubernetes.io/zone\r\n    #   whenUnsatisfiable: DoNotSchedule\r\n    #   labelSelector:\r\n    #     matchLabels:\r\n    #       app: thanos-ruler\r\n\r\n    ## SecurityContext holds pod-level security attributes and common container settings.\r\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\r\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\r\n    ##\r\n    securityContext:\r\n      runAsGroup: 2000\r\n      runAsNonRoot: true\r\n      runAsUser: 1000\r\n      fsGroup: 2000\r\n      seccompProfile:\r\n        type: RuntimeDefault\r\n\r\n    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.\r\n    ## Note this is only for the ThanosRuler UI, not the gossip communication.\r\n    ##\r\n    listenLocal: false\r\n\r\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.\r\n    ##\r\n    containers: []\r\n\r\n    # Additional volumes on the output StatefulSet definition.\r\n    volumes: []\r\n\r\n    # Additional VolumeMounts on the output StatefulSet definition.\r\n    volumeMounts: []\r\n\r\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\r\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\r\n    initContainers: []\r\n\r\n    ## Priority class assigned to the Pods\r\n    ##\r\n    priorityClassName: \"\"\r\n\r\n    ## PortName to use for ThanosRuler.\r\n    ##\r\n    portName: \"web\"\r\n\r\n    ## WebTLSConfig defines the TLS parameters for HTTPS\r\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerwebspec\r\n    web: {}\r\n\r\n    ## Additional configuration which is not covered by the properties above. (passed through tpl)\r\n    additionalConfig: {}\r\n\r\n    ## Additional configuration which is not covered by the properties above.\r\n    ## Useful, if you need advanced templating\r\n    additionalConfigString: \"\"\r\n\r\n  ## ExtraSecret can be used to store various data in an extra secret\r\n  ## (use it for example to store hashed basic auth credentials)\r\n  extraSecret:\r\n    ## if not set, name will be auto generated\r\n    # name: \"\"\r\n    annotations: {}\r\n    data: {}\r\n  #   auth: |\r\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\r\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\r\n\r\n## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.\r\n##\r\ncleanPrometheusOperatorObjectNames: false\r\n\r\n## Extra manifests to deploy as an array\r\nextraManifests: []\r\n  # - apiVersion: v1\r\n  #   kind: ConfigMap\r\n  #   metadata:\r\n  #   labels:\r\n  #     name: prometheus-extra\r\n  #   data:\r\n  #     extra-data: \"value\"\r\n"
            ],
            "verify": false,
            "version": "63.1.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "ClusterRole",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/clusterroles/node-api",
            "ignore_fields": null,
            "kind": "ClusterRole",
            "live_manifest_incluster": "1eb8226efacf836b7392e44b9ce741f53cbe573380f73907f68414aa87c95298",
            "live_uid": "1bd822fa-5007-4f4b-abf4-17a52355fd4f",
            "name": "node-api",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "1bd822fa-5007-4f4b-abf4-17a52355fd4f",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: rbac.authorization.k8s.io/v1\r\nkind: ClusterRole\r\nmetadata:\r\n  name: node-api # This defines a role and what API it can access\r\nrules:\r\n- apiGroups: [\"\"]\r\n  resources: [\"pods\"]\r\n  verbs: [\"delete\", \"get\", \"list\"]\r\n",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: node-api\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - delete\n  - get\n  - list\n",
            "yaml_incluster": "1eb8226efacf836b7392e44b9ce741f53cbe573380f73907f68414aa87c95298"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "RoleBinding",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "rbac.authorization.k8s.io/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/rbac.authorization.k8s.io/v1/namespaces/sre-challenge/rolebindings/sre-challenge",
            "ignore_fields": null,
            "kind": "RoleBinding",
            "live_manifest_incluster": "5fdd9a4cd3755b227a015852846d4d83f83cd59bbf5a3023e5770db66d81f308",
            "live_uid": "afef92d6-bf7f-4f9a-847f-d111c7ce7850",
            "name": "sre-challenge",
            "namespace": "sre-challenge",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "afef92d6-bf7f-4f9a-847f-d111c7ce7850",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "kind: RoleBinding\r\napiVersion: rbac.authorization.k8s.io/v1\r\nmetadata:\r\n  name: sre-challenge\r\n  namespace: sre-challenge\r\nsubjects:\r\n- kind: ServiceAccount\r\n  name: node-api\r\n  namespace: sre-challenge\r\n  apiGroup: \"\"\r\nroleRef:\r\n  kind: ClusterRole\r\n  name: node-api\r\n  apiGroup: \"\"\r\n",
            "yaml_body_parsed": "apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: sre-challenge\n  namespace: sre-challenge\nroleRef:\n  apiGroup: \"\"\n  kind: ClusterRole\n  name: node-api\nsubjects:\n- apiGroup: \"\"\n  kind: ServiceAccount\n  name: node-api\n  namespace: sre-challenge\n",
            "yaml_incluster": "5fdd9a4cd3755b227a015852846d4d83f83cd59bbf5a3023e5770db66d81f308"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "chaos-test",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "batch/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/batch/v1/namespaces/sre-challenge/cronjobs/kill-pod",
            "ignore_fields": null,
            "kind": "CronJob",
            "live_manifest_incluster": "44539b310ab161ae35e73a15150a8eb04984313e82285dbdd2709cc7a387c6c0",
            "live_uid": "f4c0bd24-1be2-48d6-b04a-a13db9b4eb1f",
            "name": "kill-pod",
            "namespace": "sre-challenge",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "f4c0bd24-1be2-48d6-b04a-a13db9b4eb1f",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: batch/v1\r\nkind: CronJob\r\nmetadata:\r\n  name: kill-pod\r\n  namespace: sre-challenge\r\nspec:\r\n  schedule: \"*/5 * * * *\"\r\n  successfulJobsHistoryLimit: 0\r\n  failedJobsHistoryLimit: 0\r\n  jobTemplate:\r\n    spec:\r\n      activeDeadlineSeconds: 30\r\n      template:\r\n        spec:\r\n          serviceAccountName: node-api\r\n          containers:\r\n          - name: kill-pod\r\n            image: bitnami/kubectl:latest\r\n            command:\r\n              - sh\r\n              - -c\r\n              - |\r\n                kubectl delete pod $(kubectl get pods -n sre-challenge -l App=\"node-api\" | grep \"node-\"  |awk '{print $1}' | head -1)\r\n          restartPolicy: OnFailure\r\n",
            "yaml_body_parsed": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: kill-pod\n  namespace: sre-challenge\nspec:\n  failedJobsHistoryLimit: 0\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 30\n      template:\n        spec:\n          containers:\n          - command:\n            - sh\n            - -c\n            - |\n              kubectl delete pod $(kubectl get pods -n sre-challenge -l App=\"node-api\" | grep \"node-\"  |awk '{print $1}' | head -1)\n            image: bitnami/kubectl:latest\n            name: kill-pod\n          restartPolicy: OnFailure\n          serviceAccountName: node-api\n  schedule: '*/5 * * * *'\n  successfulJobsHistoryLimit: 0\n",
            "yaml_incluster": "44539b310ab161ae35e73a15150a8eb04984313e82285dbdd2709cc7a387c6c0"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "pod-monitor",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "monitoring.coreos.com/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/monitoring.coreos.com/v1/namespaces/sre-challenge/podmonitors/node-api",
            "ignore_fields": null,
            "kind": "PodMonitor",
            "live_manifest_incluster": "bd50363fe156db0a795a5c04ef475baede649625ac37e9b001dfe5c35913799a",
            "live_uid": "62a42fb4-4031-46b3-a4d8-4698d2103da3",
            "name": "node-api",
            "namespace": "sre-challenge",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "62a42fb4-4031-46b3-a4d8-4698d2103da3",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: monitoring.coreos.com/v1\r\nkind: PodMonitor\r\nmetadata:\r\n  name: node-api\r\n  namespace: \"sre-challenge\"\r\n  labels:\r\n    App: \"node-api\"\r\n    release: \"kube-prometheus-stack\"\r\nspec:\r\n  namespaceSelector:\r\n    matchNames:\r\n      - sre-challenge\r\n  selector:\r\n    matchLabels:\r\n      App: \"node-api\"\r\n  podMetricsEndpoints:\r\n  - targetPort: 3000\r\n    path: /metrics\r\n    interval: 5s\r\n\r\n",
            "yaml_body_parsed": "apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  labels:\n    App: node-api\n    release: kube-prometheus-stack\n  name: node-api\n  namespace: sre-challenge\nspec:\n  namespaceSelector:\n    matchNames:\n    - sre-challenge\n  podMetricsEndpoints:\n  - interval: 5s\n    path: /metrics\n    targetPort: 3000\n  selector:\n    matchLabels:\n      App: node-api\n",
            "yaml_incluster": "bd50363fe156db0a795a5c04ef475baede649625ac37e9b001dfe5c35913799a"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "service-account",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/sre-challenge/serviceaccounts/node-api",
            "ignore_fields": null,
            "kind": "ServiceAccount",
            "live_manifest_incluster": "943fc927adae114269d6b5ade0d068636ab0de5be4694dd5c7a131a8d6bbcb83",
            "live_uid": "434ae638-81ee-4a04-a367-0a7eb9a6d027",
            "name": "node-api",
            "namespace": "sre-challenge",
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "434ae638-81ee-4a04-a367-0a7eb9a6d027",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": true,
            "yaml_body": "apiVersion: v1\r\nkind: ServiceAccount\r\nmetadata:\r\n  name: node-api # this is service account for binding the pod\r\n  namespace: sre-challenge\r\n",
            "yaml_body_parsed": "apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: node-api\n  namespace: sre-challenge\n",
            "yaml_incluster": "943fc927adae114269d6b5ade0d068636ab0de5be4694dd5c7a131a8d6bbcb83"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "yaml_incluster"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "yaml_body"
              }
            ],
            [
              {
                "type": "get_attr",
                "value": "live_manifest_incluster"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_config_map_v1",
      "name": "redis",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "binary_data": {},
            "data": {
              "init.sh": "set -ex\n# Generate redis server-id from pod ordinal index.\n[[ `hostname` =~ -([0-9]+)$ ]] || exit 1\nordinal=${BASH_REMATCH[1]}\n# Copy appropriate redis config files from config-map to respective directories.\nif [[ $ordinal -eq 0 ]]; then\n    cp /mnt/master.conf /etc/redis-config.conf\nelse\n    cp /mnt/slave.conf /etc/redis-config.conf\nfi",
              "master.conf": "# Redis configuration file example.\r\n#\r\n# Note that in order to read the configuration file, Redis must be\r\n# started with the file path as first argument:\r\n#\r\n# ./redis-server /path/to/redis.conf\r\n\r\n# Note on units: when memory size is needed, it is possible to specify\r\n# it in the usual form of 1k 5GB 4M and so forth:\r\n#\r\n# 1k =\u003e 1000 bytes\r\n# 1kb =\u003e 1024 bytes\r\n# 1m =\u003e 1000000 bytes\r\n# 1mb =\u003e 1024*1024 bytes\r\n# 1g =\u003e 1000000000 bytes\r\n# 1gb =\u003e 1024*1024*1024 bytes\r\n#\r\n# units are case insensitive so 1GB 1Gb 1gB are all the same.\r\n\r\n################################## INCLUDES ###################################\r\n\r\n# Include one or more other config files here.  This is useful if you\r\n# have a standard template that goes to all Redis servers but also need\r\n# to customize a few per-server settings.  Include files can include\r\n# other files, so use this wisely.\r\n#\r\n# Note that option \"include\" won't be rewritten by command \"CONFIG REWRITE\"\r\n# from admin or Redis Sentinel. Since Redis always uses the last processed\r\n# line as value of a configuration directive, you'd better put includes\r\n# at the beginning of this file to avoid overwriting config change at runtime.\r\n#\r\n# If instead you are interested in using includes to override configuration\r\n# options, it is better to use include as the last line.\r\n#\r\n# Included paths may contain wildcards. All files matching the wildcards will\r\n# be included in alphabetical order.\r\n# Note that if an include path contains a wildcards but no files match it when\r\n# the server is started, the include statement will be ignored and no error will\r\n# be emitted.  It is safe, therefore, to include wildcard files from empty\r\n# directories.\r\n#\r\n# include /path/to/local.conf\r\n# include /path/to/other.conf\r\n# include /path/to/fragments/*.conf\r\n#\r\n\r\n################################## MODULES #####################################\r\n\r\n# Load modules at startup. If the server is not able to load modules\r\n# it will abort. It is possible to use multiple loadmodule directives.\r\n#\r\n# loadmodule /path/to/my_module.so\r\n# loadmodule /path/to/other_module.so\r\n\r\n################################## NETWORK #####################################\r\n\r\n# By default, if no \"bind\" configuration directive is specified, Redis listens\r\n# for connections from all available network interfaces on the host machine.\r\n# It is possible to listen to just one or multiple selected interfaces using\r\n# the \"bind\" configuration directive, followed by one or more IP addresses.\r\n# Each address can be prefixed by \"-\", which means that redis will not fail to\r\n# start if the address is not available. Being not available only refers to\r\n# addresses that does not correspond to any network interface. Addresses that\r\n# are already in use will always fail, and unsupported protocols will always BE\r\n# silently skipped.\r\n#\r\n# Examples:\r\n#\r\n# bind 192.168.1.100 10.0.0.1     # listens on two specific IPv4 addresses\r\n# bind 127.0.0.1 ::1              # listens on loopback IPv4 and IPv6\r\n# bind * -::*                     # like the default, all available interfaces\r\n#\r\n# ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the\r\n# internet, binding to all the interfaces is dangerous and will expose the\r\n# instance to everybody on the internet. So by default we uncomment the\r\n# following bind directive, that will force Redis to listen only on the\r\n# IPv4 and IPv6 (if available) loopback interface addresses (this means Redis\r\n# will only be able to accept client connections from the same host that it is\r\n# running on).\r\n#\r\n# IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES\r\n# COMMENT OUT THE FOLLOWING LINE.\r\n#\r\n# You will also need to set a password unless you explicitly disable protected\r\n# mode.\r\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n# bind 127.0.0.1 -::1\r\n\r\n# By default, outgoing connections (from replica to master, from Sentinel to\r\n# instances, cluster bus, etc.) are not bound to a specific local address. In\r\n# most cases, this means the operating system will handle that based on routing\r\n# and the interface through which the connection goes out.\r\n#\r\n# Using bind-source-addr it is possible to configure a specific address to bind\r\n# to, which may also affect how the connection gets routed.\r\n#\r\n# Example:\r\n#\r\n# bind-source-addr 10.0.0.1\r\n\r\n# Protected mode is a layer of security protection, in order to avoid that\r\n# Redis instances left open on the internet are accessed and exploited.\r\n#\r\n# When protected mode is on and the default user has no password, the server\r\n# only accepts local connections from the IPv4 address (127.0.0.1), IPv6 address\r\n# (::1) or Unix domain sockets.\r\n#\r\n# By default protected mode is enabled. You should disable it only if\r\n# you are sure you want clients from other hosts to connect to Redis\r\n# even if no authentication is configured.\r\n# protected-mode yes\r\n\r\n# Redis uses default hardened security configuration directives to reduce the\r\n# attack surface on innocent users. Therefore, several sensitive configuration\r\n# directives are immutable, and some potentially-dangerous commands are blocked.\r\n#\r\n# Configuration directives that control files that Redis writes to (e.g., 'dir'\r\n# and 'dbfilename') and that aren't usually modified during runtime\r\n# are protected by making them immutable.\r\n#\r\n# Commands that can increase the attack surface of Redis and that aren't usually\r\n# called by users are blocked by default.\r\n#\r\n# These can be exposed to either all connections or just local ones by setting\r\n# each of the configs listed below to either of these values:\r\n#\r\n# no    - Block for any connection (remain immutable)\r\n# yes   - Allow for any connection (no protection)\r\n# local - Allow only for local connections. Ones originating from the\r\n#         IPv4 address (127.0.0.1), IPv6 address (::1) or Unix domain sockets.\r\n#\r\n# enable-protected-configs no\r\n# enable-debug-command no\r\n# enable-module-command no\r\n\r\n# Accept connections on the specified port, default is 6379 (IANA #815344).\r\n# If port 0 is specified Redis will not listen on a TCP socket.\r\nport 6379\r\n\r\n# TCP listen() backlog.\r\n#\r\n# In high requests-per-second environments you need a high backlog in order\r\n# to avoid slow clients connection issues. Note that the Linux kernel\r\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\r\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\r\n# in order to get the desired effect.\r\ntcp-backlog 511\r\n\r\n# Unix socket.\r\n#\r\n# Specify the path for the Unix socket that will be used to listen for\r\n# incoming connections. There is no default, so Redis will not listen\r\n# on a unix socket when not specified.\r\n#\r\n# unixsocket /run/redis.sock\r\n# unixsocketperm 700\r\n\r\n# Close the connection after a client is idle for N seconds (0 to disable)\r\ntimeout 0\r\n\r\n# TCP keepalive.\r\n#\r\n# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence\r\n# of communication. This is useful for two reasons:\r\n#\r\n# 1) Detect dead peers.\r\n# 2) Force network equipment in the middle to consider the connection to be\r\n#    alive.\r\n#\r\n# On Linux, the specified value (in seconds) is the period used to send ACKs.\r\n# Note that to close the connection the double of the time is needed.\r\n# On other kernels the period depends on the kernel configuration.\r\n#\r\n# A reasonable value for this option is 300 seconds, which is the new\r\n# Redis default starting with Redis 3.2.1.\r\ntcp-keepalive 300\r\n\r\n# Apply OS-specific mechanism to mark the listening socket with the specified\r\n# ID, to support advanced routing and filtering capabilities.\r\n#\r\n# On Linux, the ID represents a connection mark.\r\n# On FreeBSD, the ID represents a socket cookie ID.\r\n# On OpenBSD, the ID represents a route table ID.\r\n#\r\n# The default value is 0, which implies no marking is required.\r\n# socket-mark-id 0\r\n\r\n################################# TLS/SSL #####################################\r\n\r\n# By default, TLS/SSL is disabled. To enable it, the \"tls-port\" configuration\r\n# directive can be used to define TLS-listening ports. To enable TLS on the\r\n# default port, use:\r\n#\r\n# port 0\r\n# tls-port 6379\r\n\r\n# Configure a X.509 certificate and private key to use for authenticating the\r\n# server to connected clients, masters or cluster peers.  These files should be\r\n# PEM formatted.\r\n#\r\n# tls-cert-file redis.crt\r\n# tls-key-file redis.key\r\n#\r\n# If the key file is encrypted using a passphrase, it can be included here\r\n# as well.\r\n#\r\n# tls-key-file-pass secret\r\n\r\n# Normally Redis uses the same certificate for both server functions (accepting\r\n# connections) and client functions (replicating from a master, establishing\r\n# cluster bus connections, etc.).\r\n#\r\n# Sometimes certificates are issued with attributes that designate them as\r\n# client-only or server-only certificates. In that case it may be desired to use\r\n# different certificates for incoming (server) and outgoing (client)\r\n# connections. To do that, use the following directives:\r\n#\r\n# tls-client-cert-file client.crt\r\n# tls-client-key-file client.key\r\n#\r\n# If the key file is encrypted using a passphrase, it can be included here\r\n# as well.\r\n#\r\n# tls-client-key-file-pass secret\r\n\r\n# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange,\r\n# required by older versions of OpenSSL (\u003c3.0). Newer versions do not require\r\n# this configuration and recommend against it.\r\n#\r\n# tls-dh-params-file redis.dh\r\n\r\n# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL\r\n# clients and peers.  Redis requires an explicit configuration of at least one\r\n# of these, and will not implicitly use the system wide configuration.\r\n#\r\n# tls-ca-cert-file ca.crt\r\n# tls-ca-cert-dir /etc/ssl/certs\r\n\r\n# By default, clients (including replica servers) on a TLS port are required\r\n# to authenticate using valid client side certificates.\r\n#\r\n# If \"no\" is specified, client certificates are not required and not accepted.\r\n# If \"optional\" is specified, client certificates are accepted and must be\r\n# valid if provided, but are not required.\r\n#\r\n# tls-auth-clients no\r\n# tls-auth-clients optional\r\n\r\n# By default, a Redis replica does not attempt to establish a TLS connection\r\n# with its master.\r\n#\r\n# Use the following directive to enable TLS on replication links.\r\n#\r\n# tls-replication yes\r\n\r\n# By default, the Redis Cluster bus uses a plain TCP connection. To enable\r\n# TLS for the bus protocol, use the following directive:\r\n#\r\n# tls-cluster yes\r\n\r\n# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended\r\n# that older formally deprecated versions are kept disabled to reduce the attack surface.\r\n# You can explicitly specify TLS versions to support.\r\n# Allowed values are case insensitive and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\",\r\n# \"TLSv1.3\" (OpenSSL \u003e= 1.1.1) or any combination.\r\n# To enable only TLSv1.2 and TLSv1.3, use:\r\n#\r\n# tls-protocols \"TLSv1.2 TLSv1.3\"\r\n\r\n# Configure allowed ciphers.  See the ciphers(1ssl) manpage for more information\r\n# about the syntax of this string.\r\n#\r\n# Note: this configuration applies only to \u003c= TLSv1.2.\r\n#\r\n# tls-ciphers DEFAULT:!MEDIUM\r\n\r\n# Configure allowed TLSv1.3 ciphersuites.  See the ciphers(1ssl) manpage for more\r\n# information about the syntax of this string, and specifically for TLSv1.3\r\n# ciphersuites.\r\n#\r\n# tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256\r\n\r\n# When choosing a cipher, use the server's preference instead of the client\r\n# preference. By default, the server follows the client's preference.\r\n#\r\n# tls-prefer-server-ciphers yes\r\n\r\n# By default, TLS session caching is enabled to allow faster and less expensive\r\n# reconnections by clients that support it. Use the following directive to disable\r\n# caching.\r\n#\r\n# tls-session-caching no\r\n\r\n# Change the default number of TLS sessions cached. A zero value sets the cache\r\n# to unlimited size. The default size is 20480.\r\n#\r\n# tls-session-cache-size 5000\r\n\r\n# Change the default timeout of cached TLS sessions. The default timeout is 300\r\n# seconds.\r\n#\r\n# tls-session-cache-timeout 60\r\n\r\n################################# GENERAL #####################################\r\n\r\n# By default Redis does not run as a daemon. Use 'yes' if you need it.\r\n# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.\r\n# When Redis is supervised by upstart or systemd, this parameter has no impact.\r\ndaemonize no\r\n\r\n# If you run Redis from upstart or systemd, Redis can interact with your\r\n# supervision tree. Options:\r\n#   supervised no      - no supervision interaction\r\n#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode\r\n#                        requires \"expect stop\" in your upstart job config\r\n#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\r\n#                        on startup, and updating Redis status on a regular\r\n#                        basis.\r\n#   supervised auto    - detect upstart or systemd method based on\r\n#                        UPSTART_JOB or NOTIFY_SOCKET environment variables\r\n# Note: these supervision methods only signal \"process is ready.\"\r\n#       They do not enable continuous pings back to your supervisor.\r\n#\r\n# The default is \"no\". To run under upstart/systemd, you can simply uncomment\r\n# the line below:\r\n#\r\n# supervised auto\r\n\r\n# If a pid file is specified, Redis writes it where specified at startup\r\n# and removes it at exit.\r\n#\r\n# When the server runs non daemonized, no pid file is created if none is\r\n# specified in the configuration. When the server is daemonized, the pid file\r\n# is used even if not specified, defaulting to \"/var/run/redis.pid\".\r\n#\r\n# Creating a pid file is best effort: if Redis is not able to create it\r\n# nothing bad happens, the server will start and run normally.\r\n#\r\n# Note that on modern Linux systems \"/run/redis.pid\" is more conforming\r\n# and should be used instead.\r\npidfile /var/run/redis_6379.pid\r\n\r\n# Specify the server verbosity level.\r\n# This can be one of:\r\n# debug (a lot of information, useful for development/testing)\r\n# verbose (many rarely useful info, but not a mess like the debug level)\r\n# notice (moderately verbose, what you want in production probably)\r\n# warning (only very important / critical messages are logged)\r\nloglevel notice\r\n\r\n# Specify the log file name. Also the empty string can be used to force\r\n# Redis to log on the standard output. Note that if you use standard\r\n# output for logging but daemonize, logs will be sent to /dev/null\r\nlogfile \"\"\r\n\r\n# To enable logging to the system logger, just set 'syslog-enabled' to yes,\r\n# and optionally update the other syslog parameters to suit your needs.\r\n# syslog-enabled no\r\n\r\n# Specify the syslog identity.\r\n# syslog-ident redis\r\n\r\n# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.\r\n# syslog-facility local0\r\n\r\n# To disable the built in crash log, which will possibly produce cleaner core\r\n# dumps when they are needed, uncomment the following:\r\n#\r\n# crash-log-enabled no\r\n\r\n# To disable the fast memory check that's run as part of the crash log, which\r\n# will possibly let redis terminate sooner, uncomment the following:\r\n#\r\n# crash-memcheck-enabled no\r\n\r\n# Set the number of databases. The default database is DB 0, you can select\r\n# a different one on a per-connection basis using SELECT \u003cdbid\u003e where\r\n# dbid is a number between 0 and 'databases'-1\r\ndatabases 16\r\n\r\n# By default Redis shows an ASCII art logo only when started to log to the\r\n# standard output and if the standard output is a TTY and syslog logging is\r\n# disabled. Basically this means that normally a logo is displayed only in\r\n# interactive sessions.\r\n#\r\n# However it is possible to force the pre-4.0 behavior and always show a\r\n# ASCII art logo in startup logs by setting the following option to yes.\r\nalways-show-logo no\r\n\r\n# By default, Redis modifies the process title (as seen in 'top' and 'ps') to\r\n# provide some runtime information. It is possible to disable this and leave\r\n# the process name as executed by setting the following to no.\r\nset-proc-title yes\r\n\r\n# When changing the process title, Redis uses the following template to construct\r\n# the modified title.\r\n#\r\n# Template variables are specified in curly brackets. The following variables are\r\n# supported:\r\n#\r\n# {title}           Name of process as executed if parent, or type of child process.\r\n# {listen-addr}     Bind address or '*' followed by TCP or TLS port listening on, or\r\n#                   Unix socket if only that's available.\r\n# {server-mode}     Special mode, i.e. \"[sentinel]\" or \"[cluster]\".\r\n# {port}            TCP port listening on, or 0.\r\n# {tls-port}        TLS port listening on, or 0.\r\n# {unixsocket}      Unix domain socket listening on, or \"\".\r\n# {config-file}     Name of configuration file used.\r\n#\r\nproc-title-template \"{title} {listen-addr} {server-mode}\"\r\n\r\n################################ SNAPSHOTTING  ################################\r\n\r\n# Save the DB to disk.\r\n#\r\n# save \u003cseconds\u003e \u003cchanges\u003e [\u003cseconds\u003e \u003cchanges\u003e ...]\r\n#\r\n# Redis will save the DB if the given number of seconds elapsed and it\r\n# surpassed the given number of write operations against the DB.\r\n#\r\n# Snapshotting can be completely disabled with a single empty string argument\r\n# as in following example:\r\n#\r\n# save \"\"\r\n#\r\n# Unless specified otherwise, by default Redis will save the DB:\r\n#   * After 3600 seconds (an hour) if at least 1 change was performed\r\n#   * After 300 seconds (5 minutes) if at least 100 changes were performed\r\n#   * After 60 seconds if at least 10000 changes were performed\r\n#\r\n# You can set these explicitly by uncommenting the following line.\r\n#\r\n# save 3600 1 300 100 60 10000\r\n\r\n# By default Redis will stop accepting writes if RDB snapshots are enabled\r\n# (at least one save point) and the latest background save failed.\r\n# This will make the user aware (in a hard way) that data is not persisting\r\n# on disk properly, otherwise chances are that no one will notice and some\r\n# disaster will happen.\r\n#\r\n# If the background saving process will start working again Redis will\r\n# automatically allow writes again.\r\n#\r\n# However if you have setup your proper monitoring of the Redis server\r\n# and persistence, you may want to disable this feature so that Redis will\r\n# continue to work as usual even if there are problems with disk,\r\n# permissions, and so forth.\r\nstop-writes-on-bgsave-error yes\r\n\r\n# Compress string objects using LZF when dump .rdb databases?\r\n# By default compression is enabled as it's almost always a win.\r\n# If you want to save some CPU in the saving child set it to 'no' but\r\n# the dataset will likely be bigger if you have compressible values or keys.\r\nrdbcompression yes\r\n\r\n# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.\r\n# This makes the format more resistant to corruption but there is a performance\r\n# hit to pay (around 10%) when saving and loading RDB files, so you can disable it\r\n# for maximum performances.\r\n#\r\n# RDB files created with checksum disabled have a checksum of zero that will\r\n# tell the loading code to skip the check.\r\nrdbchecksum yes\r\n\r\n# Enables or disables full sanitization checks for ziplist and listpack etc when\r\n# loading an RDB or RESTORE payload. This reduces the chances of a assertion or\r\n# crash later on while processing commands.\r\n# Options:\r\n#   no         - Never perform full sanitization\r\n#   yes        - Always perform full sanitization\r\n#   clients    - Perform full sanitization only for user connections.\r\n#                Excludes: RDB files, RESTORE commands received from the master\r\n#                connection, and client connections which have the\r\n#                skip-sanitize-payload ACL flag.\r\n# The default should be 'clients' but since it currently affects cluster\r\n# resharding via MIGRATE, it is temporarily set to 'no' by default.\r\n#\r\n# sanitize-dump-payload no\r\n\r\n# The filename where to dump the DB\r\ndbfilename dump.rdb\r\n\r\n# Remove RDB files used by replication in instances without persistence\r\n# enabled. By default this option is disabled, however there are environments\r\n# where for regulations or other security concerns, RDB files persisted on\r\n# disk by masters in order to feed replicas, or stored on disk by replicas\r\n# in order to load them for the initial synchronization, should be deleted\r\n# ASAP. Note that this option ONLY WORKS in instances that have both AOF\r\n# and RDB persistence disabled, otherwise is completely ignored.\r\n#\r\n# An alternative (and sometimes better) way to obtain the same effect is\r\n# to use diskless replication on both master and replicas instances. However\r\n# in the case of replicas, diskless is not always an option.\r\nrdb-del-sync-files no\r\n\r\n# The working directory.\r\n#\r\n# The DB will be written inside this directory, with the filename specified\r\n# above using the 'dbfilename' configuration directive.\r\n#\r\n# The Append Only File will also be created inside this directory.\r\n#\r\n# Note that you must specify a directory here, not a file name.\r\ndir /data\r\n\r\n################################# REPLICATION #################################\r\n\r\n# Master-Replica replication. Use replicaof to make a Redis instance a copy of\r\n# another Redis server. A few things to understand ASAP about Redis replication.\r\n#\r\n#   +------------------+      +---------------+\r\n#   |      Master      | ---\u003e |    Replica    |\r\n#   | (receive writes) |      |  (exact copy) |\r\n#   +------------------+      +---------------+\r\n#\r\n# 1) Redis replication is asynchronous, but you can configure a master to\r\n#    stop accepting writes if it appears to be not connected with at least\r\n#    a given number of replicas.\r\n# 2) Redis replicas are able to perform a partial resynchronization with the\r\n#    master if the replication link is lost for a relatively small amount of\r\n#    time. You may want to configure the replication backlog size (see the next\r\n#    sections of this file) with a sensible value depending on your needs.\r\n# 3) Replication is automatic and does not need user intervention. After a\r\n#    network partition replicas automatically try to reconnect to masters\r\n#    and resynchronize with them.\r\n#\r\n# replicaof \u003cmasterip\u003e \u003cmasterport\u003e\r\n\r\n# If the master is password protected (using the \"requirepass\" configuration\r\n# directive below) it is possible to tell the replica to authenticate before\r\n# starting the replication synchronization process, otherwise the master will\r\n# refuse the replica request.\r\n#\r\n# masterauth \u003cmaster-password\u003e\r\n#\r\n# However this is not enough if you are using Redis ACLs (for Redis version\r\n# 6 or greater), and the default user is not capable of running the PSYNC\r\n# command and/or other commands needed for replication. In this case it's\r\n# better to configure a special user to use with replication, and specify the\r\n# masteruser configuration as such:\r\n#\r\n# masteruser \u003cusername\u003e\r\n#\r\n# When masteruser is specified, the replica will authenticate against its\r\n# master using the new AUTH form: AUTH \u003cusername\u003e \u003cpassword\u003e.\r\n\r\n# When a replica loses its connection with the master, or when the replication\r\n# is still in progress, the replica can act in two different ways:\r\n#\r\n# 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will\r\n#    still reply to client requests, possibly with out of date data, or the\r\n#    data set may just be empty if this is the first synchronization.\r\n#\r\n# 2) If replica-serve-stale-data is set to 'no' the replica will reply with error\r\n#    \"MASTERDOWN Link with MASTER is down and replica-serve-stale-data is set to 'no'\"\r\n#    to all data access commands, excluding commands such as:\r\n#    INFO, REPLICAOF, AUTH, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,\r\n#    UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,\r\n#    HOST and LATENCY.\r\n#\r\nreplica-serve-stale-data yes\r\n\r\n# You can configure a replica instance to accept writes or not. Writing against\r\n# a replica instance may be useful to store some ephemeral data (because data\r\n# written on a replica will be easily deleted after resync with the master) but\r\n# may also cause problems if clients are writing to it because of a\r\n# misconfiguration.\r\n#\r\n# Since Redis 2.6 by default replicas are read-only.\r\n#\r\n# Note: read only replicas are not designed to be exposed to untrusted clients\r\n# on the internet. It's just a protection layer against misuse of the instance.\r\n# Still a read only replica exports by default all the administrative commands\r\n# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve\r\n# security of read only replicas using 'rename-command' to shadow all the\r\n# administrative / dangerous commands.\r\nreplica-read-only yes\r\n\r\n# Replication SYNC strategy: disk or socket.\r\n#\r\n# New replicas and reconnecting replicas that are not able to continue the\r\n# replication process just receiving differences, need to do what is called a\r\n# \"full synchronization\". An RDB file is transmitted from the master to the\r\n# replicas.\r\n#\r\n# The transmission can happen in two different ways:\r\n#\r\n# 1) Disk-backed: The Redis master creates a new process that writes the RDB\r\n#                 file on disk. Later the file is transferred by the parent\r\n#                 process to the replicas incrementally.\r\n# 2) Diskless: The Redis master creates a new process that directly writes the\r\n#              RDB file to replica sockets, without touching the disk at all.\r\n#\r\n# With disk-backed replication, while the RDB file is generated, more replicas\r\n# can be queued and served with the RDB file as soon as the current child\r\n# producing the RDB file finishes its work. With diskless replication instead\r\n# once the transfer starts, new replicas arriving will be queued and a new\r\n# transfer will start when the current one terminates.\r\n#\r\n# When diskless replication is used, the master waits a configurable amount of\r\n# time (in seconds) before starting the transfer in the hope that multiple\r\n# replicas will arrive and the transfer can be parallelized.\r\n#\r\n# With slow disks and fast (large bandwidth) networks, diskless replication\r\n# works better.\r\nrepl-diskless-sync yes\r\n\r\n# When diskless replication is enabled, it is possible to configure the delay\r\n# the server waits in order to spawn the child that transfers the RDB via socket\r\n# to the replicas.\r\n#\r\n# This is important since once the transfer starts, it is not possible to serve\r\n# new replicas arriving, that will be queued for the next RDB transfer, so the\r\n# server waits a delay in order to let more replicas arrive.\r\n#\r\n# The delay is specified in seconds, and by default is 5 seconds. To disable\r\n# it entirely just set it to 0 seconds and the transfer will start ASAP.\r\nrepl-diskless-sync-delay 5\r\n\r\n# When diskless replication is enabled with a delay, it is possible to let\r\n# the replication start before the maximum delay is reached if the maximum\r\n# number of replicas expected have connected. Default of 0 means that the\r\n# maximum is not defined and Redis will wait the full delay.\r\nrepl-diskless-sync-max-replicas 0\r\n\r\n# -----------------------------------------------------------------------------\r\n# WARNING: RDB diskless load is experimental. Since in this setup the replica\r\n# does not immediately store an RDB on disk, it may cause data loss during\r\n# failovers. RDB diskless load + Redis modules not handling I/O reads may also\r\n# cause Redis to abort in case of I/O errors during the initial synchronization\r\n# stage with the master. Use only if you know what you are doing.\r\n# -----------------------------------------------------------------------------\r\n#\r\n# Replica can load the RDB it reads from the replication link directly from the\r\n# socket, or store the RDB to a file and read that file after it was completely\r\n# received from the master.\r\n#\r\n# In many cases the disk is slower than the network, and storing and loading\r\n# the RDB file may increase replication time (and even increase the master's\r\n# Copy on Write memory and replica buffers).\r\n# However, parsing the RDB file directly from the socket may mean that we have\r\n# to flush the contents of the current database before the full rdb was\r\n# received. For this reason we have the following options:\r\n#\r\n# \"disabled\"    - Don't use diskless load (store the rdb file to the disk first)\r\n# \"on-empty-db\" - Use diskless load only when it is completely safe.\r\n# \"swapdb\"      - Keep current db contents in RAM while parsing the data directly\r\n#                 from the socket. Replicas in this mode can keep serving current\r\n#                 data set while replication is in progress, except for cases where\r\n#                 they can't recognize master as having a data set from same\r\n#                 replication history.\r\n#                 Note that this requires sufficient memory, if you don't have it,\r\n#                 you risk an OOM kill.\r\nrepl-diskless-load disabled\r\n\r\n# Master send PINGs to its replicas in a predefined interval. It's possible to\r\n# change this interval with the repl_ping_replica_period option. The default\r\n# value is 10 seconds.\r\n#\r\n# repl-ping-replica-period 10\r\n\r\n# The following option sets the replication timeout for:\r\n#\r\n# 1) Bulk transfer I/O during SYNC, from the point of view of replica.\r\n# 2) Master timeout from the point of view of replicas (data, pings).\r\n# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).\r\n#\r\n# It is important to make sure that this value is greater than the value\r\n# specified for repl-ping-replica-period otherwise a timeout will be detected\r\n# every time there is low traffic between the master and the replica. The default\r\n# value is 60 seconds.\r\n#\r\n# repl-timeout 60\r\n\r\n# Disable TCP_NODELAY on the replica socket after SYNC?\r\n#\r\n# If you select \"yes\" Redis will use a smaller number of TCP packets and\r\n# less bandwidth to send data to replicas. But this can add a delay for\r\n# the data to appear on the replica side, up to 40 milliseconds with\r\n# Linux kernels using a default configuration.\r\n#\r\n# If you select \"no\" the delay for data to appear on the replica side will\r\n# be reduced but more bandwidth will be used for replication.\r\n#\r\n# By default we optimize for low latency, but in very high traffic conditions\r\n# or when the master and replicas are many hops away, turning this to \"yes\" may\r\n# be a good idea.\r\nrepl-disable-tcp-nodelay no\r\n\r\n# Set the replication backlog size. The backlog is a buffer that accumulates\r\n# replica data when replicas are disconnected for some time, so that when a\r\n# replica wants to reconnect again, often a full resync is not needed, but a\r\n# partial resync is enough, just passing the portion of data the replica\r\n# missed while disconnected.\r\n#\r\n# The bigger the replication backlog, the longer the replica can endure the\r\n# disconnect and later be able to perform a partial resynchronization.\r\n#\r\n# The backlog is only allocated if there is at least one replica connected.\r\n#\r\n# repl-backlog-size 1mb\r\n\r\n# After a master has no connected replicas for some time, the backlog will be\r\n# freed. The following option configures the amount of seconds that need to\r\n# elapse, starting from the time the last replica disconnected, for the backlog\r\n# buffer to be freed.\r\n#\r\n# Note that replicas never free the backlog for timeout, since they may be\r\n# promoted to masters later, and should be able to correctly \"partially\r\n# resynchronize\" with other replicas: hence they should always accumulate backlog.\r\n#\r\n# A value of 0 means to never release the backlog.\r\n#\r\n# repl-backlog-ttl 3600\r\n\r\n# The replica priority is an integer number published by Redis in the INFO\r\n# output. It is used by Redis Sentinel in order to select a replica to promote\r\n# into a master if the master is no longer working correctly.\r\n#\r\n# A replica with a low priority number is considered better for promotion, so\r\n# for instance if there are three replicas with priority 10, 100, 25 Sentinel\r\n# will pick the one with priority 10, that is the lowest.\r\n#\r\n# However a special priority of 0 marks the replica as not able to perform the\r\n# role of master, so a replica with priority of 0 will never be selected by\r\n# Redis Sentinel for promotion.\r\n#\r\n# By default the priority is 100.\r\nreplica-priority 100\r\n\r\n# The propagation error behavior controls how Redis will behave when it is\r\n# unable to handle a command being processed in the replication stream from a master\r\n# or processed while reading from an AOF file. Errors that occur during propagation\r\n# are unexpected, and can cause data inconsistency. However, there are edge cases\r\n# in earlier versions of Redis where it was possible for the server to replicate or persist\r\n# commands that would fail on future versions. For this reason the default behavior\r\n# is to ignore such errors and continue processing commands.\r\n#\r\n# If an application wants to ensure there is no data divergence, this configuration\r\n# should be set to 'panic' instead. The value can also be set to 'panic-on-replicas'\r\n# to only panic when a replica encounters an error on the replication stream. One of\r\n# these two panic values will become the default value in the future once there are\r\n# sufficient safety mechanisms in place to prevent false positive crashes.\r\n#\r\n# propagation-error-behavior ignore\r\n\r\n# Replica ignore disk write errors controls the behavior of a replica when it is\r\n# unable to persist a write command received from its master to disk. By default,\r\n# this configuration is set to 'no' and will crash the replica in this condition.\r\n# It is not recommended to change this default, however in order to be compatible\r\n# with older versions of Redis this config can be toggled to 'yes' which will just\r\n# log a warning and execute the write command it got from the master.\r\n#\r\n# replica-ignore-disk-write-errors no\r\n\r\n# -----------------------------------------------------------------------------\r\n# By default, Redis Sentinel includes all replicas in its reports. A replica\r\n# can be excluded from Redis Sentinel's announcements. An unannounced replica\r\n# will be ignored by the 'sentinel replicas \u003cmaster\u003e' command and won't be\r\n# exposed to Redis Sentinel's clients.\r\n#\r\n# This option does not change the behavior of replica-priority. Even with\r\n# replica-announced set to 'no', the replica can be promoted to master. To\r\n# prevent this behavior, set replica-priority to 0.\r\n#\r\n# replica-announced yes\r\n\r\n# It is possible for a master to stop accepting writes if there are less than\r\n# N replicas connected, having a lag less or equal than M seconds.\r\n#\r\n# The N replicas need to be in \"online\" state.\r\n#\r\n# The lag in seconds, that must be \u003c= the specified value, is calculated from\r\n# the last ping received from the replica, that is usually sent every second.\r\n#\r\n# This option does not GUARANTEE that N replicas will accept the write, but\r\n# will limit the window of exposure for lost writes in case not enough replicas\r\n# are available, to the specified number of seconds.\r\n#\r\n# For example to require at least 3 replicas with a lag \u003c= 10 seconds use:\r\n#\r\n# min-replicas-to-write 3\r\n# min-replicas-max-lag 10\r\n#\r\n# Setting one or the other to 0 disables the feature.\r\n#\r\n# By default min-replicas-to-write is set to 0 (feature disabled) and\r\n# min-replicas-max-lag is set to 10.\r\n\r\n# A Redis master is able to list the address and port of the attached\r\n# replicas in different ways. For example the \"INFO replication\" section\r\n# offers this information, which is used, among other tools, by\r\n# Redis Sentinel in order to discover replica instances.\r\n# Another place where this info is available is in the output of the\r\n# \"ROLE\" command of a master.\r\n#\r\n# The listed IP address and port normally reported by a replica is\r\n# obtained in the following way:\r\n#\r\n#   IP: The address is auto detected by checking the peer address\r\n#   of the socket used by the replica to connect with the master.\r\n#\r\n#   Port: The port is communicated by the replica during the replication\r\n#   handshake, and is normally the port that the replica is using to\r\n#   listen for connections.\r\n#\r\n# However when port forwarding or Network Address Translation (NAT) is\r\n# used, the replica may actually be reachable via different IP and port\r\n# pairs. The following two options can be used by a replica in order to\r\n# report to its master a specific set of IP and port, so that both INFO\r\n# and ROLE will report those values.\r\n#\r\n# There is no need to use both the options if you need to override just\r\n# the port or the IP address.\r\n#\r\n# replica-announce-ip 5.5.5.5\r\n# replica-announce-port 1234\r\n\r\n############################### KEYS TRACKING #################################\r\n\r\n# Redis implements server assisted support for client side caching of values.\r\n# This is implemented using an invalidation table that remembers, using\r\n# a radix key indexed by key name, what clients have which keys. In turn\r\n# this is used in order to send invalidation messages to clients. Please\r\n# check this page to understand more about the feature:\r\n#\r\n#   https://redis.io/topics/client-side-caching\r\n#\r\n# When tracking is enabled for a client, all the read only queries are assumed\r\n# to be cached: this will force Redis to store information in the invalidation\r\n# table. When keys are modified, such information is flushed away, and\r\n# invalidation messages are sent to the clients. However if the workload is\r\n# heavily dominated by reads, Redis could use more and more memory in order\r\n# to track the keys fetched by many clients.\r\n#\r\n# For this reason it is possible to configure a maximum fill value for the\r\n# invalidation table. By default it is set to 1M of keys, and once this limit\r\n# is reached, Redis will start to evict keys in the invalidation table\r\n# even if they were not modified, just to reclaim memory: this will in turn\r\n# force the clients to invalidate the cached values. Basically the table\r\n# maximum size is a trade off between the memory you want to spend server\r\n# side to track information about who cached what, and the ability of clients\r\n# to retain cached objects in memory.\r\n#\r\n# If you set the value to 0, it means there are no limits, and Redis will\r\n# retain as many keys as needed in the invalidation table.\r\n# In the \"stats\" INFO section, you can find information about the number of\r\n# keys in the invalidation table at every given moment.\r\n#\r\n# Note: when key tracking is used in broadcasting mode, no memory is used\r\n# in the server side so this setting is useless.\r\n#\r\n# tracking-table-max-keys 1000000\r\n\r\n################################## SECURITY ###################################\r\n\r\n# Warning: since Redis is pretty fast, an outside user can try up to\r\n# 1 million passwords per second against a modern box. This means that you\r\n# should use very strong passwords, otherwise they will be very easy to break.\r\n# Note that because the password is really a shared secret between the client\r\n# and the server, and should not be memorized by any human, the password\r\n# can be easily a long string from /dev/urandom or whatever, so by using a\r\n# long and unguessable password no brute force attack will be possible.\r\n\r\n# Redis ACL users are defined in the following format:\r\n#\r\n#   user \u003cusername\u003e ... acl rules ...\r\n#\r\n# For example:\r\n#\r\n#   user worker +@list +@connection ~jobs:* on \u003effa9203c493aa99\r\n#\r\n# The special username \"default\" is used for new connections. If this user\r\n# has the \"nopass\" rule, then new connections will be immediately authenticated\r\n# as the \"default\" user without the need of any password provided via the\r\n# AUTH command. Otherwise if the \"default\" user is not flagged with \"nopass\"\r\n# the connections will start in not authenticated state, and will require\r\n# AUTH (or the HELLO command AUTH option) in order to be authenticated and\r\n# start to work.\r\n#\r\n# The ACL rules that describe what a user can do are the following:\r\n#\r\n#  on           Enable the user: it is possible to authenticate as this user.\r\n#  off          Disable the user: it's no longer possible to authenticate\r\n#               with this user, however the already authenticated connections\r\n#               will still work.\r\n#  skip-sanitize-payload    RESTORE dump-payload sanitization is skipped.\r\n#  sanitize-payload         RESTORE dump-payload is sanitized (default).\r\n#  +\u003ccommand\u003e   Allow the execution of that command.\r\n#               May be used with `|` for allowing subcommands (e.g \"+config|get\")\r\n#  -\u003ccommand\u003e   Disallow the execution of that command.\r\n#               May be used with `|` for blocking subcommands (e.g \"-config|set\")\r\n#  +@\u003ccategory\u003e Allow the execution of all the commands in such category\r\n#               with valid categories are like @admin, @set, @sortedset, ...\r\n#               and so forth, see the full list in the server.c file where\r\n#               the Redis command table is described and defined.\r\n#               The special category @all means all the commands, but currently\r\n#               present in the server, and that will be loaded in the future\r\n#               via modules.\r\n#  +\u003ccommand\u003e|first-arg  Allow a specific first argument of an otherwise\r\n#                        disabled command. It is only supported on commands with\r\n#                        no sub-commands, and is not allowed as negative form\r\n#                        like -SELECT|1, only additive starting with \"+\". This\r\n#                        feature is deprecated and may be removed in the future.\r\n#  allcommands  Alias for +@all. Note that it implies the ability to execute\r\n#               all the future commands loaded via the modules system.\r\n#  nocommands   Alias for -@all.\r\n#  ~\u003cpattern\u003e   Add a pattern of keys that can be mentioned as part of\r\n#               commands. For instance ~* allows all the keys. The pattern\r\n#               is a glob-style pattern like the one of KEYS.\r\n#               It is possible to specify multiple patterns.\r\n# %R~\u003cpattern\u003e  Add key read pattern that specifies which keys can be read \r\n#               from.\r\n# %W~\u003cpattern\u003e  Add key write pattern that specifies which keys can be\r\n#               written to. \r\n#  allkeys      Alias for ~*\r\n#  resetkeys    Flush the list of allowed keys patterns.\r\n#  \u0026\u003cpattern\u003e   Add a glob-style pattern of Pub/Sub channels that can be\r\n#               accessed by the user. It is possible to specify multiple channel\r\n#               patterns.\r\n#  allchannels  Alias for \u0026*\r\n#  resetchannels            Flush the list of allowed channel patterns.\r\n#  \u003e\u003cpassword\u003e  Add this password to the list of valid password for the user.\r\n#               For example \u003emypass will add \"mypass\" to the list.\r\n#               This directive clears the \"nopass\" flag (see later).\r\n#  \u003c\u003cpassword\u003e  Remove this password from the list of valid passwords.\r\n#  nopass       All the set passwords of the user are removed, and the user\r\n#               is flagged as requiring no password: it means that every\r\n#               password will work against this user. If this directive is\r\n#               used for the default user, every new connection will be\r\n#               immediately authenticated with the default user without\r\n#               any explicit AUTH command required. Note that the \"resetpass\"\r\n#               directive will clear this condition.\r\n#  resetpass    Flush the list of allowed passwords. Moreover removes the\r\n#               \"nopass\" status. After \"resetpass\" the user has no associated\r\n#               passwords and there is no way to authenticate without adding\r\n#               some password (or setting it as \"nopass\" later).\r\n#  reset        Performs the following actions: resetpass, resetkeys, off,\r\n#               -@all. The user returns to the same state it has immediately\r\n#               after its creation.\r\n# (\u003coptions\u003e)   Create a new selector with the options specified within the\r\n#               parentheses and attach it to the user. Each option should be \r\n#               space separated. The first character must be ( and the last \r\n#               character must be ).\r\n# clearselectors            Remove all of the currently attached selectors. \r\n#                           Note this does not change the \"root\" user permissions,\r\n#                           which are the permissions directly applied onto the\r\n#                           user (outside the parentheses).\r\n#\r\n# ACL rules can be specified in any order: for instance you can start with\r\n# passwords, then flags, or key patterns. However note that the additive\r\n# and subtractive rules will CHANGE MEANING depending on the ordering.\r\n# For instance see the following example:\r\n#\r\n#   user alice on +@all -DEBUG ~* \u003esomepassword\r\n#\r\n# This will allow \"alice\" to use all the commands with the exception of the\r\n# DEBUG command, since +@all added all the commands to the set of the commands\r\n# alice can use, and later DEBUG was removed. However if we invert the order\r\n# of two ACL rules the result will be different:\r\n#\r\n#   user alice on -DEBUG +@all ~* \u003esomepassword\r\n#\r\n# Now DEBUG was removed when alice had yet no commands in the set of allowed\r\n# commands, later all the commands are added, so the user will be able to\r\n# execute everything.\r\n#\r\n# Basically ACL rules are processed left-to-right.\r\n#\r\n# The following is a list of command categories and their meanings:\r\n# * keyspace - Writing or reading from keys, databases, or their metadata \r\n#     in a type agnostic way. Includes DEL, RESTORE, DUMP, RENAME, EXISTS, DBSIZE,\r\n#     KEYS, EXPIRE, TTL, FLUSHALL, etc. Commands that may modify the keyspace,\r\n#     key or metadata will also have `write` category. Commands that only read\r\n#     the keyspace, key or metadata will have the `read` category.\r\n# * read - Reading from keys (values or metadata). Note that commands that don't\r\n#     interact with keys, will not have either `read` or `write`.\r\n# * write - Writing to keys (values or metadata)\r\n# * admin - Administrative commands. Normal applications will never need to use\r\n#     these. Includes REPLICAOF, CONFIG, DEBUG, SAVE, MONITOR, ACL, SHUTDOWN, etc.\r\n# * dangerous - Potentially dangerous (each should be considered with care for\r\n#     various reasons). This includes FLUSHALL, MIGRATE, RESTORE, SORT, KEYS,\r\n#     CLIENT, DEBUG, INFO, CONFIG, SAVE, REPLICAOF, etc.\r\n# * connection - Commands affecting the connection or other connections.\r\n#     This includes AUTH, SELECT, COMMAND, CLIENT, ECHO, PING, etc.\r\n# * blocking - Potentially blocking the connection until released by another\r\n#     command.\r\n# * fast - Fast O(1) commands. May loop on the number of arguments, but not the\r\n#     number of elements in the key.\r\n# * slow - All commands that are not Fast.\r\n# * pubsub - PUBLISH / SUBSCRIBE related\r\n# * transaction - WATCH / MULTI / EXEC related commands.\r\n# * scripting - Scripting related.\r\n# * set - Data type: sets related.\r\n# * sortedset - Data type: zsets related.\r\n# * list - Data type: lists related.\r\n# * hash - Data type: hashes related.\r\n# * string - Data type: strings related.\r\n# * bitmap - Data type: bitmaps related.\r\n# * hyperloglog - Data type: hyperloglog related.\r\n# * geo - Data type: geo related.\r\n# * stream - Data type: streams related.\r\n#\r\n# For more information about ACL configuration please refer to\r\n# the Redis web site at https://redis.io/topics/acl\r\n\r\n# ACL LOG\r\n#\r\n# The ACL Log tracks failed commands and authentication events associated\r\n# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked\r\n# by ACLs. The ACL Log is stored in memory. You can reclaim memory with\r\n# ACL LOG RESET. Define the maximum entry length of the ACL Log below.\r\nacllog-max-len 128\r\n\r\n# Using an external ACL file\r\n#\r\n# Instead of configuring users here in this file, it is possible to use\r\n# a stand-alone file just listing users. The two methods cannot be mixed:\r\n# if you configure users here and at the same time you activate the external\r\n# ACL file, the server will refuse to start.\r\n#\r\n# The format of the external ACL user file is exactly the same as the\r\n# format that is used inside redis.conf to describe users.\r\n#\r\n# aclfile /etc/redis/users.acl\r\n\r\n# IMPORTANT NOTE: starting with Redis 6 \"requirepass\" is just a compatibility\r\n# layer on top of the new ACL system. The option effect will be just setting\r\n# the password for the default user. Clients will still authenticate using\r\n# AUTH \u003cpassword\u003e as usually, or more explicitly with AUTH default \u003cpassword\u003e\r\n# if they follow the new protocol: both will work.\r\n#\r\n# The requirepass is not compatible with aclfile option and the ACL LOAD\r\n# command, these will cause requirepass to be ignored.\r\n#\r\n# requirepass foobared\r\n\r\n# New users are initialized with restrictive permissions by default, via the\r\n# equivalent of this ACL rule 'off resetkeys -@all'. Starting with Redis 6.2, it\r\n# is possible to manage access to Pub/Sub channels with ACL rules as well. The\r\n# default Pub/Sub channels permission if new users is controlled by the\r\n# acl-pubsub-default configuration directive, which accepts one of these values:\r\n#\r\n# allchannels: grants access to all Pub/Sub channels\r\n# resetchannels: revokes access to all Pub/Sub channels\r\n#\r\n# From Redis 7.0, acl-pubsub-default defaults to 'resetchannels' permission.\r\n#\r\n# acl-pubsub-default resetchannels\r\n\r\n# Command renaming (DEPRECATED).\r\n#\r\n# ------------------------------------------------------------------------\r\n# WARNING: avoid using this option if possible. Instead use ACLs to remove\r\n# commands from the default user, and put them only in some admin user you\r\n# create for administrative purposes.\r\n# ------------------------------------------------------------------------\r\n#\r\n# It is possible to change the name of dangerous commands in a shared\r\n# environment. For instance the CONFIG command may be renamed into something\r\n# hard to guess so that it will still be available for internal-use tools\r\n# but not available for general clients.\r\n#\r\n# Example:\r\n#\r\n# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\r\n#\r\n# It is also possible to completely kill a command by renaming it into\r\n# an empty string:\r\n#\r\n# rename-command CONFIG \"\"\r\n#\r\n# Please note that changing the name of commands that are logged into the\r\n# AOF file or transmitted to replicas may cause problems.\r\n\r\n################################### CLIENTS ####################################\r\n\r\n# Set the max number of connected clients at the same time. By default\r\n# this limit is set to 10000 clients, however if the Redis server is not\r\n# able to configure the process file limit to allow for the specified limit\r\n# the max number of allowed clients is set to the current file limit\r\n# minus 32 (as Redis reserves a few file descriptors for internal uses).\r\n#\r\n# Once the limit is reached Redis will close all the new connections sending\r\n# an error 'max number of clients reached'.\r\n#\r\n# IMPORTANT: When Redis Cluster is used, the max number of connections is also\r\n# shared with the cluster bus: every node in the cluster will use two\r\n# connections, one incoming and another outgoing. It is important to size the\r\n# limit accordingly in case of very large clusters.\r\n#\r\n# maxclients 10000\r\n\r\n############################## MEMORY MANAGEMENT ################################\r\n\r\n# Set a memory usage limit to the specified amount of bytes.\r\n# When the memory limit is reached Redis will try to remove keys\r\n# according to the eviction policy selected (see maxmemory-policy).\r\n#\r\n# If Redis can't remove keys according to the policy, or if the policy is\r\n# set to 'noeviction', Redis will start to reply with errors to commands\r\n# that would use more memory, like SET, LPUSH, and so on, and will continue\r\n# to reply to read-only commands like GET.\r\n#\r\n# This option is usually useful when using Redis as an LRU or LFU cache, or to\r\n# set a hard memory limit for an instance (using the 'noeviction' policy).\r\n#\r\n# WARNING: If you have replicas attached to an instance with maxmemory on,\r\n# the size of the output buffers needed to feed the replicas are subtracted\r\n# from the used memory count, so that network problems / resyncs will\r\n# not trigger a loop where keys are evicted, and in turn the output\r\n# buffer of replicas is full with DELs of keys evicted triggering the deletion\r\n# of more keys, and so forth until the database is completely emptied.\r\n#\r\n# In short... if you have replicas attached it is suggested that you set a lower\r\n# limit for maxmemory so that there is some free RAM on the system for replica\r\n# output buffers (but this is not needed if the policy is 'noeviction').\r\n#\r\n# maxmemory \u003cbytes\u003e\r\n\r\n# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory\r\n# is reached. You can select one from the following behaviors:\r\n#\r\n# volatile-lru -\u003e Evict using approximated LRU, only keys with an expire set.\r\n# allkeys-lru -\u003e Evict any key using approximated LRU.\r\n# volatile-lfu -\u003e Evict using approximated LFU, only keys with an expire set.\r\n# allkeys-lfu -\u003e Evict any key using approximated LFU.\r\n# volatile-random -\u003e Remove a random key having an expire set.\r\n# allkeys-random -\u003e Remove a random key, any key.\r\n# volatile-ttl -\u003e Remove the key with the nearest expire time (minor TTL)\r\n# noeviction -\u003e Don't evict anything, just return an error on write operations.\r\n#\r\n# LRU means Least Recently Used\r\n# LFU means Least Frequently Used\r\n#\r\n# Both LRU, LFU and volatile-ttl are implemented using approximated\r\n# randomized algorithms.\r\n#\r\n# Note: with any of the above policies, when there are no suitable keys for\r\n# eviction, Redis will return an error on write operations that require\r\n# more memory. These are usually commands that create new keys, add data or\r\n# modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,\r\n# SORT (due to the STORE argument), and EXEC (if the transaction includes any\r\n# command that requires memory).\r\n#\r\n# The default is:\r\n#\r\n# maxmemory-policy noeviction\r\n\r\n# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated\r\n# algorithms (in order to save memory), so you can tune it for speed or\r\n# accuracy. By default Redis will check five keys and pick the one that was\r\n# used least recently, you can change the sample size using the following\r\n# configuration directive.\r\n#\r\n# The default of 5 produces good enough results. 10 Approximates very closely\r\n# true LRU but costs more CPU. 3 is faster but not very accurate.\r\n#\r\n# maxmemory-samples 5\r\n\r\n# Eviction processing is designed to function well with the default setting.\r\n# If there is an unusually large amount of write traffic, this value may need to\r\n# be increased.  Decreasing this value may reduce latency at the risk of\r\n# eviction processing effectiveness\r\n#   0 = minimum latency, 10 = default, 100 = process without regard to latency\r\n#\r\n# maxmemory-eviction-tenacity 10\r\n\r\n# Starting from Redis 5, by default a replica will ignore its maxmemory setting\r\n# (unless it is promoted to master after a failover or manually). It means\r\n# that the eviction of keys will be just handled by the master, sending the\r\n# DEL commands to the replica as keys evict in the master side.\r\n#\r\n# This behavior ensures that masters and replicas stay consistent, and is usually\r\n# what you want, however if your replica is writable, or you want the replica\r\n# to have a different memory setting, and you are sure all the writes performed\r\n# to the replica are idempotent, then you may change this default (but be sure\r\n# to understand what you are doing).\r\n#\r\n# Note that since the replica by default does not evict, it may end using more\r\n# memory than the one set via maxmemory (there are certain buffers that may\r\n# be larger on the replica, or data structures may sometimes take more memory\r\n# and so forth). So make sure you monitor your replicas and make sure they\r\n# have enough memory to never hit a real out-of-memory condition before the\r\n# master hits the configured maxmemory setting.\r\n#\r\n# replica-ignore-maxmemory yes\r\n\r\n# Redis reclaims expired keys in two ways: upon access when those keys are\r\n# found to be expired, and also in background, in what is called the\r\n# \"active expire key\". The key space is slowly and interactively scanned\r\n# looking for expired keys to reclaim, so that it is possible to free memory\r\n# of keys that are expired and will never be accessed again in a short time.\r\n#\r\n# The default effort of the expire cycle will try to avoid having more than\r\n# ten percent of expired keys still in memory, and will try to avoid consuming\r\n# more than 25% of total memory and to add latency to the system. However\r\n# it is possible to increase the expire \"effort\" that is normally set to\r\n# \"1\", to a greater value, up to the value \"10\". At its maximum value the\r\n# system will use more CPU, longer cycles (and technically may introduce\r\n# more latency), and will tolerate less already expired keys still present\r\n# in the system. It's a tradeoff between memory, CPU and latency.\r\n#\r\n# active-expire-effort 1\r\n\r\n############################# LAZY FREEING ####################################\r\n\r\n# Redis has two primitives to delete keys. One is called DEL and is a blocking\r\n# deletion of the object. It means that the server stops processing new commands\r\n# in order to reclaim all the memory associated with an object in a synchronous\r\n# way. If the key deleted is associated with a small object, the time needed\r\n# in order to execute the DEL command is very small and comparable to most other\r\n# O(1) or O(log_N) commands in Redis. However if the key is associated with an\r\n# aggregated value containing millions of elements, the server can block for\r\n# a long time (even seconds) in order to complete the operation.\r\n#\r\n# For the above reasons Redis also offers non blocking deletion primitives\r\n# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and\r\n# FLUSHDB commands, in order to reclaim memory in background. Those commands\r\n# are executed in constant time. Another thread will incrementally free the\r\n# object in the background as fast as possible.\r\n#\r\n# DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.\r\n# It's up to the design of the application to understand when it is a good\r\n# idea to use one or the other. However the Redis server sometimes has to\r\n# delete keys or flush the whole database as a side effect of other operations.\r\n# Specifically Redis deletes objects independently of a user call in the\r\n# following scenarios:\r\n#\r\n# 1) On eviction, because of the maxmemory and maxmemory policy configurations,\r\n#    in order to make room for new data, without going over the specified\r\n#    memory limit.\r\n# 2) Because of expire: when a key with an associated time to live (see the\r\n#    EXPIRE command) must be deleted from memory.\r\n# 3) Because of a side effect of a command that stores data on a key that may\r\n#    already exist. For example the RENAME command may delete the old key\r\n#    content when it is replaced with another one. Similarly SUNIONSTORE\r\n#    or SORT with STORE option may delete existing keys. The SET command\r\n#    itself removes any old content of the specified key in order to replace\r\n#    it with the specified string.\r\n# 4) During replication, when a replica performs a full resynchronization with\r\n#    its master, the content of the whole database is removed in order to\r\n#    load the RDB file just transferred.\r\n#\r\n# In all the above cases the default is to delete objects in a blocking way,\r\n# like if DEL was called. However you can configure each case specifically\r\n# in order to instead release memory in a non-blocking way like if UNLINK\r\n# was called, using the following configuration directives.\r\n\r\nlazyfree-lazy-eviction no\r\nlazyfree-lazy-expire no\r\nlazyfree-lazy-server-del no\r\nreplica-lazy-flush no\r\n\r\n# It is also possible, for the case when to replace the user code DEL calls\r\n# with UNLINK calls is not easy, to modify the default behavior of the DEL\r\n# command to act exactly like UNLINK, using the following configuration\r\n# directive:\r\n\r\nlazyfree-lazy-user-del no\r\n\r\n# FLUSHDB, FLUSHALL, SCRIPT FLUSH and FUNCTION FLUSH support both asynchronous and synchronous\r\n# deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the\r\n# commands. When neither flag is passed, this directive will be used to determine\r\n# if the data should be deleted asynchronously.\r\n\r\nlazyfree-lazy-user-flush no\r\n\r\n################################ THREADED I/O #################################\r\n\r\n# Redis is mostly single threaded, however there are certain threaded\r\n# operations such as UNLINK, slow I/O accesses and other things that are\r\n# performed on side threads.\r\n#\r\n# Now it is also possible to handle Redis clients socket reads and writes\r\n# in different I/O threads. Since especially writing is so slow, normally\r\n# Redis users use pipelining in order to speed up the Redis performances per\r\n# core, and spawn multiple instances in order to scale more. Using I/O\r\n# threads it is possible to easily speedup two times Redis without resorting\r\n# to pipelining nor sharding of the instance.\r\n#\r\n# By default threading is disabled, we suggest enabling it only in machines\r\n# that have at least 4 or more cores, leaving at least one spare core.\r\n# Using more than 8 threads is unlikely to help much. We also recommend using\r\n# threaded I/O only if you actually have performance problems, with Redis\r\n# instances being able to use a quite big percentage of CPU time, otherwise\r\n# there is no point in using this feature.\r\n#\r\n# So for instance if you have a four cores boxes, try to use 2 or 3 I/O\r\n# threads, if you have a 8 cores, try to use 6 threads. In order to\r\n# enable I/O threads use the following configuration directive:\r\n#\r\n# io-threads 4\r\n#\r\n# Setting io-threads to 1 will just use the main thread as usual.\r\n# When I/O threads are enabled, we only use threads for writes, that is\r\n# to thread the write(2) syscall and transfer the client buffers to the\r\n# socket. However it is also possible to enable threading of reads and\r\n# protocol parsing using the following configuration directive, by setting\r\n# it to yes:\r\n#\r\n# io-threads-do-reads no\r\n#\r\n# Usually threading reads doesn't help much.\r\n#\r\n# NOTE 1: This configuration directive cannot be changed at runtime via\r\n# CONFIG SET. Also, this feature currently does not work when SSL is\r\n# enabled.\r\n#\r\n# NOTE 2: If you want to test the Redis speedup using redis-benchmark, make\r\n# sure you also run the benchmark itself in threaded mode, using the\r\n# --threads option to match the number of Redis threads, otherwise you'll not\r\n# be able to notice the improvements.\r\n\r\n############################ KERNEL OOM CONTROL ##############################\r\n\r\n# On Linux, it is possible to hint the kernel OOM killer on what processes\r\n# should be killed first when out of memory.\r\n#\r\n# Enabling this feature makes Redis actively control the oom_score_adj value\r\n# for all its processes, depending on their role. The default scores will\r\n# attempt to have background child processes killed before all others, and\r\n# replicas killed before masters.\r\n#\r\n# Redis supports these options:\r\n#\r\n# no:       Don't make changes to oom-score-adj (default).\r\n# yes:      Alias to \"relative\" see below.\r\n# absolute: Values in oom-score-adj-values are written as is to the kernel.\r\n# relative: Values are used relative to the initial value of oom_score_adj when\r\n#           the server starts and are then clamped to a range of -1000 to 1000.\r\n#           Because typically the initial value is 0, they will often match the\r\n#           absolute values.\r\noom-score-adj no\r\n\r\n# When oom-score-adj is used, this directive controls the specific values used\r\n# for master, replica and background child processes. Values range -2000 to\r\n# 2000 (higher means more likely to be killed).\r\n#\r\n# Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)\r\n# can freely increase their value, but not decrease it below its initial\r\n# settings. This means that setting oom-score-adj to \"relative\" and setting the\r\n# oom-score-adj-values to positive values will always succeed.\r\noom-score-adj-values 0 200 800\r\n\r\n\r\n#################### KERNEL transparent hugepage CONTROL ######################\r\n\r\n# Usually the kernel Transparent Huge Pages control is set to \"madvise\" or\r\n# or \"never\" by default (/sys/kernel/mm/transparent_hugepage/enabled), in which\r\n# case this config has no effect. On systems in which it is set to \"always\",\r\n# redis will attempt to disable it specifically for the redis process in order\r\n# to avoid latency problems specifically with fork(2) and CoW.\r\n# If for some reason you prefer to keep it enabled, you can set this config to\r\n# \"no\" and the kernel global to \"always\".\r\n\r\ndisable-thp yes\r\n\r\n############################## APPEND ONLY MODE ###############################\r\n\r\n# By default Redis asynchronously dumps the dataset on disk. This mode is\r\n# good enough in many applications, but an issue with the Redis process or\r\n# a power outage may result into a few minutes of writes lost (depending on\r\n# the configured save points).\r\n#\r\n# The Append Only File is an alternative persistence mode that provides\r\n# much better durability. For instance using the default data fsync policy\r\n# (see later in the config file) Redis can lose just one second of writes in a\r\n# dramatic event like a server power outage, or a single write if something\r\n# wrong with the Redis process itself happens, but the operating system is\r\n# still running correctly.\r\n#\r\n# AOF and RDB persistence can be enabled at the same time without problems.\r\n# If the AOF is enabled on startup Redis will load the AOF, that is the file\r\n# with the better durability guarantees.\r\n#\r\n# Please check https://redis.io/topics/persistence for more information.\r\n\r\nappendonly no\r\n\r\n# The base name of the append only file.\r\n#\r\n# Redis 7 and newer use a set of append-only files to persist the dataset\r\n# and changes applied to it. There are two basic types of files in use:\r\n#\r\n# - Base files, which are a snapshot representing the complete state of the\r\n#   dataset at the time the file was created. Base files can be either in\r\n#   the form of RDB (binary serialized) or AOF (textual commands).\r\n# - Incremental files, which contain additional commands that were applied\r\n#   to the dataset following the previous file.\r\n#\r\n# In addition, manifest files are used to track the files and the order in\r\n# which they were created and should be applied.\r\n#\r\n# Append-only file names are created by Redis following a specific pattern.\r\n# The file name's prefix is based on the 'appendfilename' configuration\r\n# parameter, followed by additional information about the sequence and type.\r\n#\r\n# For example, if appendfilename is set to appendonly.aof, the following file\r\n# names could be derived:\r\n#\r\n# - appendonly.aof.1.base.rdb as a base file.\r\n# - appendonly.aof.1.incr.aof, appendonly.aof.2.incr.aof as incremental files.\r\n# - appendonly.aof.manifest as a manifest file.\r\n\r\nappendfilename \"appendonly.aof\"\r\n\r\n# For convenience, Redis stores all persistent append-only files in a dedicated\r\n# directory. The name of the directory is determined by the appenddirname\r\n# configuration parameter.\r\n\r\nappenddirname \"appendonlydir\"\r\n\r\n# The fsync() call tells the Operating System to actually write data on disk\r\n# instead of waiting for more data in the output buffer. Some OS will really flush\r\n# data on disk, some other OS will just try to do it ASAP.\r\n#\r\n# Redis supports three different modes:\r\n#\r\n# no: don't fsync, just let the OS flush the data when it wants. Faster.\r\n# always: fsync after every write to the append only log. Slow, Safest.\r\n# everysec: fsync only one time every second. Compromise.\r\n#\r\n# The default is \"everysec\", as that's usually the right compromise between\r\n# speed and data safety. It's up to you to understand if you can relax this to\r\n# \"no\" that will let the operating system flush the output buffer when\r\n# it wants, for better performances (but if you can live with the idea of\r\n# some data loss consider the default persistence mode that's snapshotting),\r\n# or on the contrary, use \"always\" that's very slow but a bit safer than\r\n# everysec.\r\n#\r\n# More details please check the following article:\r\n# http://antirez.com/post/redis-persistence-demystified.html\r\n#\r\n# If unsure, use \"everysec\".\r\n\r\n# appendfsync always\r\nappendfsync everysec\r\n# appendfsync no\r\n\r\n# When the AOF fsync policy is set to always or everysec, and a background\r\n# saving process (a background save or AOF log background rewriting) is\r\n# performing a lot of I/O against the disk, in some Linux configurations\r\n# Redis may block too long on the fsync() call. Note that there is no fix for\r\n# this currently, as even performing fsync in a different thread will block\r\n# our synchronous write(2) call.\r\n#\r\n# In order to mitigate this problem it's possible to use the following option\r\n# that will prevent fsync() from being called in the main process while a\r\n# BGSAVE or BGREWRITEAOF is in progress.\r\n#\r\n# This means that while another child is saving, the durability of Redis is\r\n# the same as \"appendfsync none\". In practical terms, this means that it is\r\n# possible to lose up to 30 seconds of log in the worst scenario (with the\r\n# default Linux settings).\r\n#\r\n# If you have latency problems turn this to \"yes\". Otherwise leave it as\r\n# \"no\" that is the safest pick from the point of view of durability.\r\n\r\nno-appendfsync-on-rewrite no\r\n\r\n# Automatic rewrite of the append only file.\r\n# Redis is able to automatically rewrite the log file implicitly calling\r\n# BGREWRITEAOF when the AOF log size grows by the specified percentage.\r\n#\r\n# This is how it works: Redis remembers the size of the AOF file after the\r\n# latest rewrite (if no rewrite has happened since the restart, the size of\r\n# the AOF at startup is used).\r\n#\r\n# This base size is compared to the current size. If the current size is\r\n# bigger than the specified percentage, the rewrite is triggered. Also\r\n# you need to specify a minimal size for the AOF file to be rewritten, this\r\n# is useful to avoid rewriting the AOF file even if the percentage increase\r\n# is reached but it is still pretty small.\r\n#\r\n# Specify a percentage of zero in order to disable the automatic AOF\r\n# rewrite feature.\r\n\r\nauto-aof-rewrite-percentage 100\r\nauto-aof-rewrite-min-size 64mb\r\n\r\n# An AOF file may be found to be truncated at the end during the Redis\r\n# startup process, when the AOF data gets loaded back into memory.\r\n# This may happen when the system where Redis is running\r\n# crashes, especially when an ext4 filesystem is mounted without the\r\n# data=ordered option (however this can't happen when Redis itself\r\n# crashes or aborts but the operating system still works correctly).\r\n#\r\n# Redis can either exit with an error when this happens, or load as much\r\n# data as possible (the default now) and start if the AOF file is found\r\n# to be truncated at the end. The following option controls this behavior.\r\n#\r\n# If aof-load-truncated is set to yes, a truncated AOF file is loaded and\r\n# the Redis server starts emitting a log to inform the user of the event.\r\n# Otherwise if the option is set to no, the server aborts with an error\r\n# and refuses to start. When the option is set to no, the user requires\r\n# to fix the AOF file using the \"redis-check-aof\" utility before to restart\r\n# the server.\r\n#\r\n# Note that if the AOF file will be found to be corrupted in the middle\r\n# the server will still exit with an error. This option only applies when\r\n# Redis will try to read more data from the AOF file but not enough bytes\r\n# will be found.\r\naof-load-truncated yes\r\n\r\n# Redis can create append-only base files in either RDB or AOF formats. Using\r\n# the RDB format is always faster and more efficient, and disabling it is only\r\n# supported for backward compatibility purposes.\r\naof-use-rdb-preamble yes\r\n\r\n# Redis supports recording timestamp annotations in the AOF to support restoring\r\n# the data from a specific point-in-time. However, using this capability changes\r\n# the AOF format in a way that may not be compatible with existing AOF parsers.\r\naof-timestamp-enabled no\r\n\r\n################################ SHUTDOWN #####################################\r\n\r\n# Maximum time to wait for replicas when shutting down, in seconds.\r\n#\r\n# During shut down, a grace period allows any lagging replicas to catch up with\r\n# the latest replication offset before the master exists. This period can\r\n# prevent data loss, especially for deployments without configured disk backups.\r\n#\r\n# The 'shutdown-timeout' value is the grace period's duration in seconds. It is\r\n# only applicable when the instance has replicas. To disable the feature, set\r\n# the value to 0.\r\n#\r\n# shutdown-timeout 10\r\n\r\n# When Redis receives a SIGINT or SIGTERM, shutdown is initiated and by default\r\n# an RDB snapshot is written to disk in a blocking operation if save points are configured.\r\n# The options used on signaled shutdown can include the following values:\r\n# default:  Saves RDB snapshot only if save points are configured.\r\n#           Waits for lagging replicas to catch up.\r\n# save:     Forces a DB saving operation even if no save points are configured.\r\n# nosave:   Prevents DB saving operation even if one or more save points are configured.\r\n# now:      Skips waiting for lagging replicas.\r\n# force:    Ignores any errors that would normally prevent the server from exiting.\r\n#\r\n# Any combination of values is allowed as long as \"save\" and \"nosave\" are not set simultaneously.\r\n# Example: \"nosave force now\"\r\n#\r\n# shutdown-on-sigint default\r\n# shutdown-on-sigterm default\r\n\r\n################ NON-DETERMINISTIC LONG BLOCKING COMMANDS #####################\r\n\r\n# Maximum time in milliseconds for EVAL scripts, functions and in some cases\r\n# modules' commands before Redis can start processing or rejecting other clients.\r\n#\r\n# If the maximum execution time is reached Redis will start to reply to most\r\n# commands with a BUSY error.\r\n#\r\n# In this state Redis will only allow a handful of commands to be executed.\r\n# For instance, SCRIPT KILL, FUNCTION KILL, SHUTDOWN NOSAVE and possibly some\r\n# module specific 'allow-busy' commands.\r\n#\r\n# SCRIPT KILL and FUNCTION KILL will only be able to stop a script that did not\r\n# yet call any write commands, so SHUTDOWN NOSAVE may be the only way to stop\r\n# the server in the case a write command was already issued by the script when\r\n# the user doesn't want to wait for the natural termination of the script.\r\n#\r\n# The default is 5 seconds. It is possible to set it to 0 or a negative value\r\n# to disable this mechanism (uninterrupted execution). Note that in the past\r\n# this config had a different name, which is now an alias, so both of these do\r\n# the same:\r\n# lua-time-limit 5000\r\n# busy-reply-threshold 5000\r\n\r\n################################ REDIS CLUSTER  ###############################\r\n\r\n# Normal Redis instances can't be part of a Redis Cluster; only nodes that are\r\n# started as cluster nodes can. In order to start a Redis instance as a\r\n# cluster node enable the cluster support uncommenting the following:\r\n#\r\n# cluster-enabled yes\r\n\r\n# Every cluster node has a cluster configuration file. This file is not\r\n# intended to be edited by hand. It is created and updated by Redis nodes.\r\n# Every Redis Cluster node requires a different cluster configuration file.\r\n# Make sure that instances running in the same system do not have\r\n# overlapping cluster configuration file names.\r\n#\r\n# cluster-config-file nodes-6379.conf\r\n\r\n# Cluster node timeout is the amount of milliseconds a node must be unreachable\r\n# for it to be considered in failure state.\r\n# Most other internal time limits are a multiple of the node timeout.\r\n#\r\n# cluster-node-timeout 15000\r\n\r\n# The cluster port is the port that the cluster bus will listen for inbound connections on. When set \r\n# to the default value, 0, it will be bound to the command port + 10000. Setting this value requires \r\n# you to specify the cluster bus port when executing cluster meet.\r\n# cluster-port 0\r\n\r\n# A replica of a failing master will avoid to start a failover if its data\r\n# looks too old.\r\n#\r\n# There is no simple way for a replica to actually have an exact measure of\r\n# its \"data age\", so the following two checks are performed:\r\n#\r\n# 1) If there are multiple replicas able to failover, they exchange messages\r\n#    in order to try to give an advantage to the replica with the best\r\n#    replication offset (more data from the master processed).\r\n#    Replicas will try to get their rank by offset, and apply to the start\r\n#    of the failover a delay proportional to their rank.\r\n#\r\n# 2) Every single replica computes the time of the last interaction with\r\n#    its master. This can be the last ping or command received (if the master\r\n#    is still in the \"connected\" state), or the time that elapsed since the\r\n#    disconnection with the master (if the replication link is currently down).\r\n#    If the last interaction is too old, the replica will not try to failover\r\n#    at all.\r\n#\r\n# The point \"2\" can be tuned by user. Specifically a replica will not perform\r\n# the failover if, since the last interaction with the master, the time\r\n# elapsed is greater than:\r\n#\r\n#   (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period\r\n#\r\n# So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor\r\n# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the\r\n# replica will not try to failover if it was not able to talk with the master\r\n# for longer than 310 seconds.\r\n#\r\n# A large cluster-replica-validity-factor may allow replicas with too old data to failover\r\n# a master, while a too small value may prevent the cluster from being able to\r\n# elect a replica at all.\r\n#\r\n# For maximum availability, it is possible to set the cluster-replica-validity-factor\r\n# to a value of 0, which means, that replicas will always try to failover the\r\n# master regardless of the last time they interacted with the master.\r\n# (However they'll always try to apply a delay proportional to their\r\n# offset rank).\r\n#\r\n# Zero is the only value able to guarantee that when all the partitions heal\r\n# the cluster will always be able to continue.\r\n#\r\n# cluster-replica-validity-factor 10\r\n\r\n# Cluster replicas are able to migrate to orphaned masters, that are masters\r\n# that are left without working replicas. This improves the cluster ability\r\n# to resist to failures as otherwise an orphaned master can't be failed over\r\n# in case of failure if it has no working replicas.\r\n#\r\n# Replicas migrate to orphaned masters only if there are still at least a\r\n# given number of other working replicas for their old master. This number\r\n# is the \"migration barrier\". A migration barrier of 1 means that a replica\r\n# will migrate only if there is at least 1 other working replica for its master\r\n# and so forth. It usually reflects the number of replicas you want for every\r\n# master in your cluster.\r\n#\r\n# Default is 1 (replicas migrate only if their masters remain with at least\r\n# one replica). To disable migration just set it to a very large value or\r\n# set cluster-allow-replica-migration to 'no'.\r\n# A value of 0 can be set but is useful only for debugging and dangerous\r\n# in production.\r\n#\r\n# cluster-migration-barrier 1\r\n\r\n# Turning off this option allows to use less automatic cluster configuration.\r\n# It both disables migration to orphaned masters and migration from masters\r\n# that became empty.\r\n#\r\n# Default is 'yes' (allow automatic migrations).\r\n#\r\n# cluster-allow-replica-migration yes\r\n\r\n# By default Redis Cluster nodes stop accepting queries if they detect there\r\n# is at least a hash slot uncovered (no available node is serving it).\r\n# This way if the cluster is partially down (for example a range of hash slots\r\n# are no longer covered) all the cluster becomes, eventually, unavailable.\r\n# It automatically returns available as soon as all the slots are covered again.\r\n#\r\n# However sometimes you want the subset of the cluster which is working,\r\n# to continue to accept queries for the part of the key space that is still\r\n# covered. In order to do so, just set the cluster-require-full-coverage\r\n# option to no.\r\n#\r\n# cluster-require-full-coverage yes\r\n\r\n# This option, when set to yes, prevents replicas from trying to failover its\r\n# master during master failures. However the replica can still perform a\r\n# manual failover, if forced to do so.\r\n#\r\n# This is useful in different scenarios, especially in the case of multiple\r\n# data center operations, where we want one side to never be promoted if not\r\n# in the case of a total DC failure.\r\n#\r\n# cluster-replica-no-failover no\r\n\r\n# This option, when set to yes, allows nodes to serve read traffic while the\r\n# cluster is in a down state, as long as it believes it owns the slots.\r\n#\r\n# This is useful for two cases.  The first case is for when an application\r\n# doesn't require consistency of data during node failures or network partitions.\r\n# One example of this is a cache, where as long as the node has the data it\r\n# should be able to serve it.\r\n#\r\n# The second use case is for configurations that don't meet the recommended\r\n# three shards but want to enable cluster mode and scale later. A\r\n# master outage in a 1 or 2 shard configuration causes a read/write outage to the\r\n# entire cluster without this option set, with it set there is only a write outage.\r\n# Without a quorum of masters, slot ownership will not change automatically.\r\n#\r\n# cluster-allow-reads-when-down no\r\n\r\n# This option, when set to yes, allows nodes to serve pubsub shard traffic while\r\n# the cluster is in a down state, as long as it believes it owns the slots.\r\n#\r\n# This is useful if the application would like to use the pubsub feature even when\r\n# the cluster global stable state is not OK. If the application wants to make sure only\r\n# one shard is serving a given channel, this feature should be kept as yes.\r\n#\r\n# cluster-allow-pubsubshard-when-down yes\r\n\r\n# Cluster link send buffer limit is the limit on the memory usage of an individual\r\n# cluster bus link's send buffer in bytes. Cluster links would be freed if they exceed\r\n# this limit. This is to primarily prevent send buffers from growing unbounded on links\r\n# toward slow peers (E.g. PubSub messages being piled up).\r\n# This limit is disabled by default. Enable this limit when 'mem_cluster_links' INFO field\r\n# and/or 'send-buffer-allocated' entries in the 'CLUSTER LINKS` command output continuously increase.\r\n# Minimum limit of 1gb is recommended so that cluster link buffer can fit in at least a single\r\n# PubSub message by default. (client-query-buffer-limit default value is 1gb)\r\n#\r\n# cluster-link-sendbuf-limit 0\r\n \r\n# Clusters can configure their announced hostname using this config. This is a common use case for \r\n# applications that need to use TLS Server Name Indication (SNI) or dealing with DNS based\r\n# routing. By default this value is only shown as additional metadata in the CLUSTER SLOTS\r\n# command, but can be changed using 'cluster-preferred-endpoint-type' config. This value is \r\n# communicated along the clusterbus to all nodes, setting it to an empty string will remove \r\n# the hostname and also propagate the removal.\r\n#\r\n# cluster-announce-hostname \"\"\r\n\r\n# Clusters can advertise how clients should connect to them using either their IP address,\r\n# a user defined hostname, or by declaring they have no endpoint. Which endpoint is\r\n# shown as the preferred endpoint is set by using the cluster-preferred-endpoint-type\r\n# config with values 'ip', 'hostname', or 'unknown-endpoint'. This value controls how\r\n# the endpoint returned for MOVED/ASKING requests as well as the first field of CLUSTER SLOTS. \r\n# If the preferred endpoint type is set to hostname, but no announced hostname is set, a '?' \r\n# will be returned instead.\r\n#\r\n# When a cluster advertises itself as having an unknown endpoint, it's indicating that\r\n# the server doesn't know how clients can reach the cluster. This can happen in certain \r\n# networking situations where there are multiple possible routes to the node, and the \r\n# server doesn't know which one the client took. In this case, the server is expecting\r\n# the client to reach out on the same endpoint it used for making the last request, but use\r\n# the port provided in the response.\r\n#\r\n# cluster-preferred-endpoint-type ip\r\n\r\n# In order to setup your cluster make sure to read the documentation\r\n# available at https://redis.io web site.\r\n\r\n########################## CLUSTER DOCKER/NAT support  ########################\r\n\r\n# In certain deployments, Redis Cluster nodes address discovery fails, because\r\n# addresses are NAT-ted or because ports are forwarded (the typical case is\r\n# Docker and other containers).\r\n#\r\n# In order to make Redis Cluster working in such environments, a static\r\n# configuration where each node knows its public address is needed. The\r\n# following four options are used for this scope, and are:\r\n#\r\n# * cluster-announce-ip\r\n# * cluster-announce-port\r\n# * cluster-announce-tls-port\r\n# * cluster-announce-bus-port\r\n#\r\n# Each instructs the node about its address, client ports (for connections\r\n# without and with TLS) and cluster message bus port. The information is then\r\n# published in the header of the bus packets so that other nodes will be able to\r\n# correctly map the address of the node publishing the information.\r\n#\r\n# If cluster-tls is set to yes and cluster-announce-tls-port is omitted or set\r\n# to zero, then cluster-announce-port refers to the TLS port. Note also that\r\n# cluster-announce-tls-port has no effect if cluster-tls is set to no.\r\n#\r\n# If the above options are not used, the normal Redis Cluster auto-detection\r\n# will be used instead.\r\n#\r\n# Note that when remapped, the bus port may not be at the fixed offset of\r\n# clients port + 10000, so you can specify any port and bus-port depending\r\n# on how they get remapped. If the bus-port is not set, a fixed offset of\r\n# 10000 will be used as usual.\r\n#\r\n# Example:\r\n#\r\n# cluster-announce-ip 10.1.1.5\r\n# cluster-announce-tls-port 6379\r\n# cluster-announce-port 0\r\n# cluster-announce-bus-port 6380\r\n\r\n################################## SLOW LOG ###################################\r\n\r\n# The Redis Slow Log is a system to log queries that exceeded a specified\r\n# execution time. The execution time does not include the I/O operations\r\n# like talking with the client, sending the reply and so forth,\r\n# but just the time needed to actually execute the command (this is the only\r\n# stage of command execution where the thread is blocked and can not serve\r\n# other requests in the meantime).\r\n#\r\n# You can configure the slow log with two parameters: one tells Redis\r\n# what is the execution time, in microseconds, to exceed in order for the\r\n# command to get logged, and the other parameter is the length of the\r\n# slow log. When a new command is logged the oldest one is removed from the\r\n# queue of logged commands.\r\n\r\n# The following time is expressed in microseconds, so 1000000 is equivalent\r\n# to one second. Note that a negative number disables the slow log, while\r\n# a value of zero forces the logging of every command.\r\nslowlog-log-slower-than 10000\r\n\r\n# There is no limit to this length. Just be aware that it will consume memory.\r\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\r\nslowlog-max-len 128\r\n\r\n################################ LATENCY MONITOR ##############################\r\n\r\n# The Redis latency monitoring subsystem samples different operations\r\n# at runtime in order to collect data related to possible sources of\r\n# latency of a Redis instance.\r\n#\r\n# Via the LATENCY command this information is available to the user that can\r\n# print graphs and obtain reports.\r\n#\r\n# The system only logs operations that were performed in a time equal or\r\n# greater than the amount of milliseconds specified via the\r\n# latency-monitor-threshold configuration directive. When its value is set\r\n# to zero, the latency monitor is turned off.\r\n#\r\n# By default latency monitoring is disabled since it is mostly not needed\r\n# if you don't have latency issues, and collecting data has a performance\r\n# impact, that while very small, can be measured under big load. Latency\r\n# monitoring can easily be enabled at runtime using the command\r\n# \"CONFIG SET latency-monitor-threshold \u003cmilliseconds\u003e\" if needed.\r\nlatency-monitor-threshold 0\r\n\r\n################################ LATENCY TRACKING ##############################\r\n\r\n# The Redis extended latency monitoring tracks the per command latencies and enables\r\n# exporting the percentile distribution via the INFO latencystats command,\r\n# and cumulative latency distributions (histograms) via the LATENCY command.\r\n#\r\n# By default, the extended latency monitoring is enabled since the overhead\r\n# of keeping track of the command latency is very small.\r\n# latency-tracking yes\r\n\r\n# By default the exported latency percentiles via the INFO latencystats command\r\n# are the p50, p99, and p999.\r\n# latency-tracking-info-percentiles 50 99 99.9\r\n\r\n############################# EVENT NOTIFICATION ##############################\r\n\r\n# Redis can notify Pub/Sub clients about events happening in the key space.\r\n# This feature is documented at https://redis.io/topics/notifications\r\n#\r\n# For instance if keyspace events notification is enabled, and a client\r\n# performs a DEL operation on key \"foo\" stored in the Database 0, two\r\n# messages will be published via Pub/Sub:\r\n#\r\n# PUBLISH __keyspace@0__:foo del\r\n# PUBLISH __keyevent@0__:del foo\r\n#\r\n# It is possible to select the events that Redis will notify among a set\r\n# of classes. Every class is identified by a single character:\r\n#\r\n#  K     Keyspace events, published with __keyspace@\u003cdb\u003e__ prefix.\r\n#  E     Keyevent events, published with __keyevent@\u003cdb\u003e__ prefix.\r\n#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...\r\n#  $     String commands\r\n#  l     List commands\r\n#  s     Set commands\r\n#  h     Hash commands\r\n#  z     Sorted set commands\r\n#  x     Expired events (events generated every time a key expires)\r\n#  e     Evicted events (events generated when a key is evicted for maxmemory)\r\n#  n     New key events (Note: not included in the 'A' class)\r\n#  t     Stream commands\r\n#  d     Module key type events\r\n#  m     Key-miss events (Note: It is not included in the 'A' class)\r\n#  A     Alias for g$lshzxetd, so that the \"AKE\" string means all the events\r\n#        (Except key-miss events which are excluded from 'A' due to their\r\n#         unique nature).\r\n#\r\n#  The \"notify-keyspace-events\" takes as argument a string that is composed\r\n#  of zero or multiple characters. The empty string means that notifications\r\n#  are disabled.\r\n#\r\n#  Example: to enable list and generic events, from the point of view of the\r\n#           event name, use:\r\n#\r\n#  notify-keyspace-events Elg\r\n#\r\n#  Example 2: to get the stream of the expired keys subscribing to channel\r\n#             name __keyevent@0__:expired use:\r\n#\r\n#  notify-keyspace-events Ex\r\n#\r\n#  By default all notifications are disabled because most users don't need\r\n#  this feature and the feature has some overhead. Note that if you don't\r\n#  specify at least one of K or E, no events will be delivered.\r\nnotify-keyspace-events \"\"\r\n\r\n############################### ADVANCED CONFIG ###############################\r\n\r\n# Hashes are encoded using a memory efficient data structure when they have a\r\n# small number of entries, and the biggest entry does not exceed a given\r\n# threshold. These thresholds can be configured using the following directives.\r\nhash-max-listpack-entries 512\r\nhash-max-listpack-value 64\r\n\r\n# Lists are also encoded in a special way to save a lot of space.\r\n# The number of entries allowed per internal list node can be specified\r\n# as a fixed maximum size or a maximum number of elements.\r\n# For a fixed maximum size, use -5 through -1, meaning:\r\n# -5: max size: 64 Kb  \u003c-- not recommended for normal workloads\r\n# -4: max size: 32 Kb  \u003c-- not recommended\r\n# -3: max size: 16 Kb  \u003c-- probably not recommended\r\n# -2: max size: 8 Kb   \u003c-- good\r\n# -1: max size: 4 Kb   \u003c-- good\r\n# Positive numbers mean store up to _exactly_ that number of elements\r\n# per list node.\r\n# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),\r\n# but if your use case is unique, adjust the settings as necessary.\r\nlist-max-listpack-size -2\r\n\r\n# Lists may also be compressed.\r\n# Compress depth is the number of quicklist ziplist nodes from *each* side of\r\n# the list to *exclude* from compression.  The head and tail of the list\r\n# are always uncompressed for fast push/pop operations.  Settings are:\r\n# 0: disable all list compression\r\n# 1: depth 1 means \"don't start compressing until after 1 node into the list,\r\n#    going from either the head or tail\"\r\n#    So: [head]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[tail]\r\n#    [head], [tail] will always be uncompressed; inner nodes will compress.\r\n# 2: [head]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[tail]\r\n#    2 here means: don't compress head or head-\u003enext or tail-\u003eprev or tail,\r\n#    but compress all nodes between them.\r\n# 3: [head]-\u003e[next]-\u003e[next]-\u003enode-\u003enode-\u003e...-\u003enode-\u003e[prev]-\u003e[prev]-\u003e[tail]\r\n# etc.\r\nlist-compress-depth 0\r\n\r\n# Sets have a special encoding in just one case: when a set is composed\r\n# of just strings that happen to be integers in radix 10 in the range\r\n# of 64 bit signed integers.\r\n# The following configuration setting sets the limit in the size of the\r\n# set in order to use this special memory saving encoding.\r\nset-max-intset-entries 512\r\n\r\n# Similarly to hashes and lists, sorted sets are also specially encoded in\r\n# order to save a lot of space. This encoding is only used when the length and\r\n# elements of a sorted set are below the following limits:\r\nzset-max-listpack-entries 128\r\nzset-max-listpack-value 64\r\n\r\n# HyperLogLog sparse representation bytes limit. The limit includes the\r\n# 16 bytes header. When an HyperLogLog using the sparse representation crosses\r\n# this limit, it is converted into the dense representation.\r\n#\r\n# A value greater than 16000 is totally useless, since at that point the\r\n# dense representation is more memory efficient.\r\n#\r\n# The suggested value is ~ 3000 in order to have the benefits of\r\n# the space efficient encoding without slowing down too much PFADD,\r\n# which is O(N) with the sparse encoding. The value can be raised to\r\n# ~ 10000 when CPU is not a concern, but space is, and the data set is\r\n# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.\r\nhll-sparse-max-bytes 3000\r\n\r\n# Streams macro node max size / items. The stream data structure is a radix\r\n# tree of big nodes that encode multiple items inside. Using this configuration\r\n# it is possible to configure how big a single node can be in bytes, and the\r\n# maximum number of items it may contain before switching to a new node when\r\n# appending new stream entries. If any of the following settings are set to\r\n# zero, the limit is ignored, so for instance it is possible to set just a\r\n# max entries limit by setting max-bytes to 0 and max-entries to the desired\r\n# value.\r\nstream-node-max-bytes 4096\r\nstream-node-max-entries 100\r\n\r\n# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in\r\n# order to help rehashing the main Redis hash table (the one mapping top-level\r\n# keys to values). The hash table implementation Redis uses (see dict.c)\r\n# performs a lazy rehashing: the more operation you run into a hash table\r\n# that is rehashing, the more rehashing \"steps\" are performed, so if the\r\n# server is idle the rehashing is never complete and some more memory is used\r\n# by the hash table.\r\n#\r\n# The default is to use this millisecond 10 times every second in order to\r\n# actively rehash the main dictionaries, freeing memory when possible.\r\n#\r\n# If unsure:\r\n# use \"activerehashing no\" if you have hard latency requirements and it is\r\n# not a good thing in your environment that Redis can reply from time to time\r\n# to queries with 2 milliseconds delay.\r\n#\r\n# use \"activerehashing yes\" if you don't have such hard requirements but\r\n# want to free memory asap when possible.\r\nactiverehashing yes\r\n\r\n# The client output buffer limits can be used to force disconnection of clients\r\n# that are not reading data from the server fast enough for some reason (a\r\n# common reason is that a Pub/Sub client can't consume messages as fast as the\r\n# publisher can produce them).\r\n#\r\n# The limit can be set differently for the three different classes of clients:\r\n#\r\n# normal -\u003e normal clients including MONITOR clients\r\n# replica -\u003e replica clients\r\n# pubsub -\u003e clients subscribed to at least one pubsub channel or pattern\r\n#\r\n# The syntax of every client-output-buffer-limit directive is the following:\r\n#\r\n# client-output-buffer-limit \u003cclass\u003e \u003chard limit\u003e \u003csoft limit\u003e \u003csoft seconds\u003e\r\n#\r\n# A client is immediately disconnected once the hard limit is reached, or if\r\n# the soft limit is reached and remains reached for the specified number of\r\n# seconds (continuously).\r\n# So for instance if the hard limit is 32 megabytes and the soft limit is\r\n# 16 megabytes / 10 seconds, the client will get disconnected immediately\r\n# if the size of the output buffers reach 32 megabytes, but will also get\r\n# disconnected if the client reaches 16 megabytes and continuously overcomes\r\n# the limit for 10 seconds.\r\n#\r\n# By default normal clients are not limited because they don't receive data\r\n# without asking (in a push way), but just after a request, so only\r\n# asynchronous clients may create a scenario where data is requested faster\r\n# than it can read.\r\n#\r\n# Instead there is a default limit for pubsub and replica clients, since\r\n# subscribers and replicas receive data in a push fashion.\r\n#\r\n# Note that it doesn't make sense to set the replica clients output buffer\r\n# limit lower than the repl-backlog-size config (partial sync will succeed\r\n# and then replica will get disconnected).\r\n# Such a configuration is ignored (the size of repl-backlog-size will be used).\r\n# This doesn't have memory consumption implications since the replica client\r\n# will share the backlog buffers memory.\r\n#\r\n# Both the hard or the soft limit can be disabled by setting them to zero.\r\nclient-output-buffer-limit normal 0 0 0\r\nclient-output-buffer-limit replica 256mb 64mb 60\r\nclient-output-buffer-limit pubsub 32mb 8mb 60\r\n\r\n# Client query buffers accumulate new commands. They are limited to a fixed\r\n# amount by default in order to avoid that a protocol desynchronization (for\r\n# instance due to a bug in the client) will lead to unbound memory usage in\r\n# the query buffer. However you can configure it here if you have very special\r\n# needs, such us huge multi/exec requests or alike.\r\n#\r\n# client-query-buffer-limit 1gb\r\n\r\n# In some scenarios client connections can hog up memory leading to OOM\r\n# errors or data eviction. To avoid this we can cap the accumulated memory\r\n# used by all client connections (all pubsub and normal clients). Once we\r\n# reach that limit connections will be dropped by the server freeing up\r\n# memory. The server will attempt to drop the connections using the most \r\n# memory first. We call this mechanism \"client eviction\".\r\n#\r\n# Client eviction is configured using the maxmemory-clients setting as follows:\r\n# 0 - client eviction is disabled (default)\r\n#\r\n# A memory value can be used for the client eviction threshold,\r\n# for example:\r\n# maxmemory-clients 1g\r\n#\r\n# A percentage value (between 1% and 100%) means the client eviction threshold\r\n# is based on a percentage of the maxmemory setting. For example to set client\r\n# eviction at 5% of maxmemory:\r\n# maxmemory-clients 5%\r\n\r\n# In the Redis protocol, bulk requests, that are, elements representing single\r\n# strings, are normally limited to 512 mb. However you can change this limit\r\n# here, but must be 1mb or greater\r\n#\r\n# proto-max-bulk-len 512mb\r\n\r\n# Redis calls an internal function to perform many background tasks, like\r\n# closing connections of clients in timeout, purging expired keys that are\r\n# never requested, and so forth.\r\n#\r\n# Not all tasks are performed with the same frequency, but Redis checks for\r\n# tasks to perform according to the specified \"hz\" value.\r\n#\r\n# By default \"hz\" is set to 10. Raising the value will use more CPU when\r\n# Redis is idle, but at the same time will make Redis more responsive when\r\n# there are many keys expiring at the same time, and timeouts may be\r\n# handled with more precision.\r\n#\r\n# The range is between 1 and 500, however a value over 100 is usually not\r\n# a good idea. Most users should use the default of 10 and raise this up to\r\n# 100 only in environments where very low latency is required.\r\nhz 10\r\n\r\n# Normally it is useful to have an HZ value which is proportional to the\r\n# number of clients connected. This is useful in order, for instance, to\r\n# avoid too many clients are processed for each background task invocation\r\n# in order to avoid latency spikes.\r\n#\r\n# Since the default HZ value by default is conservatively set to 10, Redis\r\n# offers, and enables by default, the ability to use an adaptive HZ value\r\n# which will temporarily raise when there are many connected clients.\r\n#\r\n# When dynamic HZ is enabled, the actual configured HZ will be used\r\n# as a baseline, but multiples of the configured HZ value will be actually\r\n# used as needed once more clients are connected. In this way an idle\r\n# instance will use very little CPU time while a busy instance will be\r\n# more responsive.\r\ndynamic-hz yes\r\n\r\n# When a child rewrites the AOF file, if the following option is enabled\r\n# the file will be fsync-ed every 4 MB of data generated. This is useful\r\n# in order to commit the file to the disk more incrementally and avoid\r\n# big latency spikes.\r\naof-rewrite-incremental-fsync yes\r\n\r\n# When redis saves RDB file, if the following option is enabled\r\n# the file will be fsync-ed every 4 MB of data generated. This is useful\r\n# in order to commit the file to the disk more incrementally and avoid\r\n# big latency spikes.\r\nrdb-save-incremental-fsync yes\r\n\r\n# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good\r\n# idea to start with the default settings and only change them after investigating\r\n# how to improve the performances and how the keys LFU change over time, which\r\n# is possible to inspect via the OBJECT FREQ command.\r\n#\r\n# There are two tunable parameters in the Redis LFU implementation: the\r\n# counter logarithm factor and the counter decay time. It is important to\r\n# understand what the two parameters mean before changing them.\r\n#\r\n# The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis\r\n# uses a probabilistic increment with logarithmic behavior. Given the value\r\n# of the old counter, when a key is accessed, the counter is incremented in\r\n# this way:\r\n#\r\n# 1. A random number R between 0 and 1 is extracted.\r\n# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).\r\n# 3. The counter is incremented only if R \u003c P.\r\n#\r\n# The default lfu-log-factor is 10. This is a table of how the frequency\r\n# counter changes with a different number of accesses with different\r\n# logarithmic factors:\r\n#\r\n# +--------+------------+------------+------------+------------+------------+\r\n# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |\r\n# +--------+------------+------------+------------+------------+------------+\r\n# | 0      | 104        | 255        | 255        | 255        | 255        |\r\n# +--------+------------+------------+------------+------------+------------+\r\n# | 1      | 18         | 49         | 255        | 255        | 255        |\r\n# +--------+------------+------------+------------+------------+------------+\r\n# | 10     | 10         | 18         | 142        | 255        | 255        |\r\n# +--------+------------+------------+------------+------------+------------+\r\n# | 100    | 8          | 11         | 49         | 143        | 255        |\r\n# +--------+------------+------------+------------+------------+------------+\r\n#\r\n# NOTE: The above table was obtained by running the following commands:\r\n#\r\n#   redis-benchmark -n 1000000 incr foo\r\n#   redis-cli object freq foo\r\n#\r\n# NOTE 2: The counter initial value is 5 in order to give new objects a chance\r\n# to accumulate hits.\r\n#\r\n# The counter decay time is the time, in minutes, that must elapse in order\r\n# for the key counter to be divided by two (or decremented if it has a value\r\n# less \u003c= 10).\r\n#\r\n# The default value for the lfu-decay-time is 1. A special value of 0 means to\r\n# decay the counter every time it happens to be scanned.\r\n#\r\n# lfu-log-factor 10\r\n# lfu-decay-time 1\r\n\r\n########################### ACTIVE DEFRAGMENTATION #######################\r\n#\r\n# What is active defragmentation?\r\n# -------------------------------\r\n#\r\n# Active (online) defragmentation allows a Redis server to compact the\r\n# spaces left between small allocations and deallocations of data in memory,\r\n# thus allowing to reclaim back memory.\r\n#\r\n# Fragmentation is a natural process that happens with every allocator (but\r\n# less so with Jemalloc, fortunately) and certain workloads. Normally a server\r\n# restart is needed in order to lower the fragmentation, or at least to flush\r\n# away all the data and create it again. However thanks to this feature\r\n# implemented by Oran Agra for Redis 4.0 this process can happen at runtime\r\n# in a \"hot\" way, while the server is running.\r\n#\r\n# Basically when the fragmentation is over a certain level (see the\r\n# configuration options below) Redis will start to create new copies of the\r\n# values in contiguous memory regions by exploiting certain specific Jemalloc\r\n# features (in order to understand if an allocation is causing fragmentation\r\n# and to allocate it in a better place), and at the same time, will release the\r\n# old copies of the data. This process, repeated incrementally for all the keys\r\n# will cause the fragmentation to drop back to normal values.\r\n#\r\n# Important things to understand:\r\n#\r\n# 1. This feature is disabled by default, and only works if you compiled Redis\r\n#    to use the copy of Jemalloc we ship with the source code of Redis.\r\n#    This is the default with Linux builds.\r\n#\r\n# 2. You never need to enable this feature if you don't have fragmentation\r\n#    issues.\r\n#\r\n# 3. Once you experience fragmentation, you can enable this feature when\r\n#    needed with the command \"CONFIG SET activedefrag yes\".\r\n#\r\n# The configuration parameters are able to fine tune the behavior of the\r\n# defragmentation process. If you are not sure about what they mean it is\r\n# a good idea to leave the defaults untouched.\r\n\r\n# Active defragmentation is disabled by default\r\n# activedefrag no\r\n\r\n# Minimum amount of fragmentation waste to start active defrag\r\n# active-defrag-ignore-bytes 100mb\r\n\r\n# Minimum percentage of fragmentation to start active defrag\r\n# active-defrag-threshold-lower 10\r\n\r\n# Maximum percentage of fragmentation at which we use maximum effort\r\n# active-defrag-threshold-upper 100\r\n\r\n# Minimal effort for defrag in CPU percentage, to be used when the lower\r\n# threshold is reached\r\n# active-defrag-cycle-min 1\r\n\r\n# Maximal effort for defrag in CPU percentage, to be used when the upper\r\n# threshold is reached\r\n# active-defrag-cycle-max 25\r\n\r\n# Maximum number of set/hash/zset/list fields that will be processed from\r\n# the main dictionary scan\r\n# active-defrag-max-scan-fields 1000\r\n\r\n# Jemalloc background thread for purging will be enabled by default\r\njemalloc-bg-thread yes\r\n\r\n# It is possible to pin different threads and processes of Redis to specific\r\n# CPUs in your system, in order to maximize the performances of the server.\r\n# This is useful both in order to pin different Redis threads in different\r\n# CPUs, but also in order to make sure that multiple Redis instances running\r\n# in the same host will be pinned to different CPUs.\r\n#\r\n# Normally you can do this using the \"taskset\" command, however it is also\r\n# possible to this via Redis configuration directly, both in Linux and FreeBSD.\r\n#\r\n# You can pin the server/IO threads, bio threads, aof rewrite child process, and\r\n# the bgsave child process. The syntax to specify the cpu list is the same as\r\n# the taskset command:\r\n#\r\n# Set redis server/io threads to cpu affinity 0,2,4,6:\r\n# server_cpulist 0-7:2\r\n#\r\n# Set bio threads to cpu affinity 1,3:\r\n# bio_cpulist 1,3\r\n#\r\n# Set aof rewrite child process to cpu affinity 8,9,10,11:\r\n# aof_rewrite_cpulist 8-11\r\n#\r\n# Set bgsave child process to cpu affinity 1,10,11\r\n# bgsave_cpulist 1,10-11\r\n\r\n# In some cases redis will emit warnings and even refuse to start if it detects\r\n# that the system is in bad state, it is possible to suppress these warnings\r\n# by setting the following config which takes a space delimited list of warnings\r\n# to suppress\r\n#\r\n# ignore-warnings ARM64-COW-BUG",
              "slave.conf": "        slaveof redis-ss-0.redis-service.sre-challenge 6379\r\n        maxmemory 400mb\r\n        maxmemory-policy allkeys-lru\r\n        timeout 0\r\n        dir /data\r\n"
            },
            "id": "sre-challenge/redis-ss-configuration",
            "immutable": false,
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "app": "redis"
                },
                "name": "redis-ss-configuration",
                "namespace": "sre-challenge",
                "resource_version": "710",
                "uid": "1ed53bca-fdc5-4c1e-9fd7-7961d1a9a0fe"
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_deployment",
      "name": "node-api-deployment",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "sre-challenge/node-api",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 9,
                "labels": {
                  "App": "node-api"
                },
                "name": "node-api",
                "namespace": "sre-challenge",
                "resource_version": "5442",
                "uid": "8695b9c7-eee0-45de-b513-922ddd5d64dd"
              }
            ],
            "spec": [
              {
                "min_ready_seconds": 0,
                "paused": false,
                "progress_deadline_seconds": 600,
                "replicas": "2",
                "revision_history_limit": 10,
                "selector": [
                  {
                    "match_expressions": [],
                    "match_labels": {
                      "App": "node-api"
                    }
                  }
                ],
                "strategy": [
                  {
                    "rolling_update": [
                      {
                        "max_surge": "25%",
                        "max_unavailable": "25%"
                      }
                    ],
                    "type": "RollingUpdate"
                  }
                ],
                "template": [
                  {
                    "metadata": [
                      {
                        "annotations": {},
                        "generate_name": "",
                        "generation": 0,
                        "labels": {
                          "App": "node-api"
                        },
                        "name": "",
                        "namespace": "",
                        "resource_version": "",
                        "uid": ""
                      }
                    ],
                    "spec": [
                      {
                        "active_deadline_seconds": 0,
                        "affinity": [],
                        "automount_service_account_token": true,
                        "container": [
                          {
                            "args": [],
                            "command": [],
                            "env": [],
                            "env_from": [],
                            "image": "suhanad14/node-api:latest",
                            "image_pull_policy": "Always",
                            "lifecycle": [],
                            "liveness_probe": [
                              {
                                "exec": [],
                                "failure_threshold": 3,
                                "grpc": [],
                                "http_get": [
                                  {
                                    "host": "",
                                    "http_header": [],
                                    "path": "/",
                                    "port": "3000",
                                    "scheme": "HTTP"
                                  }
                                ],
                                "initial_delay_seconds": 120,
                                "period_seconds": 10,
                                "success_threshold": 1,
                                "tcp_socket": [],
                                "timeout_seconds": 10
                              }
                            ],
                            "name": "node-api",
                            "port": [
                              {
                                "container_port": 3000,
                                "host_ip": "",
                                "host_port": 0,
                                "name": "",
                                "protocol": "TCP"
                              }
                            ],
                            "readiness_probe": [
                              {
                                "exec": [],
                                "failure_threshold": 3,
                                "grpc": [],
                                "http_get": [
                                  {
                                    "host": "",
                                    "http_header": [],
                                    "path": "/",
                                    "port": "3000",
                                    "scheme": "HTTP"
                                  }
                                ],
                                "initial_delay_seconds": 30,
                                "period_seconds": 10,
                                "success_threshold": 1,
                                "tcp_socket": [],
                                "timeout_seconds": 10
                              }
                            ],
                            "resources": [
                              {
                                "limits": {
                                  "cpu": "500m",
                                  "memory": "512Mi"
                                },
                                "requests": {
                                  "cpu": "250m",
                                  "memory": "50Mi"
                                }
                              }
                            ],
                            "security_context": [],
                            "startup_probe": [],
                            "stdin": false,
                            "stdin_once": false,
                            "termination_message_path": "/dev/termination-log",
                            "termination_message_policy": "File",
                            "tty": false,
                            "volume_mount": [],
                            "working_dir": ""
                          }
                        ],
                        "dns_config": [],
                        "dns_policy": "ClusterFirst",
                        "enable_service_links": true,
                        "host_aliases": [],
                        "host_ipc": false,
                        "host_network": false,
                        "host_pid": false,
                        "hostname": "",
                        "image_pull_secrets": [],
                        "init_container": [],
                        "node_name": "",
                        "node_selector": {},
                        "os": [],
                        "priority_class_name": "",
                        "readiness_gate": [],
                        "restart_policy": "Always",
                        "runtime_class_name": "",
                        "scheduler_name": "default-scheduler",
                        "security_context": [],
                        "service_account_name": "",
                        "share_process_namespace": false,
                        "subdomain": "",
                        "termination_grace_period_seconds": 30,
                        "toleration": [],
                        "topology_spread_constraint": [],
                        "volume": []
                      }
                    ]
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_rollout": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwLCJ1cGRhdGUiOjYwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_horizontal_pod_autoscaler",
      "name": "pod_scaler",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "sre-challenge/node-hpi",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "node-hpi",
                "namespace": "sre-challenge",
                "resource_version": "5440",
                "uid": "5ea5dda6-3281-40c4-bceb-de067ca1b874"
              }
            ],
            "spec": [
              {
                "behavior": [],
                "max_replicas": 10,
                "metric": [],
                "min_replicas": 2,
                "scale_target_ref": [
                  {
                    "api_version": "apps/v1",
                    "kind": "Deployment",
                    "name": "node-api"
                  }
                ],
                "target_cpu_utilization_percentage": 50
              }
            ]
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "helm_release.prometheus",
            "kubernetes_namespace.kube-namespace"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "kube-namespace",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "prometheus",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "prometheus",
                "resource_version": "698",
                "uid": "d1505c4c-f29b-4ce2-96c6-32ee617f9f98"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "sre-challenge",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "sre-challenge",
            "metadata": [
              {
                "annotations": {
                  "name": "sre-challenge"
                },
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "mylabel": "sre-challenge"
                },
                "name": "sre-challenge",
                "resource_version": "699",
                "uid": "2fa135c6-dd10-4443-b4bc-2ad69cf73254"
              }
            ],
            "timeouts": null,
            "wait_for_default_service_account": false
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_service_v1",
      "name": "node-api-service",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "sre-challenge/node-api-service",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "App": "node-api"
                },
                "name": "node-api-service",
                "namespace": "sre-challenge",
                "resource_version": "721",
                "uid": "97b89956-abfd-4346-8bb4-caed46439aea"
              }
            ],
            "spec": [
              {
                "allocate_load_balancer_node_ports": true,
                "cluster_ip": "10.108.247.154",
                "cluster_ips": [
                  "10.108.247.154"
                ],
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "",
                "health_check_node_port": 0,
                "internal_traffic_policy": "Cluster",
                "ip_families": [
                  "IPv4"
                ],
                "ip_family_policy": "SingleStack",
                "load_balancer_class": "",
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "app_protocol": "",
                    "name": "",
                    "node_port": 0,
                    "port": 3000,
                    "protocol": "TCP",
                    "target_port": "3000"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "App": "node-api"
                },
                "session_affinity": "None",
                "session_affinity_config": [],
                "type": "ClusterIP"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": []
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_load_balancer": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_service_v1",
      "name": "redis-service",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "sre-challenge/redis-service",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "app": "redis"
                },
                "name": "redis-service",
                "namespace": "sre-challenge",
                "resource_version": "708",
                "uid": "02f02dd1-e215-4c6d-89a2-701b31167aff"
              }
            ],
            "spec": [
              {
                "allocate_load_balancer_node_ports": true,
                "cluster_ip": "None",
                "cluster_ips": [
                  "None"
                ],
                "external_ips": [],
                "external_name": "",
                "external_traffic_policy": "",
                "health_check_node_port": 0,
                "internal_traffic_policy": "Cluster",
                "ip_families": [
                  "IPv4"
                ],
                "ip_family_policy": "SingleStack",
                "load_balancer_class": "",
                "load_balancer_ip": "",
                "load_balancer_source_ranges": [],
                "port": [
                  {
                    "app_protocol": "",
                    "name": "",
                    "node_port": 0,
                    "port": 6379,
                    "protocol": "TCP",
                    "target_port": "6379"
                  }
                ],
                "publish_not_ready_addresses": false,
                "selector": {
                  "app": "redis"
                },
                "session_affinity": "None",
                "session_affinity_config": [],
                "type": "ClusterIP"
              }
            ],
            "status": [
              {
                "load_balancer": [
                  {
                    "ingress": []
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_load_balancer": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_stateful_set_v1",
      "name": "redis-ss",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "id": "sre-challenge/redis-ss",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 1,
                "labels": {},
                "name": "redis-ss",
                "namespace": "sre-challenge",
                "resource_version": "920",
                "uid": "b42fee6e-c224-4f22-a0d0-864b2d0c42bb"
              }
            ],
            "spec": [
              {
                "min_ready_seconds": 0,
                "persistent_volume_claim_retention_policy": [
                  {
                    "when_deleted": "Retain",
                    "when_scaled": "Retain"
                  }
                ],
                "pod_management_policy": "OrderedReady",
                "replicas": "1",
                "revision_history_limit": 0,
                "selector": [
                  {
                    "match_expressions": [],
                    "match_labels": {
                      "app": "redis"
                    }
                  }
                ],
                "service_name": "redis-service",
                "template": [
                  {
                    "metadata": [
                      {
                        "annotations": {
                          "reloader.stakater.com/auto": "true"
                        },
                        "generate_name": "",
                        "generation": 0,
                        "labels": {
                          "app": "redis"
                        },
                        "name": "",
                        "namespace": "",
                        "resource_version": "",
                        "uid": ""
                      }
                    ],
                    "spec": [
                      {
                        "active_deadline_seconds": 0,
                        "affinity": [],
                        "automount_service_account_token": true,
                        "container": [
                          {
                            "args": [],
                            "command": [
                              "redis-server",
                              "/etc/redis-config.conf"
                            ],
                            "env": [],
                            "env_from": [],
                            "image": "redis:7.0.0",
                            "image_pull_policy": "IfNotPresent",
                            "lifecycle": [],
                            "liveness_probe": [],
                            "name": "redis",
                            "port": [
                              {
                                "container_port": 6379,
                                "host_ip": "",
                                "host_port": 0,
                                "name": "redis-ss",
                                "protocol": "TCP"
                              }
                            ],
                            "readiness_probe": [],
                            "resources": [
                              {
                                "limits": {
                                  "cpu": "1",
                                  "memory": "1Gi"
                                },
                                "requests": {
                                  "cpu": "500m",
                                  "memory": "100Mi"
                                }
                              }
                            ],
                            "security_context": [],
                            "startup_probe": [],
                            "stdin": false,
                            "stdin_once": false,
                            "termination_message_path": "/dev/termination-log",
                            "termination_message_policy": "File",
                            "tty": false,
                            "volume_mount": [
                              {
                                "mount_path": "/data",
                                "mount_propagation": "None",
                                "name": "redis-data",
                                "read_only": false,
                                "sub_path": ""
                              },
                              {
                                "mount_path": "/etc",
                                "mount_propagation": "None",
                                "name": "redis-claim",
                                "read_only": false,
                                "sub_path": ""
                              }
                            ],
                            "working_dir": ""
                          }
                        ],
                        "dns_config": [],
                        "dns_policy": "ClusterFirst",
                        "enable_service_links": true,
                        "host_aliases": [],
                        "host_ipc": false,
                        "host_network": false,
                        "host_pid": false,
                        "hostname": "",
                        "image_pull_secrets": [],
                        "init_container": [
                          {
                            "args": [],
                            "command": [
                              "/bin/bash",
                              "-c",
                              "/mnt/init.sh"
                            ],
                            "env": [],
                            "env_from": [],
                            "image": "redis:7.0.0",
                            "image_pull_policy": "IfNotPresent",
                            "lifecycle": [],
                            "liveness_probe": [],
                            "name": "init-redis",
                            "port": [],
                            "readiness_probe": [],
                            "resources": [
                              {
                                "limits": {},
                                "requests": {}
                              }
                            ],
                            "security_context": [],
                            "startup_probe": [],
                            "stdin": false,
                            "stdin_once": false,
                            "termination_message_path": "/dev/termination-log",
                            "termination_message_policy": "File",
                            "tty": false,
                            "volume_mount": [
                              {
                                "mount_path": "/etc",
                                "mount_propagation": "None",
                                "name": "redis-claim",
                                "read_only": false,
                                "sub_path": ""
                              },
                              {
                                "mount_path": "/mnt/",
                                "mount_propagation": "None",
                                "name": "config-map",
                                "read_only": false,
                                "sub_path": ""
                              }
                            ],
                            "working_dir": ""
                          }
                        ],
                        "node_name": "",
                        "node_selector": {},
                        "os": [],
                        "priority_class_name": "",
                        "readiness_gate": [],
                        "restart_policy": "Always",
                        "runtime_class_name": "",
                        "scheduler_name": "default-scheduler",
                        "security_context": [],
                        "service_account_name": "",
                        "share_process_namespace": false,
                        "subdomain": "",
                        "termination_grace_period_seconds": 30,
                        "toleration": [],
                        "topology_spread_constraint": [],
                        "volume": [
                          {
                            "aws_elastic_block_store": [],
                            "azure_disk": [],
                            "azure_file": [],
                            "ceph_fs": [],
                            "cinder": [],
                            "config_map": [
                              {
                                "default_mode": "0700",
                                "items": [],
                                "name": "redis-ss-configuration",
                                "optional": false
                              }
                            ],
                            "csi": [],
                            "downward_api": [],
                            "empty_dir": [],
                            "ephemeral": [],
                            "fc": [],
                            "flex_volume": [],
                            "flocker": [],
                            "gce_persistent_disk": [],
                            "git_repo": [],
                            "glusterfs": [],
                            "host_path": [],
                            "iscsi": [],
                            "local": [],
                            "name": "config-map",
                            "nfs": [],
                            "persistent_volume_claim": [],
                            "photon_persistent_disk": [],
                            "projected": [],
                            "quobyte": [],
                            "rbd": [],
                            "secret": [],
                            "vsphere_volume": []
                          }
                        ]
                      }
                    ]
                  }
                ],
                "update_strategy": [],
                "volume_claim_template": [
                  {
                    "metadata": [
                      {
                        "annotations": {},
                        "generate_name": "",
                        "generation": 0,
                        "labels": {},
                        "name": "redis-data",
                        "namespace": "default",
                        "resource_version": "",
                        "uid": ""
                      }
                    ],
                    "spec": [
                      {
                        "access_modes": [
                          "ReadWriteOnce"
                        ],
                        "resources": [
                          {
                            "limits": {},
                            "requests": {
                              "storage": "1Gi"
                            }
                          }
                        ],
                        "selector": [],
                        "storage_class_name": "",
                        "volume_mode": "Filesystem",
                        "volume_name": ""
                      }
                    ]
                  },
                  {
                    "metadata": [
                      {
                        "annotations": {},
                        "generate_name": "",
                        "generation": 0,
                        "labels": {},
                        "name": "redis-claim",
                        "namespace": "default",
                        "resource_version": "",
                        "uid": ""
                      }
                    ],
                    "spec": [
                      {
                        "access_modes": [
                          "ReadWriteOnce"
                        ],
                        "resources": [
                          {
                            "limits": {},
                            "requests": {
                              "storage": "1Gi"
                            }
                          }
                        ],
                        "selector": [],
                        "storage_class_name": "",
                        "volume_mode": "Filesystem",
                        "volume_name": ""
                      }
                    ]
                  }
                ]
              }
            ],
            "timeouts": null,
            "wait_for_rollout": true
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDAsImRlbGV0ZSI6NjAwMDAwMDAwMDAwLCJyZWFkIjo2MDAwMDAwMDAwMDAsInVwZGF0ZSI6NjAwMDAwMDAwMDAwfSwic2NoZW1hX3ZlcnNpb24iOiIxIn0=",
          "dependencies": [
            "kubernetes_config_map_v1.redis",
            "kubernetes_service_v1.redis-service"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
